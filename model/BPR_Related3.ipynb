{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf1bbef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import library and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9e933a87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3f75fcd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Send Rates</th>\n",
       "      <th>Block Size</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>Throughput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.81</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>30</td>\n",
       "      <td>1.13</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>40</td>\n",
       "      <td>1.14</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.14</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>193.2</td>\n",
       "      <td>160</td>\n",
       "      <td>2.90</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>200.9</td>\n",
       "      <td>170</td>\n",
       "      <td>2.23</td>\n",
       "      <td>154.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>199.8</td>\n",
       "      <td>180</td>\n",
       "      <td>2.01</td>\n",
       "      <td>157.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>200.6</td>\n",
       "      <td>190</td>\n",
       "      <td>2.11</td>\n",
       "      <td>160.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>195.5</td>\n",
       "      <td>200</td>\n",
       "      <td>2.31</td>\n",
       "      <td>161.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Send Rates  Block Size  Avg Latency  Throughput\n",
       "0          10.0          10         0.30        10.0\n",
       "1          10.0          20         0.81        10.0\n",
       "2          10.0          30         1.13         9.9\n",
       "3          10.0          40         1.14         9.9\n",
       "4          10.0          50         1.14         9.9\n",
       "..          ...         ...          ...         ...\n",
       "395       193.2         160         2.90       159.0\n",
       "396       200.9         170         2.23       154.9\n",
       "397       199.8         180         2.01       157.2\n",
       "398       200.6         190         2.11       160.4\n",
       "399       195.5         200         2.31       161.8\n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Data/Blockchain Performance Data/Related3/BPD_related3.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "87cc3a5c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.  ,  10.  ,   0.3 ,  10.  ],\n",
       "       [ 10.  ,  20.  ,   0.81,  10.  ],\n",
       "       [ 10.  ,  30.  ,   1.13,   9.9 ],\n",
       "       ...,\n",
       "       [199.8 , 180.  ,   2.01, 157.2 ],\n",
       "       [200.6 , 190.  ,   2.11, 160.4 ],\n",
       "       [195.5 , 200.  ,   2.31, 161.8 ]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = df.values\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8dea8c34",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0efe76e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x221186706b0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "edae8d8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read raw data and divide training set and test set\n",
    "raw_data = xy\n",
    "X = raw_data[:, :2]\n",
    "Y1 = raw_data[:, -2:-1]\n",
    "Y2 = raw_data[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "34886bb7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10. ,  10. ],\n",
       "       [ 10. ,  20. ],\n",
       "       [ 10. ,  30. ],\n",
       "       [ 10. ,  40. ],\n",
       "       [ 10. ,  50. ],\n",
       "       [ 10. ,  60. ],\n",
       "       [ 10. ,  70. ],\n",
       "       [ 10. ,  80. ],\n",
       "       [ 10. ,  90. ],\n",
       "       [ 10. , 100. ],\n",
       "       [ 10. , 110. ],\n",
       "       [ 10. , 120. ],\n",
       "       [ 10. , 130. ],\n",
       "       [ 10. , 140. ],\n",
       "       [ 10. , 150. ],\n",
       "       [ 10. , 160. ],\n",
       "       [ 10. , 170. ],\n",
       "       [ 10. , 180. ],\n",
       "       [ 10. , 190. ],\n",
       "       [ 10. , 200. ],\n",
       "       [ 20.1,  10. ],\n",
       "       [ 20.1,  20. ],\n",
       "       [ 20.1,  30. ],\n",
       "       [ 20.1,  40. ],\n",
       "       [ 20.1,  50. ],\n",
       "       [ 20.1,  60. ],\n",
       "       [ 20.1,  70. ],\n",
       "       [ 20.1,  80. ],\n",
       "       [ 20.1,  90. ],\n",
       "       [ 20.1, 100. ],\n",
       "       [ 20.1, 110. ],\n",
       "       [ 20.1, 120. ],\n",
       "       [ 20.1, 130. ],\n",
       "       [ 20.1, 140. ],\n",
       "       [ 20.1, 150. ],\n",
       "       [ 20.1, 160. ],\n",
       "       [ 20.1, 170. ],\n",
       "       [ 20.1, 180. ],\n",
       "       [ 20.1, 190. ],\n",
       "       [ 20.1, 200. ],\n",
       "       [ 30.1,  10. ],\n",
       "       [ 30.1,  20. ],\n",
       "       [ 30.1,  30. ],\n",
       "       [ 30.1,  40. ],\n",
       "       [ 30.1,  50. ],\n",
       "       [ 30.1,  60. ],\n",
       "       [ 30.1,  70. ],\n",
       "       [ 30.1,  80. ],\n",
       "       [ 30.1,  90. ],\n",
       "       [ 30.1, 100. ],\n",
       "       [ 30.1, 110. ],\n",
       "       [ 30.1, 120. ],\n",
       "       [ 30.1, 130. ],\n",
       "       [ 30.1, 140. ],\n",
       "       [ 30.1, 150. ],\n",
       "       [ 30.1, 160. ],\n",
       "       [ 30.1, 170. ],\n",
       "       [ 30.1, 180. ],\n",
       "       [ 30.1, 190. ],\n",
       "       [ 30.1, 200. ],\n",
       "       [ 40.2,  10. ],\n",
       "       [ 40.2,  20. ],\n",
       "       [ 40.2,  30. ],\n",
       "       [ 40.2,  40. ],\n",
       "       [ 40.2,  50. ],\n",
       "       [ 40.2,  60. ],\n",
       "       [ 40.2,  70. ],\n",
       "       [ 40.2,  80. ],\n",
       "       [ 40.2,  90. ],\n",
       "       [ 40.2, 100. ],\n",
       "       [ 40.2, 110. ],\n",
       "       [ 40.2, 120. ],\n",
       "       [ 40.2, 130. ],\n",
       "       [ 40.2, 140. ],\n",
       "       [ 40.2, 150. ],\n",
       "       [ 40.2, 160. ],\n",
       "       [ 40.2, 170. ],\n",
       "       [ 40.2, 180. ],\n",
       "       [ 40.2, 190. ],\n",
       "       [ 40.2, 200. ],\n",
       "       [ 50.2,  10. ],\n",
       "       [ 50.2,  20. ],\n",
       "       [ 50.2,  30. ],\n",
       "       [ 50.2,  40. ],\n",
       "       [ 50.2,  50. ],\n",
       "       [ 50.2,  60. ],\n",
       "       [ 50.2,  70. ],\n",
       "       [ 50.2,  80. ],\n",
       "       [ 50.2,  90. ],\n",
       "       [ 50.2, 100. ],\n",
       "       [ 50.2, 110. ],\n",
       "       [ 50.2, 120. ],\n",
       "       [ 50.2, 130. ],\n",
       "       [ 50.2, 140. ],\n",
       "       [ 50.2, 150. ],\n",
       "       [ 50.2, 160. ],\n",
       "       [ 50.2, 170. ],\n",
       "       [ 50.2, 180. ],\n",
       "       [ 50.2, 190. ],\n",
       "       [ 50.2, 200. ],\n",
       "       [ 60.3,  10. ],\n",
       "       [ 60.3,  20. ],\n",
       "       [ 60.3,  30. ],\n",
       "       [ 60.3,  40. ],\n",
       "       [ 60.2,  50. ],\n",
       "       [ 60.3,  60. ],\n",
       "       [ 60.3,  70. ],\n",
       "       [ 60.2,  80. ],\n",
       "       [ 60.2,  90. ],\n",
       "       [ 60.2, 100. ],\n",
       "       [ 60.2, 110. ],\n",
       "       [ 60.3, 120. ],\n",
       "       [ 60.2, 130. ],\n",
       "       [ 60.2, 140. ],\n",
       "       [ 60.2, 150. ],\n",
       "       [ 60.2, 160. ],\n",
       "       [ 60.3, 170. ],\n",
       "       [ 60.3, 180. ],\n",
       "       [ 60.2, 190. ],\n",
       "       [ 60.3, 200. ],\n",
       "       [ 70.4,  10. ],\n",
       "       [ 70.3,  20. ],\n",
       "       [ 70.4,  30. ],\n",
       "       [ 70.3,  40. ],\n",
       "       [ 70.3,  50. ],\n",
       "       [ 70.3,  60. ],\n",
       "       [ 70.3,  70. ],\n",
       "       [ 70.3,  80. ],\n",
       "       [ 70.3,  90. ],\n",
       "       [ 70.3, 100. ],\n",
       "       [ 70.3, 110. ],\n",
       "       [ 70.3, 120. ],\n",
       "       [ 70.4, 130. ],\n",
       "       [ 70.3, 140. ],\n",
       "       [ 70.3, 150. ],\n",
       "       [ 70.3, 160. ],\n",
       "       [ 70.3, 170. ],\n",
       "       [ 70.3, 180. ],\n",
       "       [ 70.3, 190. ],\n",
       "       [ 70.3, 200. ],\n",
       "       [ 80.4,  10. ],\n",
       "       [ 80.4,  20. ],\n",
       "       [ 80.2,  30. ],\n",
       "       [ 80.3,  40. ],\n",
       "       [ 80.3,  50. ],\n",
       "       [ 80.3,  60. ],\n",
       "       [ 80.4,  70. ],\n",
       "       [ 80.3,  80. ],\n",
       "       [ 80.2,  90. ],\n",
       "       [ 80.3, 100. ],\n",
       "       [ 80.3, 110. ],\n",
       "       [ 80.3, 120. ],\n",
       "       [ 80.4, 130. ],\n",
       "       [ 80.3, 140. ],\n",
       "       [ 80.4, 150. ],\n",
       "       [ 80.3, 160. ],\n",
       "       [ 80.3, 170. ],\n",
       "       [ 80.4, 180. ],\n",
       "       [ 80.4, 190. ],\n",
       "       [ 80.4, 200. ],\n",
       "       [ 90.4,  10. ],\n",
       "       [ 90.4,  20. ],\n",
       "       [ 90.3,  30. ],\n",
       "       [ 90.4,  40. ],\n",
       "       [ 90.4,  50. ],\n",
       "       [ 90.4,  60. ],\n",
       "       [ 90.3,  70. ],\n",
       "       [ 90.4,  80. ],\n",
       "       [ 90.3,  90. ],\n",
       "       [ 90.3, 100. ],\n",
       "       [ 90.2, 110. ],\n",
       "       [ 90.4, 120. ],\n",
       "       [ 90.3, 130. ],\n",
       "       [ 90.3, 140. ],\n",
       "       [ 90.2, 150. ],\n",
       "       [ 90.4, 160. ],\n",
       "       [ 90.4, 170. ],\n",
       "       [ 90.3, 180. ],\n",
       "       [ 90.4, 190. ],\n",
       "       [ 90.3, 200. ],\n",
       "       [100.5,  10. ],\n",
       "       [100.3,  20. ],\n",
       "       [100.4,  30. ],\n",
       "       [100.4,  40. ],\n",
       "       [100.3,  50. ],\n",
       "       [100.3,  60. ],\n",
       "       [100.4,  70. ],\n",
       "       [100.3,  80. ],\n",
       "       [100.4,  90. ],\n",
       "       [100.4, 100. ],\n",
       "       [100.4, 110. ],\n",
       "       [100.3, 120. ],\n",
       "       [100.4, 130. ],\n",
       "       [100.4, 140. ],\n",
       "       [100.4, 150. ],\n",
       "       [100.3, 160. ],\n",
       "       [100.4, 170. ],\n",
       "       [100.4, 180. ],\n",
       "       [100.4, 190. ],\n",
       "       [100.4, 200. ],\n",
       "       [110.5,  10. ],\n",
       "       [110.4,  20. ],\n",
       "       [110.4,  30. ],\n",
       "       [110.4,  40. ],\n",
       "       [110.4,  50. ],\n",
       "       [110.4,  60. ],\n",
       "       [110.5,  70. ],\n",
       "       [110.2,  80. ],\n",
       "       [110.2,  90. ],\n",
       "       [110.4, 100. ],\n",
       "       [110.4, 110. ],\n",
       "       [110.4, 120. ],\n",
       "       [110.1, 130. ],\n",
       "       [110.4, 140. ],\n",
       "       [110.5, 150. ],\n",
       "       [110.4, 160. ],\n",
       "       [110.4, 170. ],\n",
       "       [110.3, 180. ],\n",
       "       [110.4, 190. ],\n",
       "       [110.5, 200. ],\n",
       "       [120.4,  10. ],\n",
       "       [120.5,  20. ],\n",
       "       [120.5,  30. ],\n",
       "       [120.4,  40. ],\n",
       "       [120.4,  50. ],\n",
       "       [120.3,  60. ],\n",
       "       [120.5,  70. ],\n",
       "       [120.4,  80. ],\n",
       "       [120.4,  90. ],\n",
       "       [120.4, 100. ],\n",
       "       [120.2, 110. ],\n",
       "       [120.2, 120. ],\n",
       "       [120.5, 130. ],\n",
       "       [120.2, 140. ],\n",
       "       [120.4, 150. ],\n",
       "       [120.4, 160. ],\n",
       "       [120.4, 170. ],\n",
       "       [120.4, 180. ],\n",
       "       [120.4, 190. ],\n",
       "       [120.4, 200. ],\n",
       "       [130.5,  10. ],\n",
       "       [130.5,  20. ],\n",
       "       [130.6,  30. ],\n",
       "       [130.2,  40. ],\n",
       "       [130.4,  50. ],\n",
       "       [130.5,  60. ],\n",
       "       [130.5,  70. ],\n",
       "       [130.2,  80. ],\n",
       "       [130.4,  90. ],\n",
       "       [130.5, 100. ],\n",
       "       [130.1, 110. ],\n",
       "       [130.5, 120. ],\n",
       "       [130.5, 130. ],\n",
       "       [130.2, 140. ],\n",
       "       [130.3, 150. ],\n",
       "       [130.5, 160. ],\n",
       "       [130.5, 170. ],\n",
       "       [130.4, 180. ],\n",
       "       [130.5, 190. ],\n",
       "       [130.7, 200. ],\n",
       "       [140.5,  10. ],\n",
       "       [140.5,  20. ],\n",
       "       [140.6,  30. ],\n",
       "       [140.5,  40. ],\n",
       "       [140.6,  50. ],\n",
       "       [140.6,  60. ],\n",
       "       [140.5,  70. ],\n",
       "       [140.4,  80. ],\n",
       "       [140.5,  90. ],\n",
       "       [140.4, 100. ],\n",
       "       [140.4, 110. ],\n",
       "       [140.5, 120. ],\n",
       "       [140.4, 130. ],\n",
       "       [140.4, 140. ],\n",
       "       [140.3, 150. ],\n",
       "       [140.4, 160. ],\n",
       "       [140.5, 170. ],\n",
       "       [140.6, 180. ],\n",
       "       [140.4, 190. ],\n",
       "       [140.4, 200. ],\n",
       "       [150.5,  10. ],\n",
       "       [150.6,  20. ],\n",
       "       [150.6,  30. ],\n",
       "       [150.5,  40. ],\n",
       "       [150.7,  50. ],\n",
       "       [150.6,  60. ],\n",
       "       [150.4,  70. ],\n",
       "       [150.4,  80. ],\n",
       "       [150.2,  90. ],\n",
       "       [150.3, 100. ],\n",
       "       [150.6, 110. ],\n",
       "       [150.3, 120. ],\n",
       "       [150.7, 130. ],\n",
       "       [150.4, 140. ],\n",
       "       [150.5, 150. ],\n",
       "       [150.6, 160. ],\n",
       "       [150.5, 170. ],\n",
       "       [150.5, 180. ],\n",
       "       [150.4, 190. ],\n",
       "       [150.5, 200. ],\n",
       "       [160.5,  10. ],\n",
       "       [160.5,  20. ],\n",
       "       [160.6,  30. ],\n",
       "       [160.3,  40. ],\n",
       "       [160.5,  50. ],\n",
       "       [160.7,  60. ],\n",
       "       [160.5,  70. ],\n",
       "       [160.5,  80. ],\n",
       "       [160.4,  90. ],\n",
       "       [160. , 100. ],\n",
       "       [160.2, 110. ],\n",
       "       [160.5, 120. ],\n",
       "       [160.3, 130. ],\n",
       "       [160.6, 140. ],\n",
       "       [159.7, 150. ],\n",
       "       [160.7, 160. ],\n",
       "       [160.5, 170. ],\n",
       "       [160.5, 180. ],\n",
       "       [159.5, 190. ],\n",
       "       [160.6, 200. ],\n",
       "       [170.8,  10. ],\n",
       "       [169.5,  20. ],\n",
       "       [170. ,  30. ],\n",
       "       [170.5,  40. ],\n",
       "       [170.6,  50. ],\n",
       "       [170.6,  60. ],\n",
       "       [151.9,  70. ],\n",
       "       [168.9,  80. ],\n",
       "       [170.6,  90. ],\n",
       "       [170.5, 100. ],\n",
       "       [169.6, 110. ],\n",
       "       [170.4, 120. ],\n",
       "       [170.2, 130. ],\n",
       "       [170.8, 140. ],\n",
       "       [170.4, 150. ],\n",
       "       [159.1, 160. ],\n",
       "       [170.6, 170. ],\n",
       "       [170.4, 180. ],\n",
       "       [170.3, 190. ],\n",
       "       [170.7, 200. ],\n",
       "       [180.3,  10. ],\n",
       "       [180.6,  20. ],\n",
       "       [180.6,  30. ],\n",
       "       [180.8,  40. ],\n",
       "       [179.8,  50. ],\n",
       "       [180.5,  60. ],\n",
       "       [180.6,  70. ],\n",
       "       [180.1,  80. ],\n",
       "       [180.3,  90. ],\n",
       "       [180.2, 100. ],\n",
       "       [179.4, 110. ],\n",
       "       [180.8, 120. ],\n",
       "       [180.5, 130. ],\n",
       "       [180.5, 140. ],\n",
       "       [180.1, 150. ],\n",
       "       [180.5, 160. ],\n",
       "       [179.7, 170. ],\n",
       "       [180.8, 180. ],\n",
       "       [180.5, 190. ],\n",
       "       [180.4, 200. ],\n",
       "       [190.4,  10. ],\n",
       "       [189.9,  20. ],\n",
       "       [190.5,  30. ],\n",
       "       [189.7,  40. ],\n",
       "       [190.5,  50. ],\n",
       "       [190.5,  60. ],\n",
       "       [190.5,  70. ],\n",
       "       [190.1,  80. ],\n",
       "       [190.6,  90. ],\n",
       "       [186.4, 100. ],\n",
       "       [186. , 110. ],\n",
       "       [190.5, 120. ],\n",
       "       [187.7, 130. ],\n",
       "       [190.1, 140. ],\n",
       "       [190.5, 150. ],\n",
       "       [171. , 160. ],\n",
       "       [189.9, 170. ],\n",
       "       [190.2, 180. ],\n",
       "       [177.5, 190. ],\n",
       "       [187.8, 200. ],\n",
       "       [200.8,  10. ],\n",
       "       [200.4,  20. ],\n",
       "       [200.6,  30. ],\n",
       "       [199.4,  40. ],\n",
       "       [200.2,  50. ],\n",
       "       [200.4,  60. ],\n",
       "       [200.7,  70. ],\n",
       "       [197.9,  80. ],\n",
       "       [199.2,  90. ],\n",
       "       [189.1, 100. ],\n",
       "       [187.1, 110. ],\n",
       "       [200.3, 120. ],\n",
       "       [200.4, 130. ],\n",
       "       [200.6, 140. ],\n",
       "       [196.9, 150. ],\n",
       "       [193.2, 160. ],\n",
       "       [200.9, 170. ],\n",
       "       [199.8, 180. ],\n",
       "       [200.6, 190. ],\n",
       "       [195.5, 200. ]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "650810a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3 ],\n",
       "       [0.81],\n",
       "       [1.13],\n",
       "       [1.14],\n",
       "       [1.14],\n",
       "       [1.12],\n",
       "       [1.16],\n",
       "       [1.18],\n",
       "       [1.18],\n",
       "       [1.18],\n",
       "       [1.13],\n",
       "       [1.14],\n",
       "       [1.21],\n",
       "       [1.19],\n",
       "       [1.14],\n",
       "       [1.16],\n",
       "       [1.17],\n",
       "       [1.18],\n",
       "       [1.18],\n",
       "       [1.17],\n",
       "       [0.18],\n",
       "       [0.44],\n",
       "       [0.71],\n",
       "       [0.96],\n",
       "       [1.13],\n",
       "       [1.14],\n",
       "       [1.14],\n",
       "       [1.17],\n",
       "       [1.2 ],\n",
       "       [1.17],\n",
       "       [1.17],\n",
       "       [1.15],\n",
       "       [1.17],\n",
       "       [1.18],\n",
       "       [1.16],\n",
       "       [1.17],\n",
       "       [1.18],\n",
       "       [1.16],\n",
       "       [1.14],\n",
       "       [1.18],\n",
       "       [0.13],\n",
       "       [0.31],\n",
       "       [0.5 ],\n",
       "       [0.66],\n",
       "       [0.83],\n",
       "       [1.04],\n",
       "       [1.16],\n",
       "       [1.14],\n",
       "       [1.16],\n",
       "       [1.15],\n",
       "       [1.15],\n",
       "       [1.15],\n",
       "       [1.17],\n",
       "       [1.16],\n",
       "       [1.16],\n",
       "       [1.16],\n",
       "       [1.17],\n",
       "       [1.15],\n",
       "       [1.14],\n",
       "       [1.17],\n",
       "       [0.1 ],\n",
       "       [0.24],\n",
       "       [0.39],\n",
       "       [0.51],\n",
       "       [0.64],\n",
       "       [0.81],\n",
       "       [0.93],\n",
       "       [1.08],\n",
       "       [1.13],\n",
       "       [1.19],\n",
       "       [1.21],\n",
       "       [1.18],\n",
       "       [1.16],\n",
       "       [1.14],\n",
       "       [1.15],\n",
       "       [1.17],\n",
       "       [1.18],\n",
       "       [1.13],\n",
       "       [1.15],\n",
       "       [1.16],\n",
       "       [0.09],\n",
       "       [0.2 ],\n",
       "       [0.33],\n",
       "       [0.42],\n",
       "       [0.53],\n",
       "       [0.68],\n",
       "       [0.78],\n",
       "       [0.92],\n",
       "       [1.03],\n",
       "       [1.2 ],\n",
       "       [1.22],\n",
       "       [1.24],\n",
       "       [1.24],\n",
       "       [1.21],\n",
       "       [1.21],\n",
       "       [1.21],\n",
       "       [1.21],\n",
       "       [1.21],\n",
       "       [1.19],\n",
       "       [1.19],\n",
       "       [0.08],\n",
       "       [0.17],\n",
       "       [0.28],\n",
       "       [0.36],\n",
       "       [0.46],\n",
       "       [0.6 ],\n",
       "       [0.68],\n",
       "       [0.8 ],\n",
       "       [0.88],\n",
       "       [0.96],\n",
       "       [1.1 ],\n",
       "       [1.22],\n",
       "       [1.26],\n",
       "       [1.34],\n",
       "       [1.29],\n",
       "       [1.28],\n",
       "       [1.24],\n",
       "       [1.23],\n",
       "       [1.22],\n",
       "       [1.23],\n",
       "       [0.07],\n",
       "       [0.16],\n",
       "       [0.25],\n",
       "       [0.32],\n",
       "       [0.41],\n",
       "       [0.55],\n",
       "       [0.61],\n",
       "       [0.73],\n",
       "       [0.8 ],\n",
       "       [0.87],\n",
       "       [0.97],\n",
       "       [1.07],\n",
       "       [1.07],\n",
       "       [1.1 ],\n",
       "       [1.07],\n",
       "       [1.11],\n",
       "       [1.16],\n",
       "       [1.06],\n",
       "       [1.05],\n",
       "       [1.02],\n",
       "       [0.07],\n",
       "       [0.14],\n",
       "       [0.25],\n",
       "       [0.29],\n",
       "       [0.36],\n",
       "       [0.51],\n",
       "       [0.56],\n",
       "       [0.71],\n",
       "       [0.75],\n",
       "       [0.8 ],\n",
       "       [0.91],\n",
       "       [1.05],\n",
       "       [0.89],\n",
       "       [1.  ],\n",
       "       [0.94],\n",
       "       [0.94],\n",
       "       [0.95],\n",
       "       [0.99],\n",
       "       [0.87],\n",
       "       [0.87],\n",
       "       [0.07],\n",
       "       [0.13],\n",
       "       [0.22],\n",
       "       [0.28],\n",
       "       [0.35],\n",
       "       [0.48],\n",
       "       [0.53],\n",
       "       [0.68],\n",
       "       [0.7 ],\n",
       "       [0.77],\n",
       "       [0.89],\n",
       "       [1.01],\n",
       "       [0.88],\n",
       "       [0.92],\n",
       "       [0.9 ],\n",
       "       [0.9 ],\n",
       "       [0.87],\n",
       "       [0.91],\n",
       "       [0.87],\n",
       "       [0.85],\n",
       "       [0.06],\n",
       "       [0.13],\n",
       "       [0.21],\n",
       "       [0.26],\n",
       "       [0.33],\n",
       "       [0.46],\n",
       "       [0.5 ],\n",
       "       [0.7 ],\n",
       "       [0.68],\n",
       "       [0.69],\n",
       "       [0.88],\n",
       "       [0.89],\n",
       "       [0.77],\n",
       "       [0.86],\n",
       "       [0.93],\n",
       "       [0.78],\n",
       "       [0.78],\n",
       "       [0.79],\n",
       "       [0.8 ],\n",
       "       [0.76],\n",
       "       [0.11],\n",
       "       [0.13],\n",
       "       [0.22],\n",
       "       [0.25],\n",
       "       [0.31],\n",
       "       [0.43],\n",
       "       [0.47],\n",
       "       [0.66],\n",
       "       [0.62],\n",
       "       [0.83],\n",
       "       [0.83],\n",
       "       [0.86],\n",
       "       [0.96],\n",
       "       [0.95],\n",
       "       [0.89],\n",
       "       [0.89],\n",
       "       [0.85],\n",
       "       [1.2 ],\n",
       "       [0.85],\n",
       "       [0.84],\n",
       "       [0.07],\n",
       "       [0.19],\n",
       "       [0.2 ],\n",
       "       [0.34],\n",
       "       [0.41],\n",
       "       [0.44],\n",
       "       [0.49],\n",
       "       [0.67],\n",
       "       [0.64],\n",
       "       [0.72],\n",
       "       [0.93],\n",
       "       [0.92],\n",
       "       [0.86],\n",
       "       [1.58],\n",
       "       [0.94],\n",
       "       [0.87],\n",
       "       [0.85],\n",
       "       [0.88],\n",
       "       [0.81],\n",
       "       [0.81],\n",
       "       [0.1 ],\n",
       "       [0.12],\n",
       "       [0.22],\n",
       "       [0.31],\n",
       "       [0.3 ],\n",
       "       [0.42],\n",
       "       [0.49],\n",
       "       [0.7 ],\n",
       "       [0.65],\n",
       "       [0.68],\n",
       "       [0.81],\n",
       "       [0.83],\n",
       "       [0.84],\n",
       "       [1.03],\n",
       "       [0.87],\n",
       "       [0.94],\n",
       "       [0.83],\n",
       "       [0.8 ],\n",
       "       [0.81],\n",
       "       [0.81],\n",
       "       [0.14],\n",
       "       [0.18],\n",
       "       [0.26],\n",
       "       [0.24],\n",
       "       [0.39],\n",
       "       [0.43],\n",
       "       [0.46],\n",
       "       [0.74],\n",
       "       [0.59],\n",
       "       [0.86],\n",
       "       [1.12],\n",
       "       [0.84],\n",
       "       [0.74],\n",
       "       [0.94],\n",
       "       [1.02],\n",
       "       [0.88],\n",
       "       [0.89],\n",
       "       [0.81],\n",
       "       [0.82],\n",
       "       [0.87],\n",
       "       [0.21],\n",
       "       [0.2 ],\n",
       "       [0.22],\n",
       "       [0.33],\n",
       "       [0.41],\n",
       "       [0.43],\n",
       "       [0.46],\n",
       "       [1.39],\n",
       "       [0.68],\n",
       "       [0.98],\n",
       "       [0.89],\n",
       "       [0.8 ],\n",
       "       [0.83],\n",
       "       [0.91],\n",
       "       [1.07],\n",
       "       [0.85],\n",
       "       [0.95],\n",
       "       [0.86],\n",
       "       [0.89],\n",
       "       [1.02],\n",
       "       [0.5 ],\n",
       "       [0.22],\n",
       "       [0.26],\n",
       "       [0.31],\n",
       "       [0.41],\n",
       "       [0.47],\n",
       "       [0.58],\n",
       "       [1.61],\n",
       "       [1.16],\n",
       "       [1.26],\n",
       "       [1.96],\n",
       "       [1.13],\n",
       "       [0.8 ],\n",
       "       [1.17],\n",
       "       [2.89],\n",
       "       [1.34],\n",
       "       [1.03],\n",
       "       [1.23],\n",
       "       [0.94],\n",
       "       [0.97],\n",
       "       [1.66],\n",
       "       [0.39],\n",
       "       [0.32],\n",
       "       [0.44],\n",
       "       [0.48],\n",
       "       [0.53],\n",
       "       [1.16],\n",
       "       [2.07],\n",
       "       [0.97],\n",
       "       [1.76],\n",
       "       [2.26],\n",
       "       [0.87],\n",
       "       [0.91],\n",
       "       [0.9 ],\n",
       "       [1.4 ],\n",
       "       [2.51],\n",
       "       [1.54],\n",
       "       [1.9 ],\n",
       "       [1.  ],\n",
       "       [1.22],\n",
       "       [1.73],\n",
       "       [0.42],\n",
       "       [0.9 ],\n",
       "       [0.81],\n",
       "       [0.81],\n",
       "       [0.75],\n",
       "       [0.79],\n",
       "       [1.89],\n",
       "       [2.07],\n",
       "       [2.32],\n",
       "       [2.44],\n",
       "       [1.27],\n",
       "       [1.07],\n",
       "       [1.24],\n",
       "       [1.42],\n",
       "       [2.59],\n",
       "       [2.33],\n",
       "       [1.51],\n",
       "       [1.08],\n",
       "       [1.43],\n",
       "       [1.59],\n",
       "       [1.44],\n",
       "       [0.88],\n",
       "       [1.15],\n",
       "       [1.04],\n",
       "       [0.86],\n",
       "       [1.33],\n",
       "       [2.61],\n",
       "       [1.85],\n",
       "       [2.64],\n",
       "       [2.35],\n",
       "       [1.22],\n",
       "       [1.2 ],\n",
       "       [1.27],\n",
       "       [1.47],\n",
       "       [3.02],\n",
       "       [2.56],\n",
       "       [2.13],\n",
       "       [3.17],\n",
       "       [1.41],\n",
       "       [2.29],\n",
       "       [1.59],\n",
       "       [1.34],\n",
       "       [1.19],\n",
       "       [1.37],\n",
       "       [1.55],\n",
       "       [1.44],\n",
       "       [2.26],\n",
       "       [2.1 ],\n",
       "       [2.7 ],\n",
       "       [3.15],\n",
       "       [1.65],\n",
       "       [2.04],\n",
       "       [1.94],\n",
       "       [2.43],\n",
       "       [2.9 ],\n",
       "       [2.23],\n",
       "       [2.01],\n",
       "       [2.11],\n",
       "       [2.31]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4d85c76d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10. ],\n",
       "       [ 10. ],\n",
       "       [  9.9],\n",
       "       [  9.9],\n",
       "       [  9.9],\n",
       "       [ 10. ],\n",
       "       [  9.8],\n",
       "       [ 10. ],\n",
       "       [ 10. ],\n",
       "       [  9.8],\n",
       "       [ 10. ],\n",
       "       [  9.9],\n",
       "       [  9.8],\n",
       "       [  9.9],\n",
       "       [  9.9],\n",
       "       [  9.8],\n",
       "       [  9.8],\n",
       "       [  9.9],\n",
       "       [  9.9],\n",
       "       [ 10. ],\n",
       "       [ 20.1],\n",
       "       [ 20.1],\n",
       "       [ 19.4],\n",
       "       [ 20.1],\n",
       "       [ 19.4],\n",
       "       [ 20. ],\n",
       "       [ 19.3],\n",
       "       [ 19.9],\n",
       "       [ 19.9],\n",
       "       [ 20. ],\n",
       "       [ 19.3],\n",
       "       [ 19.4],\n",
       "       [ 19.3],\n",
       "       [ 19.8],\n",
       "       [ 19.9],\n",
       "       [ 19.9],\n",
       "       [ 19.8],\n",
       "       [ 19.9],\n",
       "       [ 19.3],\n",
       "       [ 20. ],\n",
       "       [ 30.1],\n",
       "       [ 30.1],\n",
       "       [ 28.5],\n",
       "       [ 30.1],\n",
       "       [ 30.1],\n",
       "       [ 29.3],\n",
       "       [ 28.7],\n",
       "       [ 28.6],\n",
       "       [ 28.5],\n",
       "       [ 28.5],\n",
       "       [ 28.5],\n",
       "       [ 28.5],\n",
       "       [ 28.8],\n",
       "       [ 28.5],\n",
       "       [ 28.6],\n",
       "       [ 28.4],\n",
       "       [ 28.5],\n",
       "       [ 28.6],\n",
       "       [ 28.8],\n",
       "       [ 28.7],\n",
       "       [ 40.1],\n",
       "       [ 40.1],\n",
       "       [ 37.3],\n",
       "       [ 40.1],\n",
       "       [ 40.1],\n",
       "       [ 38.3],\n",
       "       [ 37.6],\n",
       "       [ 38.3],\n",
       "       [ 37.4],\n",
       "       [ 37.2],\n",
       "       [ 37.4],\n",
       "       [ 37.5],\n",
       "       [ 37.6],\n",
       "       [ 37.4],\n",
       "       [ 37.4],\n",
       "       [ 37.6],\n",
       "       [ 37.4],\n",
       "       [ 37.4],\n",
       "       [ 37.3],\n",
       "       [ 39.8],\n",
       "       [ 50.1],\n",
       "       [ 50.1],\n",
       "       [ 45.8],\n",
       "       [ 50.1],\n",
       "       [ 50.1],\n",
       "       [ 47. ],\n",
       "       [ 46.2],\n",
       "       [ 46.9],\n",
       "       [ 45.6],\n",
       "       [ 49.8],\n",
       "       [ 47.8],\n",
       "       [ 48.2],\n",
       "       [ 48.2],\n",
       "       [ 47.5],\n",
       "       [ 47.5],\n",
       "       [ 48.6],\n",
       "       [ 48.4],\n",
       "       [ 48.3],\n",
       "       [ 48.2],\n",
       "       [ 48.2],\n",
       "       [ 60.2],\n",
       "       [ 60.1],\n",
       "       [ 54. ],\n",
       "       [ 60.1],\n",
       "       [ 60. ],\n",
       "       [ 55.3],\n",
       "       [ 54.3],\n",
       "       [ 55.2],\n",
       "       [ 53.7],\n",
       "       [ 59.8],\n",
       "       [ 53.8],\n",
       "       [ 55. ],\n",
       "       [ 53.6],\n",
       "       [ 53.5],\n",
       "       [ 54. ],\n",
       "       [ 53.7],\n",
       "       [ 53.9],\n",
       "       [ 59.3],\n",
       "       [ 54.1],\n",
       "       [ 54.5],\n",
       "       [ 70.2],\n",
       "       [ 70.1],\n",
       "       [ 61.8],\n",
       "       [ 70. ],\n",
       "       [ 70.1],\n",
       "       [ 63.3],\n",
       "       [ 62.2],\n",
       "       [ 63. ],\n",
       "       [ 61.6],\n",
       "       [ 69.7],\n",
       "       [ 61.7],\n",
       "       [ 63.2],\n",
       "       [ 61.5],\n",
       "       [ 61.3],\n",
       "       [ 69.7],\n",
       "       [ 61.4],\n",
       "       [ 61.5],\n",
       "       [ 61.4],\n",
       "       [ 61.6],\n",
       "       [ 69.9],\n",
       "       [ 80.2],\n",
       "       [ 80.1],\n",
       "       [ 69.3],\n",
       "       [ 80. ],\n",
       "       [ 79.9],\n",
       "       [ 70.9],\n",
       "       [ 69.8],\n",
       "       [ 71. ],\n",
       "       [ 69.2],\n",
       "       [ 79.4],\n",
       "       [ 69.3],\n",
       "       [ 70.9],\n",
       "       [ 69.3],\n",
       "       [ 69.4],\n",
       "       [ 69.4],\n",
       "       [ 69.1],\n",
       "       [ 69.6],\n",
       "       [ 69.8],\n",
       "       [ 69.6],\n",
       "       [ 69.8],\n",
       "       [ 90.1],\n",
       "       [ 90.1],\n",
       "       [ 76.6],\n",
       "       [ 89.8],\n",
       "       [ 89.8],\n",
       "       [ 78.3],\n",
       "       [ 77.1],\n",
       "       [ 78.1],\n",
       "       [ 76.4],\n",
       "       [ 89.2],\n",
       "       [ 76.6],\n",
       "       [ 77.9],\n",
       "       [ 81.6],\n",
       "       [ 80.6],\n",
       "       [ 81.1],\n",
       "       [ 80.9],\n",
       "       [ 80.6],\n",
       "       [ 80.5],\n",
       "       [ 81.2],\n",
       "       [ 81.3],\n",
       "       [100.1],\n",
       "       [100.1],\n",
       "       [ 83.7],\n",
       "       [ 99.8],\n",
       "       [ 99.5],\n",
       "       [ 85.7],\n",
       "       [ 84.1],\n",
       "       [ 85.5],\n",
       "       [ 83.4],\n",
       "       [ 99. ],\n",
       "       [ 83.6],\n",
       "       [ 85.1],\n",
       "       [ 96.9],\n",
       "       [ 96.8],\n",
       "       [ 96.4],\n",
       "       [ 96.8],\n",
       "       [ 96.8],\n",
       "       [ 97.1],\n",
       "       [ 95.7],\n",
       "       [ 97.2],\n",
       "       [110.1],\n",
       "       [110. ],\n",
       "       [ 90.6],\n",
       "       [110. ],\n",
       "       [109.6],\n",
       "       [ 92.3],\n",
       "       [ 91.1],\n",
       "       [ 92.4],\n",
       "       [ 90.3],\n",
       "       [ 96.3],\n",
       "       [ 90.5],\n",
       "       [ 92.3],\n",
       "       [ 95.8],\n",
       "       [ 96. ],\n",
       "       [ 96.2],\n",
       "       [ 95.7],\n",
       "       [ 96. ],\n",
       "       [ 97.3],\n",
       "       [ 96.6],\n",
       "       [ 96.6],\n",
       "       [120. ],\n",
       "       [ 97.7],\n",
       "       [ 96.9],\n",
       "       [ 99.2],\n",
       "       [ 99.8],\n",
       "       [ 99.1],\n",
       "       [ 97.5],\n",
       "       [ 98.8],\n",
       "       [ 97.1],\n",
       "       [118.9],\n",
       "       [ 97.1],\n",
       "       [ 99.2],\n",
       "       [ 98.2],\n",
       "       [ 97.1],\n",
       "       [ 96.6],\n",
       "       [ 97.6],\n",
       "       [ 97.2],\n",
       "       [ 97.9],\n",
       "       [ 97.2],\n",
       "       [ 97.4],\n",
       "       [103.3],\n",
       "       [130.1],\n",
       "       [103.4],\n",
       "       [105.3],\n",
       "       [129.8],\n",
       "       [105.5],\n",
       "       [104.2],\n",
       "       [105.4],\n",
       "       [102.8],\n",
       "       [128.3],\n",
       "       [102.9],\n",
       "       [105.4],\n",
       "       [119.9],\n",
       "       [119.9],\n",
       "       [120.1],\n",
       "       [120.6],\n",
       "       [120.5],\n",
       "       [120.2],\n",
       "       [121. ],\n",
       "       [121.4],\n",
       "       [109.6],\n",
       "       [110.3],\n",
       "       [109.8],\n",
       "       [139.6],\n",
       "       [112.6],\n",
       "       [111.8],\n",
       "       [110.2],\n",
       "       [111.8],\n",
       "       [109.2],\n",
       "       [116.5],\n",
       "       [109.4],\n",
       "       [111.3],\n",
       "       [121.3],\n",
       "       [120.5],\n",
       "       [121.7],\n",
       "       [122.1],\n",
       "       [121.1],\n",
       "       [121.4],\n",
       "       [122.1],\n",
       "       [122.4],\n",
       "       [115.4],\n",
       "       [116.1],\n",
       "       [115.6],\n",
       "       [117.7],\n",
       "       [118.7],\n",
       "       [117.6],\n",
       "       [116.4],\n",
       "       [115.6],\n",
       "       [114.5],\n",
       "       [121.4],\n",
       "       [114.8],\n",
       "       [117.7],\n",
       "       [120. ],\n",
       "       [122. ],\n",
       "       [120.3],\n",
       "       [120.7],\n",
       "       [120.4],\n",
       "       [120.6],\n",
       "       [121. ],\n",
       "       [121.1],\n",
       "       [157.9],\n",
       "       [122.1],\n",
       "       [121.3],\n",
       "       [158.6],\n",
       "       [124.6],\n",
       "       [123.7],\n",
       "       [120.8],\n",
       "       [119.5],\n",
       "       [120.5],\n",
       "       [124.2],\n",
       "       [118.1],\n",
       "       [124. ],\n",
       "       [122. ],\n",
       "       [122.3],\n",
       "       [119.5],\n",
       "       [120.8],\n",
       "       [121.3],\n",
       "       [120.6],\n",
       "       [121.7],\n",
       "       [122.2],\n",
       "       [126.2],\n",
       "       [126.2],\n",
       "       [126.2],\n",
       "       [128.4],\n",
       "       [129.8],\n",
       "       [128.8],\n",
       "       [115.9],\n",
       "       [123. ],\n",
       "       [124.9],\n",
       "       [128. ],\n",
       "       [121.2],\n",
       "       [129.5],\n",
       "       [162.3],\n",
       "       [161. ],\n",
       "       [159.4],\n",
       "       [118.7],\n",
       "       [156.6],\n",
       "       [156.3],\n",
       "       [161.7],\n",
       "       [160.4],\n",
       "       [130.5],\n",
       "       [131.3],\n",
       "       [131.3],\n",
       "       [131.1],\n",
       "       [132.9],\n",
       "       [132.4],\n",
       "       [131.7],\n",
       "       [128.2],\n",
       "       [128.8],\n",
       "       [129.5],\n",
       "       [125.5],\n",
       "       [131.5],\n",
       "       [160.2],\n",
       "       [160.3],\n",
       "       [158.9],\n",
       "       [156.4],\n",
       "       [158.5],\n",
       "       [159.7],\n",
       "       [161.3],\n",
       "       [160.9],\n",
       "       [135.1],\n",
       "       [135.9],\n",
       "       [136. ],\n",
       "       [137. ],\n",
       "       [137.6],\n",
       "       [136.7],\n",
       "       [135.2],\n",
       "       [125.8],\n",
       "       [132.6],\n",
       "       [130.6],\n",
       "       [129.4],\n",
       "       [135.4],\n",
       "       [162.5],\n",
       "       [161.8],\n",
       "       [162.2],\n",
       "       [140.2],\n",
       "       [159.9],\n",
       "       [162.8],\n",
       "       [142.5],\n",
       "       [160.8],\n",
       "       [137.8],\n",
       "       [141.1],\n",
       "       [140.4],\n",
       "       [140.3],\n",
       "       [142.4],\n",
       "       [139.4],\n",
       "       [139.7],\n",
       "       [135.2],\n",
       "       [136.3],\n",
       "       [134.6],\n",
       "       [130.6],\n",
       "       [139. ],\n",
       "       [156.6],\n",
       "       [161.9],\n",
       "       [161. ],\n",
       "       [159. ],\n",
       "       [154.9],\n",
       "       [157.2],\n",
       "       [160.4],\n",
       "       [161.8]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a82a8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MLP1(send rates, block size) = latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a0458870",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split training set and test set\n",
    "Xtrain1, Xtest1, Ytrain1, Ytest1 = train_test_split(X, Y1, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9bdee55f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200.6, 190. ],\n",
       "       [ 70.3,  60. ],\n",
       "       [170.6,  90. ],\n",
       "       [170.7, 200. ],\n",
       "       [ 90.3, 130. ],\n",
       "       [180.6,  30. ],\n",
       "       [100.4, 180. ],\n",
       "       [150.3, 120. ],\n",
       "       [ 20.1, 100. ],\n",
       "       [150.7,  50. ],\n",
       "       [ 90.2, 150. ],\n",
       "       [187.7, 130. ],\n",
       "       [100.4,  90. ],\n",
       "       [170.6,  50. ],\n",
       "       [169.5,  20. ],\n",
       "       [120.4,  80. ],\n",
       "       [190.5, 120. ],\n",
       "       [ 10. ,  60. ],\n",
       "       [ 40.2, 190. ],\n",
       "       [120.4,  40. ],\n",
       "       [ 70.4,  30. ],\n",
       "       [130.6,  30. ],\n",
       "       [200.6,  30. ],\n",
       "       [110.5, 150. ],\n",
       "       [ 10. , 180. ],\n",
       "       [ 50.2, 130. ],\n",
       "       [190.5,  70. ],\n",
       "       [110.4,  20. ],\n",
       "       [189.9,  20. ],\n",
       "       [110.2,  80. ],\n",
       "       [ 50.2,  20. ],\n",
       "       [ 10. ,  50. ],\n",
       "       [ 90.4,  60. ],\n",
       "       [140.4, 160. ],\n",
       "       [ 10. ,  70. ],\n",
       "       [ 50.2,  10. ],\n",
       "       [ 30.1, 190. ],\n",
       "       [ 60.3,  30. ],\n",
       "       [199.8, 180. ],\n",
       "       [ 70.3, 200. ],\n",
       "       [140.5,  90. ],\n",
       "       [110.4,  50. ],\n",
       "       [ 90.4, 120. ],\n",
       "       [ 40.2,  30. ],\n",
       "       [200.4,  60. ],\n",
       "       [ 50.2,  60. ],\n",
       "       [189.1, 100. ],\n",
       "       [160.5,  80. ],\n",
       "       [159.7, 150. ],\n",
       "       [140.4, 190. ],\n",
       "       [ 40.2,  60. ],\n",
       "       [120.3,  60. ],\n",
       "       [120.4, 100. ],\n",
       "       [ 10. , 190. ],\n",
       "       [150.5, 170. ],\n",
       "       [150.4,  70. ],\n",
       "       [140.4, 130. ],\n",
       "       [ 60.3, 180. ],\n",
       "       [130.5, 190. ],\n",
       "       [190.4,  10. ],\n",
       "       [ 60.2,  80. ],\n",
       "       [ 40.2,  80. ],\n",
       "       [120.4, 150. ],\n",
       "       [130.5,  70. ],\n",
       "       [180.1, 150. ],\n",
       "       [120.5,  30. ],\n",
       "       [ 60.3,  70. ],\n",
       "       [160.2, 110. ],\n",
       "       [140.4, 110. ],\n",
       "       [160.3, 130. ],\n",
       "       [110.4, 190. ],\n",
       "       [ 50.2, 160. ],\n",
       "       [179.4, 110. ],\n",
       "       [197.9,  80. ],\n",
       "       [ 10. , 120. ],\n",
       "       [180.1,  80. ],\n",
       "       [ 30.1,  20. ],\n",
       "       [100.5,  10. ],\n",
       "       [ 70.4, 130. ],\n",
       "       [120.4,  50. ]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set\n",
    "Xtest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec3253d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.11],\n",
       "       [0.55],\n",
       "       [0.97],\n",
       "       [1.22],\n",
       "       [0.88],\n",
       "       [0.9 ],\n",
       "       [0.79],\n",
       "       [0.8 ],\n",
       "       [1.17],\n",
       "       [0.41],\n",
       "       [0.9 ],\n",
       "       [1.2 ],\n",
       "       [0.68],\n",
       "       [0.48],\n",
       "       [0.39],\n",
       "       [0.67],\n",
       "       [1.22],\n",
       "       [1.12],\n",
       "       [1.15],\n",
       "       [0.34],\n",
       "       [0.25],\n",
       "       [0.22],\n",
       "       [1.34],\n",
       "       [0.89],\n",
       "       [1.18],\n",
       "       [1.24],\n",
       "       [1.33],\n",
       "       [0.13],\n",
       "       [1.44],\n",
       "       [0.66],\n",
       "       [0.2 ],\n",
       "       [1.14],\n",
       "       [0.48],\n",
       "       [0.88],\n",
       "       [1.16],\n",
       "       [0.09],\n",
       "       [1.14],\n",
       "       [0.28],\n",
       "       [2.01],\n",
       "       [1.02],\n",
       "       [0.59],\n",
       "       [0.31],\n",
       "       [1.01],\n",
       "       [0.39],\n",
       "       [1.55],\n",
       "       [0.68],\n",
       "       [2.7 ],\n",
       "       [1.61],\n",
       "       [2.89],\n",
       "       [0.82],\n",
       "       [0.81],\n",
       "       [0.44],\n",
       "       [0.72],\n",
       "       [1.18],\n",
       "       [0.95],\n",
       "       [0.46],\n",
       "       [0.74],\n",
       "       [1.23],\n",
       "       [0.81],\n",
       "       [1.59],\n",
       "       [0.8 ],\n",
       "       [1.08],\n",
       "       [0.94],\n",
       "       [0.49],\n",
       "       [1.42],\n",
       "       [0.2 ],\n",
       "       [0.68],\n",
       "       [1.96],\n",
       "       [1.12],\n",
       "       [0.8 ],\n",
       "       [0.85],\n",
       "       [1.21],\n",
       "       [2.44],\n",
       "       [2.26],\n",
       "       [1.14],\n",
       "       [1.89],\n",
       "       [0.31],\n",
       "       [0.06],\n",
       "       [1.07],\n",
       "       [0.41]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cec710d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# property scaling\n",
    "min_max_scaler1 = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f6ca873",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21058146, 0.68421053],\n",
       "       [0.05290728, 0.15789474],\n",
       "       [0.73598743, 1.        ],\n",
       "       [0.        , 0.68421053],\n",
       "       [0.21058146, 0.52631579],\n",
       "       [0.15819801, 0.05263158],\n",
       "       [0.99738083, 0.05263158],\n",
       "       [0.52592981, 0.68421053],\n",
       "       [0.99685699, 0.57894737],\n",
       "       [0.31587218, 0.36842105],\n",
       "       [0.36877947, 1.        ],\n",
       "       [0.89313777, 0.78947368],\n",
       "       [0.73598743, 0.15789474],\n",
       "       [0.83970665, 0.94736842],\n",
       "       [0.42116291, 0.05263158],\n",
       "       [0.78837087, 0.        ],\n",
       "       [0.68412782, 0.89473684],\n",
       "       [0.73493976, 0.47368421],\n",
       "       [0.68360398, 0.        ],\n",
       "       [0.42063908, 0.68421053],\n",
       "       [0.21058146, 0.10526316],\n",
       "       [0.94342588, 0.36842105],\n",
       "       [0.42063908, 1.        ],\n",
       "       [0.05290728, 0.36842105],\n",
       "       [0.21058146, 0.47368421],\n",
       "       [0.78837087, 0.57894737],\n",
       "       [0.15819801, 0.68421053],\n",
       "       [0.57726558, 0.68421053],\n",
       "       [0.73546359, 0.94736842],\n",
       "       [0.6291252 , 0.52631579],\n",
       "       [0.26348874, 1.        ],\n",
       "       [0.94394971, 0.89473684],\n",
       "       [0.21058146, 0.94736842],\n",
       "       [0.6306967 , 0.21052632],\n",
       "       [0.36877947, 0.31578947],\n",
       "       [0.21058146, 0.57894737],\n",
       "       [0.83813515, 0.10526316],\n",
       "       [0.6306967 , 0.42105263],\n",
       "       [0.47354636, 0.31578947],\n",
       "       [0.95966475, 0.78947368],\n",
       "       [0.05290728, 1.        ],\n",
       "       [0.42116291, 0.36842105],\n",
       "       [0.99214248, 0.15789474],\n",
       "       [0.57883709, 0.05263158],\n",
       "       [0.89470927, 0.57894737],\n",
       "       [0.10529073, 1.        ],\n",
       "       [0.31639602, 0.        ],\n",
       "       [0.        , 0.63157895],\n",
       "       [0.7857517 , 0.47368421],\n",
       "       [0.97171294, 1.        ],\n",
       "       [0.84232583, 0.68421053],\n",
       "       [0.73546359, 0.68421053],\n",
       "       [0.92194866, 0.52631579],\n",
       "       [0.73546359, 0.36842105],\n",
       "       [0.89313777, 0.94736842],\n",
       "       [0.73651126, 0.78947368],\n",
       "       [0.21058146, 0.42105263],\n",
       "       [0.31587218, 0.57894737],\n",
       "       [0.31587218, 0.21052632],\n",
       "       [0.94237821, 0.84210526],\n",
       "       [0.        , 0.73684211],\n",
       "       [0.42063908, 0.10526316],\n",
       "       [0.31587218, 0.15789474],\n",
       "       [0.31587218, 0.94736842],\n",
       "       [0.26348874, 0.57894737],\n",
       "       [0.10529073, 0.57894737],\n",
       "       [0.2629649 , 0.63157895],\n",
       "       [0.        , 0.47368421],\n",
       "       [0.47302252, 0.26315789],\n",
       "       [0.        , 0.84210526],\n",
       "       [0.94552122, 0.21052632],\n",
       "       [0.        , 0.        ],\n",
       "       [0.26348874, 0.26315789],\n",
       "       [0.47354636, 0.47368421],\n",
       "       [0.52592981, 0.57894737],\n",
       "       [0.8920901 , 0.        ],\n",
       "       [0.15819801, 0.52631579],\n",
       "       [0.05290728, 0.94736842],\n",
       "       [0.36825563, 0.52631579],\n",
       "       [0.52488214, 0.42105263],\n",
       "       [0.42116291, 0.15789474],\n",
       "       [0.62964903, 0.36842105],\n",
       "       [0.87742273, 0.94736842],\n",
       "       [0.36825563, 0.26315789],\n",
       "       [0.63122053, 0.57894737],\n",
       "       [0.10529073, 0.10526316],\n",
       "       [0.42116291, 0.        ],\n",
       "       [0.57726558, 0.52631579],\n",
       "       [0.47302252, 0.57894737],\n",
       "       [0.57831325, 0.94736842],\n",
       "       [0.88947093, 0.21052632],\n",
       "       [0.99738083, 0.63157895],\n",
       "       [0.83603981, 0.52631579],\n",
       "       [0.73651126, 0.26315789],\n",
       "       [0.47354636, 0.63157895],\n",
       "       [0.36825563, 0.36842105],\n",
       "       [0.78837087, 0.21052632],\n",
       "       [0.26348874, 0.        ],\n",
       "       [0.05290728, 0.73684211],\n",
       "       [0.2629649 , 0.52631579],\n",
       "       [0.42116291, 0.78947368],\n",
       "       [0.63122053, 0.26315789],\n",
       "       [0.31587218, 0.78947368],\n",
       "       [0.94552122, 0.73684211],\n",
       "       [0.36877947, 0.73684211],\n",
       "       [0.        , 1.        ],\n",
       "       [0.3677318 , 0.10526316],\n",
       "       [0.36877947, 0.94736842],\n",
       "       [0.68308015, 0.68421053],\n",
       "       [0.10529073, 0.21052632],\n",
       "       [0.68360398, 0.57894737],\n",
       "       [0.57831325, 0.84210526],\n",
       "       [0.63122053, 0.47368421],\n",
       "       [0.2629649 , 0.42105263],\n",
       "       [0.89313777, 0.26315789],\n",
       "       [0.42063908, 0.47368421],\n",
       "       [0.15819801, 1.        ],\n",
       "       [0.93137768, 1.        ],\n",
       "       [0.21058146, 0.21052632],\n",
       "       [0.        , 0.42105263],\n",
       "       [0.94552122, 0.26315789],\n",
       "       [0.21058146, 1.        ],\n",
       "       [0.8920901 , 0.42105263],\n",
       "       [0.84075432, 0.47368421],\n",
       "       [0.84232583, 0.        ],\n",
       "       [0.05290728, 0.42105263],\n",
       "       [0.05290728, 0.57894737],\n",
       "       [0.10529073, 0.78947368],\n",
       "       [0.57831325, 0.42105263],\n",
       "       [0.05290728, 0.63157895],\n",
       "       [0.68255631, 0.73684211],\n",
       "       [0.10529073, 0.42105263],\n",
       "       [0.84023049, 0.57894737],\n",
       "       [0.6306967 , 0.89473684],\n",
       "       [0.8936616 , 0.05263158],\n",
       "       [0.05290728, 0.68421053],\n",
       "       [0.68308015, 0.36842105],\n",
       "       [0.05290728, 0.78947368],\n",
       "       [0.78732321, 0.15789474],\n",
       "       [0.15819801, 0.15789474],\n",
       "       [0.36877947, 0.89473684],\n",
       "       [0.31587218, 0.42105263],\n",
       "       [0.10529073, 0.31578947],\n",
       "       [0.57883709, 0.63157895],\n",
       "       [0.15819801, 0.31578947],\n",
       "       [0.63122053, 0.84210526],\n",
       "       [0.89156627, 0.47368421],\n",
       "       [0.68360398, 0.05263158],\n",
       "       [0.99947617, 0.        ],\n",
       "       [0.92771084, 0.52631579],\n",
       "       [0.84075432, 0.15789474],\n",
       "       [0.10529073, 0.26315789],\n",
       "       [0.52645364, 0.31578947],\n",
       "       [0.42063908, 0.42105263],\n",
       "       [0.83918282, 0.63157895],\n",
       "       [0.10529073, 0.36842105],\n",
       "       [0.2629649 , 0.68421053],\n",
       "       [0.42063908, 0.89473684],\n",
       "       [0.52645364, 1.        ],\n",
       "       [0.78941854, 0.26315789],\n",
       "       [0.10529073, 0.        ],\n",
       "       [0.05290728, 0.05263158],\n",
       "       [0.26348874, 0.05263158],\n",
       "       [0.42116291, 0.21052632],\n",
       "       [0.15819801, 0.47368421],\n",
       "       [0.10529073, 0.68421053],\n",
       "       [0.47354636, 1.        ],\n",
       "       [0.99633316, 0.21052632],\n",
       "       [0.31587218, 0.89473684],\n",
       "       [0.05290728, 0.21052632],\n",
       "       [0.84127816, 0.26315789],\n",
       "       [0.47302252, 0.36842105],\n",
       "       [0.47302252, 0.21052632],\n",
       "       [0.52592981, 0.26315789],\n",
       "       [0.31587218, 0.73684211],\n",
       "       [0.78313253, 0.94736842],\n",
       "       [0.26348874, 0.84210526],\n",
       "       [0.5243583 , 0.63157895],\n",
       "       [0.94552122, 0.10526316],\n",
       "       [0.89470927, 0.15789474],\n",
       "       [0.84023049, 0.89473684],\n",
       "       [0.57726558, 0.57894737],\n",
       "       [0.52540597, 0.89473684],\n",
       "       [0.10529073, 0.84210526],\n",
       "       [0.78941854, 0.78947368],\n",
       "       [0.73598743, 0.73684211],\n",
       "       [0.7370351 , 0.63157895],\n",
       "       [0.47302252, 0.05263158],\n",
       "       [0.21058146, 0.89473684],\n",
       "       [0.2629649 , 0.73684211],\n",
       "       [0.2629649 , 0.94736842],\n",
       "       [0.78837087, 0.05263158],\n",
       "       [0.42011524, 0.52631579],\n",
       "       [0.8936616 , 0.31578947],\n",
       "       [0.10529073, 0.73684211],\n",
       "       [0.42116291, 0.84210526],\n",
       "       [0.47354636, 0.73684211],\n",
       "       [0.47354636, 0.94736842],\n",
       "       [0.47354636, 0.10526316],\n",
       "       [0.83237297, 0.36842105],\n",
       "       [0.26348874, 0.15789474],\n",
       "       [0.57831325, 0.        ],\n",
       "       [0.31587218, 0.52631579],\n",
       "       [0.73651126, 0.52631579],\n",
       "       [0.15819801, 0.        ],\n",
       "       [0.21058146, 0.73684211],\n",
       "       [0.47354636, 0.68421053],\n",
       "       [0.36877947, 0.        ],\n",
       "       [0.3677318 , 0.42105263],\n",
       "       [0.52592981, 0.10526316],\n",
       "       [0.36877947, 0.63157895],\n",
       "       [0.74332111, 0.31578947],\n",
       "       [0.        , 0.52631579],\n",
       "       [0.68308015, 0.47368421],\n",
       "       [0.21058146, 0.84210526],\n",
       "       [0.52592981, 0.52631579],\n",
       "       [0.63122053, 0.        ],\n",
       "       [0.10529073, 0.89473684],\n",
       "       [0.78837087, 0.31578947],\n",
       "       [0.05290728, 0.84210526],\n",
       "       [0.89261393, 1.        ],\n",
       "       [0.05290728, 0.        ],\n",
       "       [0.89313777, 0.68421053],\n",
       "       [0.15819801, 0.78947368],\n",
       "       [0.52645364, 0.        ],\n",
       "       [0.15819801, 0.89473684],\n",
       "       [0.68360398, 0.15789474],\n",
       "       [0.97904662, 0.73684211],\n",
       "       [0.36825563, 0.47368421],\n",
       "       [0.84337349, 0.78947368],\n",
       "       [0.        , 0.10526316],\n",
       "       [0.10529073, 0.63157895],\n",
       "       [0.68412782, 0.10526316],\n",
       "       [0.62964903, 0.68421053],\n",
       "       [0.6322682 , 1.        ],\n",
       "       [0.47354636, 0.15789474],\n",
       "       [0.36825563, 0.57894737],\n",
       "       [0.94133054, 0.15789474],\n",
       "       [0.78889471, 0.10526316],\n",
       "       [0.47354636, 0.52631579],\n",
       "       [0.15819801, 0.73684211],\n",
       "       [0.62964903, 0.15789474],\n",
       "       [0.99895233, 0.31578947],\n",
       "       [0.21058146, 0.36842105],\n",
       "       [0.57831325, 1.        ],\n",
       "       [0.36825563, 0.15789474],\n",
       "       [0.84023049, 0.73684211],\n",
       "       [0.68360398, 0.31578947],\n",
       "       [0.73441592, 0.42105263],\n",
       "       [0.31587218, 0.84210526],\n",
       "       [0.42063908, 0.31578947],\n",
       "       [0.21058146, 0.15789474],\n",
       "       [0.88894709, 0.84210526],\n",
       "       [0.36825563, 0.78947368],\n",
       "       [0.68308015, 1.        ],\n",
       "       [0.31587218, 0.31578947],\n",
       "       [0.47302252, 0.78947368],\n",
       "       [0.68412782, 0.26315789],\n",
       "       [0.924044  , 0.47368421],\n",
       "       [0.2629649 , 0.21052632],\n",
       "       [0.36825563, 0.68421053],\n",
       "       [0.73651126, 0.10526316],\n",
       "       [0.57883709, 0.31578947],\n",
       "       [0.05290728, 0.26315789],\n",
       "       [0.47354636, 0.84210526],\n",
       "       [0.15819801, 0.21052632],\n",
       "       [0.        , 0.78947368],\n",
       "       [0.73598743, 0.89473684],\n",
       "       [0.2629649 , 0.47368421],\n",
       "       [0.84127816, 0.84210526],\n",
       "       [0.78784704, 0.42105263],\n",
       "       [0.05290728, 0.31578947],\n",
       "       [0.15819801, 0.84210526],\n",
       "       [0.10529073, 0.15789474],\n",
       "       [0.73598743, 0.        ],\n",
       "       [0.        , 0.15789474],\n",
       "       [0.94604505, 0.42105263],\n",
       "       [0.10529073, 0.47368421],\n",
       "       [0.99109481, 0.42105263],\n",
       "       [0.05290728, 0.52631579],\n",
       "       [0.31587218, 0.05263158],\n",
       "       [0.2629649 , 0.78947368],\n",
       "       [0.94342588, 0.68421053],\n",
       "       [0.52592981, 0.84210526],\n",
       "       [0.68412782, 0.21052632],\n",
       "       [0.52592981, 0.47368421],\n",
       "       [0.78837087, 0.84210526],\n",
       "       [0.        , 0.05263158],\n",
       "       [0.78889471, 0.68421053],\n",
       "       [0.05290728, 0.10526316],\n",
       "       [0.78837087, 0.89473684],\n",
       "       [0.78889471, 1.        ],\n",
       "       [0.        , 0.36842105],\n",
       "       [0.36877947, 0.05263158],\n",
       "       [0.21058146, 0.31578947],\n",
       "       [0.89313777, 0.63157895],\n",
       "       [0.63122053, 0.05263158],\n",
       "       [0.52592981, 0.78947368],\n",
       "       [0.15819801, 0.42105263],\n",
       "       [0.10529073, 0.52631579],\n",
       "       [0.36825563, 0.84210526],\n",
       "       [0.63122053, 0.63157895],\n",
       "       [0.89470927, 0.89473684],\n",
       "       [0.63017287, 0.73684211],\n",
       "       [0.68360398, 0.84210526],\n",
       "       [0.42116291, 0.94736842],\n",
       "       [0.73651126, 0.05263158],\n",
       "       [0.57831325, 0.89473684],\n",
       "       [0.15819801, 0.57894737],\n",
       "       [0.31587218, 0.47368421],\n",
       "       [0.36825563, 0.21052632],\n",
       "       [0.78103719, 0.78947368],\n",
       "       [0.31587218, 0.68421053],\n",
       "       [0.52592981, 0.15789474],\n",
       "       [0.9984285 , 0.68421053],\n",
       "       [0.63122053, 0.78947368],\n",
       "       [0.15819801, 0.63157895],\n",
       "       [1.        , 0.84210526],\n",
       "       [0.57831325, 0.78947368],\n",
       "       [0.05290728, 0.89473684]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling training set data\n",
    "Xtrain1_minmax = min_max_scaler1.fit_transform(Xtrain1)\n",
    "Xtrain1_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d632a673",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9984285 , 0.94736842],\n",
       "       [0.31587218, 0.26315789],\n",
       "       [0.84127816, 0.42105263],\n",
       "       [0.84180199, 1.        ],\n",
       "       [0.42063908, 0.63157895],\n",
       "       [0.8936616 , 0.10526316],\n",
       "       [0.47354636, 0.89473684],\n",
       "       [0.73493976, 0.57894737],\n",
       "       [0.05290728, 0.47368421],\n",
       "       [0.7370351 , 0.21052632],\n",
       "       [0.42011524, 0.73684211],\n",
       "       [0.93085385, 0.63157895],\n",
       "       [0.47354636, 0.42105263],\n",
       "       [0.84127816, 0.21052632],\n",
       "       [0.83551598, 0.05263158],\n",
       "       [0.57831325, 0.36842105],\n",
       "       [0.94552122, 0.57894737],\n",
       "       [0.        , 0.26315789],\n",
       "       [0.15819801, 0.94736842],\n",
       "       [0.57831325, 0.15789474],\n",
       "       [0.31639602, 0.10526316],\n",
       "       [0.63174437, 0.10526316],\n",
       "       [0.9984285 , 0.10526316],\n",
       "       [0.52645364, 0.73684211],\n",
       "       [0.        , 0.89473684],\n",
       "       [0.21058146, 0.63157895],\n",
       "       [0.94552122, 0.31578947],\n",
       "       [0.52592981, 0.05263158],\n",
       "       [0.94237821, 0.05263158],\n",
       "       [0.52488214, 0.36842105],\n",
       "       [0.21058146, 0.05263158],\n",
       "       [0.        , 0.21052632],\n",
       "       [0.42116291, 0.26315789],\n",
       "       [0.68308015, 0.78947368],\n",
       "       [0.        , 0.31578947],\n",
       "       [0.21058146, 0.        ],\n",
       "       [0.10529073, 0.94736842],\n",
       "       [0.26348874, 0.10526316],\n",
       "       [0.99423782, 0.89473684],\n",
       "       [0.31587218, 1.        ],\n",
       "       [0.68360398, 0.42105263],\n",
       "       [0.52592981, 0.21052632],\n",
       "       [0.42116291, 0.57894737],\n",
       "       [0.15819801, 0.10526316],\n",
       "       [0.99738083, 0.26315789],\n",
       "       [0.21058146, 0.26315789],\n",
       "       [0.93818753, 0.47368421],\n",
       "       [0.78837087, 0.36842105],\n",
       "       [0.7841802 , 0.73684211],\n",
       "       [0.68308015, 0.94736842],\n",
       "       [0.15819801, 0.26315789],\n",
       "       [0.57778942, 0.26315789],\n",
       "       [0.57831325, 0.47368421],\n",
       "       [0.        , 0.94736842],\n",
       "       [0.73598743, 0.84210526],\n",
       "       [0.73546359, 0.31578947],\n",
       "       [0.68308015, 0.63157895],\n",
       "       [0.26348874, 0.89473684],\n",
       "       [0.63122053, 0.94736842],\n",
       "       [0.94499738, 0.        ],\n",
       "       [0.2629649 , 0.36842105],\n",
       "       [0.15819801, 0.36842105],\n",
       "       [0.57831325, 0.73684211],\n",
       "       [0.63122053, 0.31578947],\n",
       "       [0.89104243, 0.73684211],\n",
       "       [0.57883709, 0.10526316],\n",
       "       [0.26348874, 0.31578947],\n",
       "       [0.78679937, 0.52631579],\n",
       "       [0.68308015, 0.52631579],\n",
       "       [0.78732321, 0.63157895],\n",
       "       [0.52592981, 0.94736842],\n",
       "       [0.21058146, 0.78947368],\n",
       "       [0.88737559, 0.52631579],\n",
       "       [0.98428497, 0.36842105],\n",
       "       [0.        , 0.57894737],\n",
       "       [0.89104243, 0.36842105],\n",
       "       [0.10529073, 0.05263158],\n",
       "       [0.47407019, 0.        ],\n",
       "       [0.31639602, 0.63157895],\n",
       "       [0.57831325, 0.21052632]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the same scaling to the test set data\n",
    "Xtest1_minmax = min_max_scaler1.transform(Xtest1)\n",
    "Xtest1_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa52ed0a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Xtest1_tensor = torch.from_numpy(Xtest1_minmax).type(torch.float32)\n",
    "Ytest1_tensor = torch.from_numpy(Ytest1).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e5b318ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9984, 0.9474],\n",
       "        [0.3159, 0.2632],\n",
       "        [0.8413, 0.4211],\n",
       "        [0.8418, 1.0000],\n",
       "        [0.4206, 0.6316],\n",
       "        [0.8937, 0.1053],\n",
       "        [0.4735, 0.8947],\n",
       "        [0.7349, 0.5789],\n",
       "        [0.0529, 0.4737],\n",
       "        [0.7370, 0.2105],\n",
       "        [0.4201, 0.7368],\n",
       "        [0.9309, 0.6316],\n",
       "        [0.4735, 0.4211],\n",
       "        [0.8413, 0.2105],\n",
       "        [0.8355, 0.0526],\n",
       "        [0.5783, 0.3684],\n",
       "        [0.9455, 0.5789],\n",
       "        [0.0000, 0.2632],\n",
       "        [0.1582, 0.9474],\n",
       "        [0.5783, 0.1579],\n",
       "        [0.3164, 0.1053],\n",
       "        [0.6317, 0.1053],\n",
       "        [0.9984, 0.1053],\n",
       "        [0.5265, 0.7368],\n",
       "        [0.0000, 0.8947],\n",
       "        [0.2106, 0.6316],\n",
       "        [0.9455, 0.3158],\n",
       "        [0.5259, 0.0526],\n",
       "        [0.9424, 0.0526],\n",
       "        [0.5249, 0.3684],\n",
       "        [0.2106, 0.0526],\n",
       "        [0.0000, 0.2105],\n",
       "        [0.4212, 0.2632],\n",
       "        [0.6831, 0.7895],\n",
       "        [0.0000, 0.3158],\n",
       "        [0.2106, 0.0000],\n",
       "        [0.1053, 0.9474],\n",
       "        [0.2635, 0.1053],\n",
       "        [0.9942, 0.8947],\n",
       "        [0.3159, 1.0000],\n",
       "        [0.6836, 0.4211],\n",
       "        [0.5259, 0.2105],\n",
       "        [0.4212, 0.5789],\n",
       "        [0.1582, 0.1053],\n",
       "        [0.9974, 0.2632],\n",
       "        [0.2106, 0.2632],\n",
       "        [0.9382, 0.4737],\n",
       "        [0.7884, 0.3684],\n",
       "        [0.7842, 0.7368],\n",
       "        [0.6831, 0.9474],\n",
       "        [0.1582, 0.2632],\n",
       "        [0.5778, 0.2632],\n",
       "        [0.5783, 0.4737],\n",
       "        [0.0000, 0.9474],\n",
       "        [0.7360, 0.8421],\n",
       "        [0.7355, 0.3158],\n",
       "        [0.6831, 0.6316],\n",
       "        [0.2635, 0.8947],\n",
       "        [0.6312, 0.9474],\n",
       "        [0.9450, 0.0000],\n",
       "        [0.2630, 0.3684],\n",
       "        [0.1582, 0.3684],\n",
       "        [0.5783, 0.7368],\n",
       "        [0.6312, 0.3158],\n",
       "        [0.8910, 0.7368],\n",
       "        [0.5788, 0.1053],\n",
       "        [0.2635, 0.3158],\n",
       "        [0.7868, 0.5263],\n",
       "        [0.6831, 0.5263],\n",
       "        [0.7873, 0.6316],\n",
       "        [0.5259, 0.9474],\n",
       "        [0.2106, 0.7895],\n",
       "        [0.8874, 0.5263],\n",
       "        [0.9843, 0.3684],\n",
       "        [0.0000, 0.5789],\n",
       "        [0.8910, 0.3684],\n",
       "        [0.1053, 0.0526],\n",
       "        [0.4741, 0.0000],\n",
       "        [0.3164, 0.6316],\n",
       "        [0.5783, 0.2105]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest1_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "832155d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# batch the training dataset\n",
    "# prepare dataset\n",
    "class BlockChainDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.len = data.shape[0]\n",
    "        self.x_data = torch.from_numpy(data).type(torch.float32)\n",
    "        self.y_data = torch.from_numpy(label).type(torch.float32)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a69f3f89",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = BlockChainDataset(Xtrain1_minmax, Ytrain1)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "14cf2abc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af6478ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# design model using class\n",
    "class Lat(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lat, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "model = Lat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f533b5c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# net = nn.Sequential(\n",
    "#     nn.Linear(2, 64), nn.BatchNorm1d(64), nn.Dropout(p=0.5), nn.ReLU(),\n",
    "#     nn.Linear(64, 64), nn.BatchNorm1d(64), nn.Dropout(p=0.5), nn.ReLU(),\n",
    "#     nn.Linear(64, 1))\n",
    "\n",
    "\n",
    "# model = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1044383",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# construct loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f46b24ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# traning cycle forward, backward, update\n",
    "def train(epoch):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        y_pred = model(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * labels.shape[0]\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('epoch:', epoch + 1, 'train_loss:', train_loss / len(Xtrain1))\n",
    "        \n",
    "\n",
    "def test():\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(Xtest1_tensor)\n",
    "        loss = criterion(y_pred, Ytest1_tensor)\n",
    "        print('test_loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f8827317",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 train_loss: 0.010410415939986706\n",
      "test_loss: tensor(0.0628)\n",
      "epoch: 20 train_loss: 0.011669123591855169\n",
      "test_loss: tensor(0.0604)\n",
      "epoch: 30 train_loss: 0.015119113959372044\n",
      "test_loss: tensor(0.0691)\n",
      "epoch: 40 train_loss: 0.011270327679812907\n",
      "test_loss: tensor(0.0685)\n",
      "epoch: 50 train_loss: 0.01103515112772584\n",
      "test_loss: tensor(0.0717)\n",
      "epoch: 60 train_loss: 0.008162179286591709\n",
      "test_loss: tensor(0.0713)\n",
      "epoch: 70 train_loss: 0.009942533797584474\n",
      "test_loss: tensor(0.0734)\n",
      "epoch: 80 train_loss: 0.009139689500443637\n",
      "test_loss: tensor(0.0722)\n",
      "epoch: 90 train_loss: 0.008866382297128439\n",
      "test_loss: tensor(0.0691)\n",
      "epoch: 100 train_loss: 0.008879442233592272\n",
      "test_loss: tensor(0.0700)\n",
      "epoch: 110 train_loss: 0.009302773722447455\n",
      "test_loss: tensor(0.0691)\n",
      "epoch: 120 train_loss: 0.010284598218277097\n",
      "test_loss: tensor(0.0661)\n",
      "epoch: 130 train_loss: 0.010853838548064232\n",
      "test_loss: tensor(0.0670)\n",
      "epoch: 140 train_loss: 0.010591573407873511\n",
      "test_loss: tensor(0.0630)\n",
      "epoch: 150 train_loss: 0.01246819889638573\n",
      "test_loss: tensor(0.0659)\n",
      "epoch: 160 train_loss: 0.009376307972706855\n",
      "test_loss: tensor(0.0645)\n",
      "epoch: 170 train_loss: 0.01049440768547356\n",
      "test_loss: tensor(0.0664)\n",
      "epoch: 180 train_loss: 0.0100470565026626\n",
      "test_loss: tensor(0.0636)\n",
      "epoch: 190 train_loss: 0.012205396452918649\n",
      "test_loss: tensor(0.0673)\n",
      "epoch: 200 train_loss: 0.0105148232774809\n",
      "test_loss: tensor(0.0698)\n",
      "epoch: 210 train_loss: 0.008767897030338645\n",
      "test_loss: tensor(0.0706)\n",
      "epoch: 220 train_loss: 0.009724721498787402\n",
      "test_loss: tensor(0.0757)\n",
      "epoch: 230 train_loss: 0.009344524005427957\n",
      "test_loss: tensor(0.0733)\n",
      "epoch: 240 train_loss: 0.009342925203964115\n",
      "test_loss: tensor(0.0743)\n",
      "epoch: 250 train_loss: 0.008741316315717996\n",
      "test_loss: tensor(0.0652)\n",
      "epoch: 260 train_loss: 0.011028502765111626\n",
      "test_loss: tensor(0.0607)\n",
      "epoch: 270 train_loss: 0.011623494466766715\n",
      "test_loss: tensor(0.0642)\n",
      "epoch: 280 train_loss: 0.010434364899992943\n",
      "test_loss: tensor(0.0810)\n",
      "epoch: 290 train_loss: 0.01259247725829482\n",
      "test_loss: tensor(0.0711)\n",
      "epoch: 300 train_loss: 0.012755184108391404\n",
      "test_loss: tensor(0.0680)\n",
      "epoch: 310 train_loss: 0.010059175733476877\n",
      "test_loss: tensor(0.0678)\n",
      "epoch: 320 train_loss: 0.013373520830646158\n",
      "test_loss: tensor(0.0703)\n",
      "epoch: 330 train_loss: 0.009552171733230353\n",
      "test_loss: tensor(0.0672)\n",
      "epoch: 340 train_loss: 0.008719607302919031\n",
      "test_loss: tensor(0.0674)\n",
      "epoch: 350 train_loss: 0.008701654616743326\n",
      "test_loss: tensor(0.0685)\n",
      "epoch: 360 train_loss: 0.011852600146085024\n",
      "test_loss: tensor(0.0675)\n",
      "epoch: 370 train_loss: 0.008538637356832624\n",
      "test_loss: tensor(0.0656)\n",
      "epoch: 380 train_loss: 0.010086780320852995\n",
      "test_loss: tensor(0.0645)\n",
      "epoch: 390 train_loss: 0.010950773302465678\n",
      "test_loss: tensor(0.0654)\n",
      "epoch: 400 train_loss: 0.01186273987405002\n",
      "test_loss: tensor(0.0638)\n",
      "epoch: 410 train_loss: 0.010552319139242173\n",
      "test_loss: tensor(0.0703)\n",
      "epoch: 420 train_loss: 0.010021545877680182\n",
      "test_loss: tensor(0.0740)\n",
      "epoch: 430 train_loss: 0.008825785084627568\n",
      "test_loss: tensor(0.0801)\n",
      "epoch: 440 train_loss: 0.009576855436898768\n",
      "test_loss: tensor(0.0709)\n",
      "epoch: 450 train_loss: 0.01068951808847487\n",
      "test_loss: tensor(0.0716)\n",
      "epoch: 460 train_loss: 0.00868192333728075\n",
      "test_loss: tensor(0.0679)\n",
      "epoch: 470 train_loss: 0.008931666798889636\n",
      "test_loss: tensor(0.0709)\n",
      "epoch: 480 train_loss: 0.011889559170231222\n",
      "test_loss: tensor(0.0733)\n",
      "epoch: 490 train_loss: 0.010099572176113725\n",
      "test_loss: tensor(0.0678)\n",
      "epoch: 500 train_loss: 0.009833696740679443\n",
      "test_loss: tensor(0.0674)\n",
      "epoch: 510 train_loss: 0.009677764377556741\n",
      "test_loss: tensor(0.0621)\n",
      "epoch: 520 train_loss: 0.0115585976280272\n",
      "test_loss: tensor(0.0655)\n",
      "epoch: 530 train_loss: 0.008580070547759533\n",
      "test_loss: tensor(0.0654)\n",
      "epoch: 540 train_loss: 0.010028955829329788\n",
      "test_loss: tensor(0.0613)\n",
      "epoch: 550 train_loss: 0.009809642867185175\n",
      "test_loss: tensor(0.0642)\n",
      "epoch: 560 train_loss: 0.011473436979576946\n",
      "test_loss: tensor(0.0616)\n",
      "epoch: 570 train_loss: 0.010735908476635813\n",
      "test_loss: tensor(0.0641)\n",
      "epoch: 580 train_loss: 0.012127005960792303\n",
      "test_loss: tensor(0.0703)\n",
      "epoch: 590 train_loss: 0.008315676171332598\n",
      "test_loss: tensor(0.0706)\n",
      "epoch: 600 train_loss: 0.008921144856140017\n",
      "test_loss: tensor(0.0694)\n",
      "epoch: 610 train_loss: 0.009513790649361909\n",
      "test_loss: tensor(0.0731)\n",
      "epoch: 620 train_loss: 0.01079386044293642\n",
      "test_loss: tensor(0.0731)\n",
      "epoch: 630 train_loss: 0.010686905402690173\n",
      "test_loss: tensor(0.0657)\n",
      "epoch: 640 train_loss: 0.009997829841449857\n",
      "test_loss: tensor(0.0633)\n",
      "epoch: 650 train_loss: 0.010500267939642071\n",
      "test_loss: tensor(0.0686)\n",
      "epoch: 660 train_loss: 0.008185419673100113\n",
      "test_loss: tensor(0.0668)\n",
      "epoch: 670 train_loss: 0.007950964476913213\n",
      "test_loss: tensor(0.0656)\n",
      "epoch: 680 train_loss: 0.008893106505274773\n",
      "test_loss: tensor(0.0646)\n",
      "epoch: 690 train_loss: 0.008428880851715803\n",
      "test_loss: tensor(0.0660)\n",
      "epoch: 700 train_loss: 0.008323779259808362\n",
      "test_loss: tensor(0.0638)\n",
      "epoch: 710 train_loss: 0.009463230543769897\n",
      "test_loss: tensor(0.0622)\n",
      "epoch: 720 train_loss: 0.011694123968482018\n",
      "test_loss: tensor(0.0653)\n",
      "epoch: 730 train_loss: 0.0144011948723346\n",
      "test_loss: tensor(0.0699)\n",
      "epoch: 740 train_loss: 0.011584334820508958\n",
      "test_loss: tensor(0.0791)\n",
      "epoch: 750 train_loss: 0.008943935623392463\n",
      "test_loss: tensor(0.0734)\n",
      "epoch: 760 train_loss: 0.008224593219347298\n",
      "test_loss: tensor(0.0679)\n",
      "epoch: 770 train_loss: 0.00812447927892208\n",
      "test_loss: tensor(0.0666)\n",
      "epoch: 780 train_loss: 0.007664771704003215\n",
      "test_loss: tensor(0.0645)\n",
      "epoch: 790 train_loss: 0.008189371461048723\n",
      "test_loss: tensor(0.0644)\n",
      "epoch: 800 train_loss: 0.007812796998769045\n",
      "test_loss: tensor(0.0682)\n",
      "epoch: 810 train_loss: 0.0085027024615556\n",
      "test_loss: tensor(0.0648)\n",
      "epoch: 820 train_loss: 0.010663857660256326\n",
      "test_loss: tensor(0.0608)\n",
      "epoch: 830 train_loss: 0.010631452407687902\n",
      "test_loss: tensor(0.0796)\n",
      "epoch: 840 train_loss: 0.015047462843358516\n",
      "test_loss: tensor(0.0704)\n",
      "epoch: 850 train_loss: 0.010558461933396757\n",
      "test_loss: tensor(0.0663)\n",
      "epoch: 860 train_loss: 0.01092928289435804\n",
      "test_loss: tensor(0.0706)\n",
      "epoch: 870 train_loss: 0.009704829659312963\n",
      "test_loss: tensor(0.0675)\n",
      "epoch: 880 train_loss: 0.008773513650521637\n",
      "test_loss: tensor(0.0681)\n",
      "epoch: 890 train_loss: 0.009700551978312433\n",
      "test_loss: tensor(0.0697)\n",
      "epoch: 900 train_loss: 0.010910622077062726\n",
      "test_loss: tensor(0.0686)\n",
      "epoch: 910 train_loss: 0.008289635903201998\n",
      "test_loss: tensor(0.0659)\n",
      "epoch: 920 train_loss: 0.00790262413211167\n",
      "test_loss: tensor(0.0639)\n",
      "epoch: 930 train_loss: 0.007884397055022418\n",
      "test_loss: tensor(0.0625)\n",
      "epoch: 940 train_loss: 0.00872567929327488\n",
      "test_loss: tensor(0.0636)\n",
      "epoch: 950 train_loss: 0.01132936323992908\n",
      "test_loss: tensor(0.0596)\n",
      "epoch: 960 train_loss: 0.01015861015766859\n",
      "test_loss: tensor(0.0674)\n",
      "epoch: 970 train_loss: 0.008760688314214348\n",
      "test_loss: tensor(0.0681)\n",
      "epoch: 980 train_loss: 0.008489258005283773\n",
      "test_loss: tensor(0.0670)\n",
      "epoch: 990 train_loss: 0.008974708942696452\n",
      "test_loss: tensor(0.0622)\n",
      "epoch: 1000 train_loss: 0.010131337773054838\n",
      "test_loss: tensor(0.0600)\n",
      "epoch: 1010 train_loss: 0.012213398050516844\n",
      "test_loss: tensor(0.0794)\n",
      "epoch: 1020 train_loss: 0.011275396961718798\n",
      "test_loss: tensor(0.0739)\n",
      "epoch: 1030 train_loss: 0.010435485700145363\n",
      "test_loss: tensor(0.0658)\n",
      "epoch: 1040 train_loss: 0.010533260647207499\n",
      "test_loss: tensor(0.0678)\n",
      "epoch: 1050 train_loss: 0.01069411956705153\n",
      "test_loss: tensor(0.0677)\n",
      "epoch: 1060 train_loss: 0.009058412024751305\n",
      "test_loss: tensor(0.0637)\n",
      "epoch: 1070 train_loss: 0.009517194284126163\n",
      "test_loss: tensor(0.0634)\n",
      "epoch: 1080 train_loss: 0.010868637589737773\n",
      "test_loss: tensor(0.0629)\n",
      "epoch: 1090 train_loss: 0.009539646096527576\n",
      "test_loss: tensor(0.0639)\n",
      "epoch: 1100 train_loss: 0.008496041968464851\n",
      "test_loss: tensor(0.0646)\n",
      "epoch: 1110 train_loss: 0.009326680819503963\n",
      "test_loss: tensor(0.0656)\n",
      "epoch: 1120 train_loss: 0.010627647326327861\n",
      "test_loss: tensor(0.0655)\n",
      "epoch: 1130 train_loss: 0.010019112238660454\n",
      "test_loss: tensor(0.0649)\n",
      "epoch: 1140 train_loss: 0.011645640945062041\n",
      "test_loss: tensor(0.0650)\n",
      "epoch: 1150 train_loss: 0.011609404743649066\n",
      "test_loss: tensor(0.0643)\n",
      "epoch: 1160 train_loss: 0.010361117497086524\n",
      "test_loss: tensor(0.0681)\n",
      "epoch: 1170 train_loss: 0.009125857264734805\n",
      "test_loss: tensor(0.0638)\n",
      "epoch: 1180 train_loss: 0.007289060670882463\n",
      "test_loss: tensor(0.0639)\n",
      "epoch: 1190 train_loss: 0.007761962013319135\n",
      "test_loss: tensor(0.0632)\n",
      "epoch: 1200 train_loss: 0.007605378166772425\n",
      "test_loss: tensor(0.0683)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1210 train_loss: 0.00902127088047564\n",
      "test_loss: tensor(0.0676)\n",
      "epoch: 1220 train_loss: 0.011413412494584918\n",
      "test_loss: tensor(0.0644)\n",
      "epoch: 1230 train_loss: 0.008963711792603135\n",
      "test_loss: tensor(0.0732)\n",
      "epoch: 1240 train_loss: 0.010761992167681455\n",
      "test_loss: tensor(0.0736)\n",
      "epoch: 1250 train_loss: 0.010085003031417728\n",
      "test_loss: tensor(0.0695)\n",
      "epoch: 1260 train_loss: 0.015766722802072763\n",
      "test_loss: tensor(0.0657)\n",
      "epoch: 1270 train_loss: 0.010102657694369554\n",
      "test_loss: tensor(0.0741)\n",
      "epoch: 1280 train_loss: 0.011593162966892123\n",
      "test_loss: tensor(0.0785)\n",
      "epoch: 1290 train_loss: 0.010319568123668433\n",
      "test_loss: tensor(0.0728)\n",
      "epoch: 1300 train_loss: 0.014261350594460964\n",
      "test_loss: tensor(0.0699)\n",
      "epoch: 1310 train_loss: 0.010295926709659398\n",
      "test_loss: tensor(0.0680)\n",
      "epoch: 1320 train_loss: 0.00853138065431267\n",
      "test_loss: tensor(0.0624)\n",
      "epoch: 1330 train_loss: 0.009388642082922161\n",
      "test_loss: tensor(0.0659)\n",
      "epoch: 1340 train_loss: 0.010804539127275348\n",
      "test_loss: tensor(0.0626)\n",
      "epoch: 1350 train_loss: 0.010030566505156458\n",
      "test_loss: tensor(0.0661)\n",
      "epoch: 1360 train_loss: 0.009350157552398741\n",
      "test_loss: tensor(0.0636)\n",
      "epoch: 1370 train_loss: 0.012131078448146582\n",
      "test_loss: tensor(0.0699)\n",
      "epoch: 1380 train_loss: 0.012044336064718664\n",
      "test_loss: tensor(0.0712)\n",
      "epoch: 1390 train_loss: 0.009800491482019424\n",
      "test_loss: tensor(0.0726)\n",
      "epoch: 1400 train_loss: 0.009897465002723038\n",
      "test_loss: tensor(0.0705)\n",
      "epoch: 1410 train_loss: 0.008275968814268708\n",
      "test_loss: tensor(0.0700)\n",
      "epoch: 1420 train_loss: 0.008375336625613272\n",
      "test_loss: tensor(0.0701)\n",
      "epoch: 1430 train_loss: 0.008597044926136731\n",
      "test_loss: tensor(0.0713)\n",
      "epoch: 1440 train_loss: 0.01008624560199678\n",
      "test_loss: tensor(0.0744)\n",
      "epoch: 1450 train_loss: 0.009614430647343398\n",
      "test_loss: tensor(0.0663)\n",
      "epoch: 1460 train_loss: 0.01008214526809752\n",
      "test_loss: tensor(0.0607)\n",
      "epoch: 1470 train_loss: 0.012191421212628483\n",
      "test_loss: tensor(0.0646)\n",
      "epoch: 1480 train_loss: 0.011015511280857027\n",
      "test_loss: tensor(0.0655)\n",
      "epoch: 1490 train_loss: 0.010787666076794267\n",
      "test_loss: tensor(0.0754)\n",
      "epoch: 1500 train_loss: 0.009060554625466465\n",
      "test_loss: tensor(0.0752)\n",
      "epoch: 1510 train_loss: 0.0091385739389807\n",
      "test_loss: tensor(0.0662)\n",
      "epoch: 1520 train_loss: 0.009253918705508114\n",
      "test_loss: tensor(0.0677)\n",
      "epoch: 1530 train_loss: 0.009816494584083558\n",
      "test_loss: tensor(0.0671)\n",
      "epoch: 1540 train_loss: 0.009724872978404164\n",
      "test_loss: tensor(0.0681)\n",
      "epoch: 1550 train_loss: 0.009012609533965588\n",
      "test_loss: tensor(0.0665)\n",
      "epoch: 1560 train_loss: 0.00873879378195852\n",
      "test_loss: tensor(0.0604)\n",
      "epoch: 1570 train_loss: 0.008780605136416852\n",
      "test_loss: tensor(0.0634)\n",
      "epoch: 1580 train_loss: 0.010013334057293832\n",
      "test_loss: tensor(0.0660)\n",
      "epoch: 1590 train_loss: 0.011142454762011766\n",
      "test_loss: tensor(0.0621)\n",
      "epoch: 1600 train_loss: 0.011813134048134088\n",
      "test_loss: tensor(0.0645)\n",
      "epoch: 1610 train_loss: 0.009561875159852207\n",
      "test_loss: tensor(0.0638)\n",
      "epoch: 1620 train_loss: 0.008321670116856694\n",
      "test_loss: tensor(0.0633)\n",
      "epoch: 1630 train_loss: 0.009532136493362486\n",
      "test_loss: tensor(0.0613)\n",
      "epoch: 1640 train_loss: 0.00928022088482976\n",
      "test_loss: tensor(0.0623)\n",
      "epoch: 1650 train_loss: 0.015858398424461483\n",
      "test_loss: tensor(0.0792)\n",
      "epoch: 1660 train_loss: 0.0088424033485353\n",
      "test_loss: tensor(0.0726)\n",
      "epoch: 1670 train_loss: 0.009213068569079042\n",
      "test_loss: tensor(0.0702)\n",
      "epoch: 1680 train_loss: 0.010074108373373747\n",
      "test_loss: tensor(0.0714)\n",
      "epoch: 1690 train_loss: 0.010252830479294062\n",
      "test_loss: tensor(0.0652)\n",
      "epoch: 1700 train_loss: 0.008365085441619157\n",
      "test_loss: tensor(0.0640)\n",
      "epoch: 1710 train_loss: 0.009862235747277737\n",
      "test_loss: tensor(0.0631)\n",
      "epoch: 1720 train_loss: 0.011143193207681179\n",
      "test_loss: tensor(0.0601)\n",
      "epoch: 1730 train_loss: 0.010394606948830187\n",
      "test_loss: tensor(0.0700)\n",
      "epoch: 1740 train_loss: 0.011241459520533681\n",
      "test_loss: tensor(0.0776)\n",
      "epoch: 1750 train_loss: 0.009694100031629205\n",
      "test_loss: tensor(0.0693)\n",
      "epoch: 1760 train_loss: 0.007997441850602627\n",
      "test_loss: tensor(0.0736)\n",
      "epoch: 1770 train_loss: 0.008590554958209396\n",
      "test_loss: tensor(0.0701)\n",
      "epoch: 1780 train_loss: 0.01028026402927935\n",
      "test_loss: tensor(0.0686)\n",
      "epoch: 1790 train_loss: 0.008736906852573157\n",
      "test_loss: tensor(0.0644)\n",
      "epoch: 1800 train_loss: 0.008524845214560628\n",
      "test_loss: tensor(0.0630)\n",
      "epoch: 1810 train_loss: 0.01259212619625032\n",
      "test_loss: tensor(0.0639)\n",
      "epoch: 1820 train_loss: 0.00980563557241112\n",
      "test_loss: tensor(0.0658)\n",
      "epoch: 1830 train_loss: 0.012475698860362172\n",
      "test_loss: tensor(0.0678)\n",
      "epoch: 1840 train_loss: 0.011345360171981156\n",
      "test_loss: tensor(0.0676)\n",
      "epoch: 1850 train_loss: 0.008718648599460721\n",
      "test_loss: tensor(0.0723)\n",
      "epoch: 1860 train_loss: 0.008600070467218756\n",
      "test_loss: tensor(0.0712)\n",
      "epoch: 1870 train_loss: 0.009486023057252168\n",
      "test_loss: tensor(0.0726)\n",
      "epoch: 1880 train_loss: 0.008233331050723791\n",
      "test_loss: tensor(0.0763)\n",
      "epoch: 1890 train_loss: 0.008512459765188395\n",
      "test_loss: tensor(0.0663)\n",
      "epoch: 1900 train_loss: 0.009353317273780703\n",
      "test_loss: tensor(0.0656)\n",
      "epoch: 1910 train_loss: 0.01242939152289182\n",
      "test_loss: tensor(0.0641)\n",
      "epoch: 1920 train_loss: 0.01277080997824669\n",
      "test_loss: tensor(0.0680)\n",
      "epoch: 1930 train_loss: 0.011622448940761388\n",
      "test_loss: tensor(0.0711)\n",
      "epoch: 1940 train_loss: 0.010829412564635276\n",
      "test_loss: tensor(0.0745)\n",
      "epoch: 1950 train_loss: 0.00921253957785666\n",
      "test_loss: tensor(0.0744)\n",
      "epoch: 1960 train_loss: 0.008009048574604095\n",
      "test_loss: tensor(0.0697)\n",
      "epoch: 1970 train_loss: 0.008393286191858352\n",
      "test_loss: tensor(0.0726)\n",
      "epoch: 1980 train_loss: 0.009217090951278806\n",
      "test_loss: tensor(0.0734)\n",
      "epoch: 1990 train_loss: 0.00964054292999208\n",
      "test_loss: tensor(0.0683)\n",
      "epoch: 2000 train_loss: 0.009530509915202856\n",
      "test_loss: tensor(0.0724)\n",
      "epoch: 2010 train_loss: 0.008196787536144256\n",
      "test_loss: tensor(0.0651)\n",
      "epoch: 2020 train_loss: 0.0093716453993693\n",
      "test_loss: tensor(0.0649)\n",
      "epoch: 2030 train_loss: 0.008952735317870974\n",
      "test_loss: tensor(0.0650)\n",
      "epoch: 2040 train_loss: 0.011942276055924595\n",
      "test_loss: tensor(0.0628)\n",
      "epoch: 2050 train_loss: 0.011256032693199813\n",
      "test_loss: tensor(0.0647)\n",
      "epoch: 2060 train_loss: 0.009700226178392769\n",
      "test_loss: tensor(0.0650)\n",
      "epoch: 2070 train_loss: 0.011637063999660312\n",
      "test_loss: tensor(0.0714)\n",
      "epoch: 2080 train_loss: 0.009208322409540415\n",
      "test_loss: tensor(0.0712)\n",
      "epoch: 2090 train_loss: 0.008149617561139166\n",
      "test_loss: tensor(0.0703)\n",
      "epoch: 2100 train_loss: 0.008476471994072199\n",
      "test_loss: tensor(0.0752)\n",
      "epoch: 2110 train_loss: 0.010594474337995052\n",
      "test_loss: tensor(0.0727)\n",
      "epoch: 2120 train_loss: 0.008266637334600091\n",
      "test_loss: tensor(0.0668)\n",
      "epoch: 2130 train_loss: 0.00969624174758792\n",
      "test_loss: tensor(0.0665)\n",
      "epoch: 2140 train_loss: 0.010939160780981182\n",
      "test_loss: tensor(0.0641)\n",
      "epoch: 2150 train_loss: 0.014280781615525484\n",
      "test_loss: tensor(0.0658)\n",
      "epoch: 2160 train_loss: 0.010495616751722992\n",
      "test_loss: tensor(0.0665)\n",
      "epoch: 2170 train_loss: 0.010826543299481272\n",
      "test_loss: tensor(0.0749)\n",
      "epoch: 2180 train_loss: 0.009202079847455025\n",
      "test_loss: tensor(0.0691)\n",
      "epoch: 2190 train_loss: 0.010733971931040287\n",
      "test_loss: tensor(0.0745)\n",
      "epoch: 2200 train_loss: 0.010846873838454485\n",
      "test_loss: tensor(0.0703)\n",
      "epoch: 2210 train_loss: 0.009268492972478271\n",
      "test_loss: tensor(0.0639)\n",
      "epoch: 2220 train_loss: 0.00834164263214916\n",
      "test_loss: tensor(0.0645)\n",
      "epoch: 2230 train_loss: 0.008712929626926779\n",
      "test_loss: tensor(0.0630)\n",
      "epoch: 2240 train_loss: 0.008793157688342036\n",
      "test_loss: tensor(0.0641)\n",
      "epoch: 2250 train_loss: 0.009567375178448856\n",
      "test_loss: tensor(0.0633)\n",
      "epoch: 2260 train_loss: 0.00993420728482306\n",
      "test_loss: tensor(0.0647)\n",
      "epoch: 2270 train_loss: 0.008689524536021054\n",
      "test_loss: tensor(0.0773)\n",
      "epoch: 2280 train_loss: 0.00865215582307428\n",
      "test_loss: tensor(0.0691)\n",
      "epoch: 2290 train_loss: 0.011771504674106837\n",
      "test_loss: tensor(0.0632)\n",
      "epoch: 2300 train_loss: 0.01283115684054792\n",
      "test_loss: tensor(0.0740)\n",
      "epoch: 2310 train_loss: 0.011062006046995521\n",
      "test_loss: tensor(0.0719)\n",
      "epoch: 2320 train_loss: 0.010848780209198595\n",
      "test_loss: tensor(0.0686)\n",
      "epoch: 2330 train_loss: 0.010590915568172932\n",
      "test_loss: tensor(0.0646)\n",
      "epoch: 2340 train_loss: 0.013565834378823638\n",
      "test_loss: tensor(0.0632)\n",
      "epoch: 2350 train_loss: 0.011077520065009594\n",
      "test_loss: tensor(0.0614)\n",
      "epoch: 2360 train_loss: 0.010553798428736627\n",
      "test_loss: tensor(0.0641)\n",
      "epoch: 2370 train_loss: 0.008999219303950668\n",
      "test_loss: tensor(0.0633)\n",
      "epoch: 2380 train_loss: 0.010783086158335209\n",
      "test_loss: tensor(0.0640)\n",
      "epoch: 2390 train_loss: 0.013009709864854812\n",
      "test_loss: tensor(0.0767)\n",
      "epoch: 2400 train_loss: 0.00986715154722333\n",
      "test_loss: tensor(0.0691)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2410 train_loss: 0.009333614353090524\n",
      "test_loss: tensor(0.0754)\n",
      "epoch: 2420 train_loss: 0.009414323884993792\n",
      "test_loss: tensor(0.0735)\n",
      "epoch: 2430 train_loss: 0.010064744716510176\n",
      "test_loss: tensor(0.0697)\n",
      "epoch: 2440 train_loss: 0.008271208102814852\n",
      "test_loss: tensor(0.0644)\n",
      "epoch: 2450 train_loss: 0.009251508093439043\n",
      "test_loss: tensor(0.0646)\n",
      "epoch: 2460 train_loss: 0.010524035105481744\n",
      "test_loss: tensor(0.0670)\n",
      "epoch: 2470 train_loss: 0.010686353291384875\n",
      "test_loss: tensor(0.0737)\n",
      "epoch: 2480 train_loss: 0.009937603888101876\n",
      "test_loss: tensor(0.0728)\n",
      "epoch: 2490 train_loss: 0.011664833407849073\n",
      "test_loss: tensor(0.0679)\n",
      "epoch: 2500 train_loss: 0.00876742082182318\n",
      "test_loss: tensor(0.0631)\n",
      "epoch: 2510 train_loss: 0.009821443073451518\n",
      "test_loss: tensor(0.0614)\n",
      "epoch: 2520 train_loss: 0.010613842005841433\n",
      "test_loss: tensor(0.0655)\n",
      "epoch: 2530 train_loss: 0.010294836317189037\n",
      "test_loss: tensor(0.0627)\n",
      "epoch: 2540 train_loss: 0.010605359287001192\n",
      "test_loss: tensor(0.0595)\n",
      "epoch: 2550 train_loss: 0.01258004056289792\n",
      "test_loss: tensor(0.0649)\n",
      "epoch: 2560 train_loss: 0.01141272708773613\n",
      "test_loss: tensor(0.0648)\n",
      "epoch: 2570 train_loss: 0.00978072565048933\n",
      "test_loss: tensor(0.0640)\n",
      "epoch: 2580 train_loss: 0.012797886924818157\n",
      "test_loss: tensor(0.0634)\n",
      "epoch: 2590 train_loss: 0.01012178563978523\n",
      "test_loss: tensor(0.0631)\n",
      "epoch: 2600 train_loss: 0.009565365896560252\n",
      "test_loss: tensor(0.0666)\n",
      "epoch: 2610 train_loss: 0.008801772072911263\n",
      "test_loss: tensor(0.0712)\n",
      "epoch: 2620 train_loss: 0.0086915104649961\n",
      "test_loss: tensor(0.0689)\n",
      "epoch: 2630 train_loss: 0.00845696865580976\n",
      "test_loss: tensor(0.0725)\n",
      "epoch: 2640 train_loss: 0.009451743867248296\n",
      "test_loss: tensor(0.0713)\n",
      "epoch: 2650 train_loss: 0.012789217568933964\n",
      "test_loss: tensor(0.0675)\n",
      "epoch: 2660 train_loss: 0.010410899715498089\n",
      "test_loss: tensor(0.0649)\n",
      "epoch: 2670 train_loss: 0.009199333540163935\n",
      "test_loss: tensor(0.0688)\n",
      "epoch: 2680 train_loss: 0.011928978678770363\n",
      "test_loss: tensor(0.0662)\n",
      "epoch: 2690 train_loss: 0.010292655299417674\n",
      "test_loss: tensor(0.0632)\n",
      "epoch: 2700 train_loss: 0.01144856158643961\n",
      "test_loss: tensor(0.0603)\n",
      "epoch: 2710 train_loss: 0.009815279114991426\n",
      "test_loss: tensor(0.0673)\n",
      "epoch: 2720 train_loss: 0.011256679357029498\n",
      "test_loss: tensor(0.0697)\n",
      "epoch: 2730 train_loss: 0.01055909190326929\n",
      "test_loss: tensor(0.0796)\n",
      "epoch: 2740 train_loss: 0.008974009426310658\n",
      "test_loss: tensor(0.0711)\n",
      "epoch: 2750 train_loss: 0.009370173700153828\n",
      "test_loss: tensor(0.0747)\n",
      "epoch: 2760 train_loss: 0.007841471419669687\n",
      "test_loss: tensor(0.0689)\n",
      "epoch: 2770 train_loss: 0.009155561216175556\n",
      "test_loss: tensor(0.0668)\n",
      "epoch: 2780 train_loss: 0.00930011011660099\n",
      "test_loss: tensor(0.0642)\n",
      "epoch: 2790 train_loss: 0.008879147889092564\n",
      "test_loss: tensor(0.0644)\n",
      "epoch: 2800 train_loss: 0.009473480447195471\n",
      "test_loss: tensor(0.0660)\n",
      "epoch: 2810 train_loss: 0.01101310052908957\n",
      "test_loss: tensor(0.0625)\n",
      "epoch: 2820 train_loss: 0.009989788988605142\n",
      "test_loss: tensor(0.0641)\n",
      "epoch: 2830 train_loss: 0.010237733414396644\n",
      "test_loss: tensor(0.0687)\n",
      "epoch: 2840 train_loss: 0.010177460429258645\n",
      "test_loss: tensor(0.0783)\n",
      "epoch: 2850 train_loss: 0.009828166710212827\n",
      "test_loss: tensor(0.0721)\n",
      "epoch: 2860 train_loss: 0.008729643770493567\n",
      "test_loss: tensor(0.0720)\n",
      "epoch: 2870 train_loss: 0.010693058697506785\n",
      "test_loss: tensor(0.0696)\n",
      "epoch: 2880 train_loss: 0.010243904707022012\n",
      "test_loss: tensor(0.0678)\n",
      "epoch: 2890 train_loss: 0.007891554408706725\n",
      "test_loss: tensor(0.0655)\n",
      "epoch: 2900 train_loss: 0.00822797219734639\n",
      "test_loss: tensor(0.0682)\n",
      "epoch: 2910 train_loss: 0.00823832959868014\n",
      "test_loss: tensor(0.0621)\n",
      "epoch: 2920 train_loss: 0.008108709659427404\n",
      "test_loss: tensor(0.0651)\n",
      "epoch: 2930 train_loss: 0.01387304465752095\n",
      "test_loss: tensor(0.0668)\n",
      "epoch: 2940 train_loss: 0.00937908049672842\n",
      "test_loss: tensor(0.0673)\n",
      "epoch: 2950 train_loss: 0.009054101188667119\n",
      "test_loss: tensor(0.0759)\n",
      "epoch: 2960 train_loss: 0.01338291116990149\n",
      "test_loss: tensor(0.0657)\n",
      "epoch: 2970 train_loss: 0.010046810121275484\n",
      "test_loss: tensor(0.0671)\n",
      "epoch: 2980 train_loss: 0.008143562218174338\n",
      "test_loss: tensor(0.0600)\n",
      "epoch: 2990 train_loss: 0.009350047982297837\n",
      "test_loss: tensor(0.0602)\n",
      "epoch: 3000 train_loss: 0.012182765640318394\n",
      "test_loss: tensor(0.0674)\n",
      "epoch: 3010 train_loss: 0.00944277853704989\n",
      "test_loss: tensor(0.0683)\n",
      "epoch: 3020 train_loss: 0.010164878377690912\n",
      "test_loss: tensor(0.0762)\n",
      "epoch: 3030 train_loss: 0.009652503533288836\n",
      "test_loss: tensor(0.0789)\n",
      "epoch: 3040 train_loss: 0.008705313969403505\n",
      "test_loss: tensor(0.0750)\n",
      "epoch: 3050 train_loss: 0.009426342975348233\n",
      "test_loss: tensor(0.0774)\n",
      "epoch: 3060 train_loss: 0.009944093227386475\n",
      "test_loss: tensor(0.0724)\n",
      "epoch: 3070 train_loss: 0.00981522616930306\n",
      "test_loss: tensor(0.0665)\n",
      "epoch: 3080 train_loss: 0.009631831664592028\n",
      "test_loss: tensor(0.0698)\n",
      "epoch: 3090 train_loss: 0.008282729797065258\n",
      "test_loss: tensor(0.0663)\n",
      "epoch: 3100 train_loss: 0.009111093729734421\n",
      "test_loss: tensor(0.0640)\n",
      "epoch: 3110 train_loss: 0.010223499964922667\n",
      "test_loss: tensor(0.0655)\n",
      "epoch: 3120 train_loss: 0.011916507873684168\n",
      "test_loss: tensor(0.0624)\n",
      "epoch: 3130 train_loss: 0.010997539153322577\n",
      "test_loss: tensor(0.0678)\n",
      "epoch: 3140 train_loss: 0.010424202773720026\n",
      "test_loss: tensor(0.0664)\n",
      "epoch: 3150 train_loss: 0.009835965977981687\n",
      "test_loss: tensor(0.0687)\n",
      "epoch: 3160 train_loss: 0.009667666582390665\n",
      "test_loss: tensor(0.0719)\n",
      "epoch: 3170 train_loss: 0.008458951348438859\n",
      "test_loss: tensor(0.0700)\n",
      "epoch: 3180 train_loss: 0.008853003941476345\n",
      "test_loss: tensor(0.0683)\n",
      "epoch: 3190 train_loss: 0.009635426988825201\n",
      "test_loss: tensor(0.0705)\n",
      "epoch: 3200 train_loss: 0.009115480724722147\n",
      "test_loss: tensor(0.0670)\n",
      "epoch: 3210 train_loss: 0.009659742144867778\n",
      "test_loss: tensor(0.0669)\n",
      "epoch: 3220 train_loss: 0.011123989848420024\n",
      "test_loss: tensor(0.0641)\n",
      "epoch: 3230 train_loss: 0.01126532459165901\n",
      "test_loss: tensor(0.0674)\n",
      "epoch: 3240 train_loss: 0.011352157895453274\n",
      "test_loss: tensor(0.0662)\n",
      "epoch: 3250 train_loss: 0.010936797340400517\n",
      "test_loss: tensor(0.0672)\n",
      "epoch: 3260 train_loss: 0.011042452696710826\n",
      "test_loss: tensor(0.0752)\n",
      "epoch: 3270 train_loss: 0.009191210707649588\n",
      "test_loss: tensor(0.0696)\n",
      "epoch: 3280 train_loss: 0.010327835660427808\n",
      "test_loss: tensor(0.0695)\n",
      "epoch: 3290 train_loss: 0.009524078108370304\n",
      "test_loss: tensor(0.0674)\n",
      "epoch: 3300 train_loss: 0.009300656709820032\n",
      "test_loss: tensor(0.0673)\n",
      "epoch: 3310 train_loss: 0.011715685762465\n",
      "test_loss: tensor(0.0699)\n",
      "epoch: 3320 train_loss: 0.008835337567143142\n",
      "test_loss: tensor(0.0695)\n",
      "epoch: 3330 train_loss: 0.010325075220316648\n",
      "test_loss: tensor(0.0666)\n",
      "epoch: 3340 train_loss: 0.009069956606253982\n",
      "test_loss: tensor(0.0651)\n",
      "epoch: 3350 train_loss: 0.00870225615799427\n",
      "test_loss: tensor(0.0649)\n",
      "epoch: 3360 train_loss: 0.008868214767426252\n",
      "test_loss: tensor(0.0637)\n",
      "epoch: 3370 train_loss: 0.008985748793929816\n",
      "test_loss: tensor(0.0633)\n",
      "epoch: 3380 train_loss: 0.008028827537782491\n",
      "test_loss: tensor(0.0736)\n",
      "epoch: 3390 train_loss: 0.00817064440343529\n",
      "test_loss: tensor(0.0685)\n",
      "epoch: 3400 train_loss: 0.009715514094568788\n",
      "test_loss: tensor(0.0634)\n",
      "epoch: 3410 train_loss: 0.012131837010383607\n",
      "test_loss: tensor(0.0646)\n",
      "epoch: 3420 train_loss: 0.010593194235116243\n",
      "test_loss: tensor(0.0852)\n",
      "epoch: 3430 train_loss: 0.009318871004506946\n",
      "test_loss: tensor(0.0749)\n",
      "epoch: 3440 train_loss: 0.00872918488457799\n",
      "test_loss: tensor(0.0672)\n",
      "epoch: 3450 train_loss: 0.010386233334429562\n",
      "test_loss: tensor(0.0618)\n",
      "epoch: 3460 train_loss: 0.01814026548527181\n",
      "test_loss: tensor(0.0714)\n",
      "epoch: 3470 train_loss: 0.013856939505785704\n",
      "test_loss: tensor(0.0722)\n",
      "epoch: 3480 train_loss: 0.013292534183710814\n",
      "test_loss: tensor(0.0698)\n",
      "epoch: 3490 train_loss: 0.011669524339959025\n",
      "test_loss: tensor(0.0665)\n",
      "epoch: 3500 train_loss: 0.010245096194557846\n",
      "test_loss: tensor(0.0619)\n",
      "epoch: 3510 train_loss: 0.010262891557067633\n",
      "test_loss: tensor(0.0637)\n",
      "epoch: 3520 train_loss: 0.008682214794680477\n",
      "test_loss: tensor(0.0630)\n",
      "epoch: 3530 train_loss: 0.009641237277537584\n",
      "test_loss: tensor(0.0635)\n",
      "epoch: 3540 train_loss: 0.010819256491959094\n",
      "test_loss: tensor(0.0733)\n",
      "epoch: 3550 train_loss: 0.00923706842586398\n",
      "test_loss: tensor(0.0702)\n",
      "epoch: 3560 train_loss: 0.010182337020523846\n",
      "test_loss: tensor(0.0696)\n",
      "epoch: 3570 train_loss: 0.009462810307741164\n",
      "test_loss: tensor(0.0668)\n",
      "epoch: 3580 train_loss: 0.008678562240675092\n",
      "test_loss: tensor(0.0661)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3590 train_loss: 0.00879057738929987\n",
      "test_loss: tensor(0.0640)\n",
      "epoch: 3600 train_loss: 0.009414127166382968\n",
      "test_loss: tensor(0.0627)\n",
      "epoch: 3610 train_loss: 0.008135971985757351\n",
      "test_loss: tensor(0.0663)\n",
      "epoch: 3620 train_loss: 0.007659137435257435\n",
      "test_loss: tensor(0.0666)\n",
      "epoch: 3630 train_loss: 0.0080071218078956\n",
      "test_loss: tensor(0.0613)\n",
      "epoch: 3640 train_loss: 0.007626781426370144\n",
      "test_loss: tensor(0.0634)\n",
      "epoch: 3650 train_loss: 0.008690246962942183\n",
      "test_loss: tensor(0.0604)\n",
      "epoch: 3660 train_loss: 0.008711417834274472\n",
      "test_loss: tensor(0.0786)\n",
      "epoch: 3670 train_loss: 0.010166504490189254\n",
      "test_loss: tensor(0.0708)\n",
      "epoch: 3680 train_loss: 0.016156853036955\n",
      "test_loss: tensor(0.0602)\n",
      "epoch: 3690 train_loss: 0.0097215938847512\n",
      "test_loss: tensor(0.0800)\n",
      "epoch: 3700 train_loss: 0.012472121557220817\n",
      "test_loss: tensor(0.0753)\n",
      "epoch: 3710 train_loss: 0.010687964130192995\n",
      "test_loss: tensor(0.0629)\n",
      "epoch: 3720 train_loss: 0.009525097697041929\n",
      "test_loss: tensor(0.0648)\n",
      "epoch: 3730 train_loss: 0.008875628048554064\n",
      "test_loss: tensor(0.0676)\n",
      "epoch: 3740 train_loss: 0.0085343805141747\n",
      "test_loss: tensor(0.0671)\n",
      "epoch: 3750 train_loss: 0.008136829710565507\n",
      "test_loss: tensor(0.0647)\n",
      "epoch: 3760 train_loss: 0.008382517169229687\n",
      "test_loss: tensor(0.0628)\n",
      "epoch: 3770 train_loss: 0.009687122306786478\n",
      "test_loss: tensor(0.0628)\n",
      "epoch: 3780 train_loss: 0.008839200669899582\n",
      "test_loss: tensor(0.0762)\n",
      "epoch: 3790 train_loss: 0.009395262715406716\n",
      "test_loss: tensor(0.0703)\n",
      "epoch: 3800 train_loss: 0.008019067393615843\n",
      "test_loss: tensor(0.0651)\n",
      "epoch: 3810 train_loss: 0.012593304505571723\n",
      "test_loss: tensor(0.0625)\n",
      "epoch: 3820 train_loss: 0.011668999446555972\n",
      "test_loss: tensor(0.0746)\n",
      "epoch: 3830 train_loss: 0.010172892408445478\n",
      "test_loss: tensor(0.0771)\n",
      "epoch: 3840 train_loss: 0.00922444702591747\n",
      "test_loss: tensor(0.0657)\n",
      "epoch: 3850 train_loss: 0.010197115177288652\n",
      "test_loss: tensor(0.0683)\n",
      "epoch: 3860 train_loss: 0.011615915223956108\n",
      "test_loss: tensor(0.0688)\n",
      "epoch: 3870 train_loss: 0.008930495334789157\n",
      "test_loss: tensor(0.0676)\n",
      "epoch: 3880 train_loss: 0.009181901882402598\n",
      "test_loss: tensor(0.0663)\n",
      "epoch: 3890 train_loss: 0.008929596422240138\n",
      "test_loss: tensor(0.0628)\n",
      "epoch: 3900 train_loss: 0.009782231133431196\n",
      "test_loss: tensor(0.0601)\n",
      "epoch: 3910 train_loss: 0.012118407944217324\n",
      "test_loss: tensor(0.0632)\n",
      "epoch: 3920 train_loss: 0.010906442021951079\n",
      "test_loss: tensor(0.0622)\n",
      "epoch: 3930 train_loss: 0.010234560770913958\n",
      "test_loss: tensor(0.0629)\n",
      "epoch: 3940 train_loss: 0.013421789184212685\n",
      "test_loss: tensor(0.0673)\n",
      "epoch: 3950 train_loss: 0.009327197866514324\n",
      "test_loss: tensor(0.0676)\n",
      "epoch: 3960 train_loss: 0.009817322390154004\n",
      "test_loss: tensor(0.0680)\n",
      "epoch: 3970 train_loss: 0.00963905886746943\n",
      "test_loss: tensor(0.0654)\n",
      "epoch: 3980 train_loss: 0.010170448198914528\n",
      "test_loss: tensor(0.0735)\n",
      "epoch: 3990 train_loss: 0.01038473395165056\n",
      "test_loss: tensor(0.0709)\n",
      "epoch: 4000 train_loss: 0.010929689835757017\n",
      "test_loss: tensor(0.0642)\n",
      "epoch: 4010 train_loss: 0.009894259390421212\n",
      "test_loss: tensor(0.0682)\n",
      "epoch: 4020 train_loss: 0.009087648917920887\n",
      "test_loss: tensor(0.0694)\n",
      "epoch: 4030 train_loss: 0.009754126193001866\n",
      "test_loss: tensor(0.0638)\n",
      "epoch: 4040 train_loss: 0.011348361801356077\n",
      "test_loss: tensor(0.0639)\n",
      "epoch: 4050 train_loss: 0.011429701512679458\n",
      "test_loss: tensor(0.0649)\n",
      "epoch: 4060 train_loss: 0.011416261759586632\n",
      "test_loss: tensor(0.0665)\n",
      "epoch: 4070 train_loss: 0.009005199209786952\n",
      "test_loss: tensor(0.0763)\n",
      "epoch: 4080 train_loss: 0.009688864275813103\n",
      "test_loss: tensor(0.0715)\n",
      "epoch: 4090 train_loss: 0.008333782246336342\n",
      "test_loss: tensor(0.0692)\n",
      "epoch: 4100 train_loss: 0.008252749405801296\n",
      "test_loss: tensor(0.0662)\n",
      "epoch: 4110 train_loss: 0.010424004355445503\n",
      "test_loss: tensor(0.0702)\n",
      "epoch: 4120 train_loss: 0.01211030944250524\n",
      "test_loss: tensor(0.0694)\n",
      "epoch: 4130 train_loss: 0.009226260613650084\n",
      "test_loss: tensor(0.0688)\n",
      "epoch: 4140 train_loss: 0.00802467952016741\n",
      "test_loss: tensor(0.0644)\n",
      "epoch: 4150 train_loss: 0.008102184114977717\n",
      "test_loss: tensor(0.0648)\n",
      "epoch: 4160 train_loss: 0.011968260165303946\n",
      "test_loss: tensor(0.0653)\n",
      "epoch: 4170 train_loss: 0.010907575534656645\n",
      "test_loss: tensor(0.0627)\n",
      "epoch: 4180 train_loss: 0.012876864522695541\n",
      "test_loss: tensor(0.0679)\n",
      "epoch: 4190 train_loss: 0.01097164354287088\n",
      "test_loss: tensor(0.0690)\n",
      "epoch: 4200 train_loss: 0.010287863435223699\n",
      "test_loss: tensor(0.0721)\n",
      "epoch: 4210 train_loss: 0.009368124580942095\n",
      "test_loss: tensor(0.0728)\n",
      "epoch: 4220 train_loss: 0.009649870567955077\n",
      "test_loss: tensor(0.0698)\n",
      "epoch: 4230 train_loss: 0.008003399730660022\n",
      "test_loss: tensor(0.0707)\n",
      "epoch: 4240 train_loss: 0.00839836266823113\n",
      "test_loss: tensor(0.0720)\n",
      "epoch: 4250 train_loss: 0.008694741688668729\n",
      "test_loss: tensor(0.0671)\n",
      "epoch: 4260 train_loss: 0.012500563822686672\n",
      "test_loss: tensor(0.0694)\n",
      "epoch: 4270 train_loss: 0.010938920266926288\n",
      "test_loss: tensor(0.0677)\n",
      "epoch: 4280 train_loss: 0.00895994019228965\n",
      "test_loss: tensor(0.0625)\n",
      "epoch: 4290 train_loss: 0.010440508741885424\n",
      "test_loss: tensor(0.0608)\n",
      "epoch: 4300 train_loss: 0.012704587401822209\n",
      "test_loss: tensor(0.0673)\n",
      "epoch: 4310 train_loss: 0.01050971713848412\n",
      "test_loss: tensor(0.0626)\n",
      "epoch: 4320 train_loss: 0.009075813251547516\n",
      "test_loss: tensor(0.0637)\n",
      "epoch: 4330 train_loss: 0.008883104915730656\n",
      "test_loss: tensor(0.0692)\n",
      "epoch: 4340 train_loss: 0.01084536574780941\n",
      "test_loss: tensor(0.0736)\n",
      "epoch: 4350 train_loss: 0.010860177222639322\n",
      "test_loss: tensor(0.0696)\n",
      "epoch: 4360 train_loss: 0.008157914318144322\n",
      "test_loss: tensor(0.0635)\n",
      "epoch: 4370 train_loss: 0.009764839545823634\n",
      "test_loss: tensor(0.0655)\n",
      "epoch: 4380 train_loss: 0.010624084831215441\n",
      "test_loss: tensor(0.0641)\n",
      "epoch: 4390 train_loss: 0.010134967742487788\n",
      "test_loss: tensor(0.0691)\n",
      "epoch: 4400 train_loss: 0.008138008043169975\n",
      "test_loss: tensor(0.0716)\n",
      "epoch: 4410 train_loss: 0.009117899695411325\n",
      "test_loss: tensor(0.0742)\n",
      "epoch: 4420 train_loss: 0.00921889771707356\n",
      "test_loss: tensor(0.0737)\n",
      "epoch: 4430 train_loss: 0.009471483808010817\n",
      "test_loss: tensor(0.0664)\n",
      "epoch: 4440 train_loss: 0.010767844831570983\n",
      "test_loss: tensor(0.0681)\n",
      "epoch: 4450 train_loss: 0.011604227358475327\n",
      "test_loss: tensor(0.0733)\n",
      "epoch: 4460 train_loss: 0.010598704963922501\n",
      "test_loss: tensor(0.0769)\n",
      "epoch: 4470 train_loss: 0.008405703492462635\n",
      "test_loss: tensor(0.0689)\n",
      "epoch: 4480 train_loss: 0.00815669479779899\n",
      "test_loss: tensor(0.0683)\n",
      "epoch: 4490 train_loss: 0.01185749713331461\n",
      "test_loss: tensor(0.0701)\n",
      "epoch: 4500 train_loss: 0.010321845347061754\n",
      "test_loss: tensor(0.0674)\n",
      "epoch: 4510 train_loss: 0.0100590739864856\n",
      "test_loss: tensor(0.0636)\n",
      "epoch: 4520 train_loss: 0.011466462048701942\n",
      "test_loss: tensor(0.0642)\n",
      "epoch: 4530 train_loss: 0.010782805760391057\n",
      "test_loss: tensor(0.0628)\n",
      "epoch: 4540 train_loss: 0.01065263939090073\n",
      "test_loss: tensor(0.0650)\n",
      "epoch: 4550 train_loss: 0.010146591416560113\n",
      "test_loss: tensor(0.0634)\n",
      "epoch: 4560 train_loss: 0.010681351041421294\n",
      "test_loss: tensor(0.0684)\n",
      "epoch: 4570 train_loss: 0.00858582486398518\n",
      "test_loss: tensor(0.0749)\n",
      "epoch: 4580 train_loss: 0.012128307297825814\n",
      "test_loss: tensor(0.0684)\n",
      "epoch: 4590 train_loss: 0.007852297206409276\n",
      "test_loss: tensor(0.0657)\n",
      "epoch: 4600 train_loss: 0.007887223293073476\n",
      "test_loss: tensor(0.0652)\n",
      "epoch: 4610 train_loss: 0.009479423449374736\n",
      "test_loss: tensor(0.0609)\n",
      "epoch: 4620 train_loss: 0.015419801278039814\n",
      "test_loss: tensor(0.0675)\n",
      "epoch: 4630 train_loss: 0.009413547674193978\n",
      "test_loss: tensor(0.0744)\n",
      "epoch: 4640 train_loss: 0.009568644873797893\n",
      "test_loss: tensor(0.0722)\n",
      "epoch: 4650 train_loss: 0.010634091310203075\n",
      "test_loss: tensor(0.0702)\n",
      "epoch: 4660 train_loss: 0.010270500835031272\n",
      "test_loss: tensor(0.0705)\n",
      "epoch: 4670 train_loss: 0.01000640499405563\n",
      "test_loss: tensor(0.0668)\n",
      "epoch: 4680 train_loss: 0.009858411317691208\n",
      "test_loss: tensor(0.0676)\n",
      "epoch: 4690 train_loss: 0.008629427663981915\n",
      "test_loss: tensor(0.0679)\n",
      "epoch: 4700 train_loss: 0.009303564368747175\n",
      "test_loss: tensor(0.0658)\n",
      "epoch: 4710 train_loss: 0.010743251023814081\n",
      "test_loss: tensor(0.0626)\n",
      "epoch: 4720 train_loss: 0.009244128619320691\n",
      "test_loss: tensor(0.0691)\n",
      "epoch: 4730 train_loss: 0.008588867052458227\n",
      "test_loss: tensor(0.0691)\n",
      "epoch: 4740 train_loss: 0.009235914517194032\n",
      "test_loss: tensor(0.0716)\n",
      "epoch: 4750 train_loss: 0.008470897562801838\n",
      "test_loss: tensor(0.0707)\n",
      "epoch: 4760 train_loss: 0.00983052202500403\n",
      "test_loss: tensor(0.0702)\n",
      "epoch: 4770 train_loss: 0.011984910117462277\n",
      "test_loss: tensor(0.0681)\n",
      "epoch: 4780 train_loss: 0.00932796539273113\n",
      "test_loss: tensor(0.0650)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4790 train_loss: 0.009696819889359175\n",
      "test_loss: tensor(0.0628)\n",
      "epoch: 4800 train_loss: 0.010321252117864787\n",
      "test_loss: tensor(0.0659)\n",
      "epoch: 4810 train_loss: 0.01249980553984642\n",
      "test_loss: tensor(0.0683)\n",
      "epoch: 4820 train_loss: 0.007993344590067864\n",
      "test_loss: tensor(0.0685)\n",
      "epoch: 4830 train_loss: 0.008328157174400986\n",
      "test_loss: tensor(0.0689)\n",
      "epoch: 4840 train_loss: 0.008906727703288198\n",
      "test_loss: tensor(0.0637)\n",
      "epoch: 4850 train_loss: 0.00870637665502727\n",
      "test_loss: tensor(0.0635)\n",
      "epoch: 4860 train_loss: 0.011591250146739184\n",
      "test_loss: tensor(0.0587)\n",
      "epoch: 4870 train_loss: 0.010406506014987826\n",
      "test_loss: tensor(0.0768)\n",
      "epoch: 4880 train_loss: 0.01103874370455742\n",
      "test_loss: tensor(0.0774)\n",
      "epoch: 4890 train_loss: 0.010328073054552078\n",
      "test_loss: tensor(0.0745)\n",
      "epoch: 4900 train_loss: 0.009946882678195833\n",
      "test_loss: tensor(0.0759)\n",
      "epoch: 4910 train_loss: 0.007961149676702917\n",
      "test_loss: tensor(0.0655)\n",
      "epoch: 4920 train_loss: 0.00815528524108231\n",
      "test_loss: tensor(0.0679)\n",
      "epoch: 4930 train_loss: 0.008374549727886916\n",
      "test_loss: tensor(0.0677)\n",
      "epoch: 4940 train_loss: 0.00854837482329458\n",
      "test_loss: tensor(0.0645)\n",
      "epoch: 4950 train_loss: 0.010736772278323769\n",
      "test_loss: tensor(0.0636)\n",
      "epoch: 4960 train_loss: 0.01017647550906986\n",
      "test_loss: tensor(0.0653)\n",
      "epoch: 4970 train_loss: 0.010420867498032749\n",
      "test_loss: tensor(0.0673)\n",
      "epoch: 4980 train_loss: 0.009406961407512427\n",
      "test_loss: tensor(0.0756)\n",
      "epoch: 4990 train_loss: 0.011620287410914898\n",
      "test_loss: tensor(0.0655)\n",
      "epoch: 5000 train_loss: 0.009063165774568916\n",
      "test_loss: tensor(0.0623)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for epoch in range(5000):\n",
    "        train(epoch)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "52419a1a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200.6, 190. ],\n",
       "       [ 70.3,  60. ],\n",
       "       [170.6,  90. ],\n",
       "       [170.7, 200. ],\n",
       "       [ 90.3, 130. ],\n",
       "       [180.6,  30. ],\n",
       "       [100.4, 180. ],\n",
       "       [150.3, 120. ],\n",
       "       [ 20.1, 100. ],\n",
       "       [150.7,  50. ],\n",
       "       [ 90.2, 150. ],\n",
       "       [187.7, 130. ],\n",
       "       [100.4,  90. ],\n",
       "       [170.6,  50. ],\n",
       "       [169.5,  20. ],\n",
       "       [120.4,  80. ],\n",
       "       [190.5, 120. ],\n",
       "       [ 10. ,  60. ],\n",
       "       [ 40.2, 190. ],\n",
       "       [120.4,  40. ],\n",
       "       [ 70.4,  30. ],\n",
       "       [130.6,  30. ],\n",
       "       [200.6,  30. ],\n",
       "       [110.5, 150. ],\n",
       "       [ 10. , 180. ],\n",
       "       [ 50.2, 130. ],\n",
       "       [190.5,  70. ],\n",
       "       [110.4,  20. ],\n",
       "       [189.9,  20. ],\n",
       "       [110.2,  80. ],\n",
       "       [ 50.2,  20. ],\n",
       "       [ 10. ,  50. ],\n",
       "       [ 90.4,  60. ],\n",
       "       [140.4, 160. ],\n",
       "       [ 10. ,  70. ],\n",
       "       [ 50.2,  10. ],\n",
       "       [ 30.1, 190. ],\n",
       "       [ 60.3,  30. ],\n",
       "       [199.8, 180. ],\n",
       "       [ 70.3, 200. ],\n",
       "       [140.5,  90. ],\n",
       "       [110.4,  50. ],\n",
       "       [ 90.4, 120. ],\n",
       "       [ 40.2,  30. ],\n",
       "       [200.4,  60. ],\n",
       "       [ 50.2,  60. ],\n",
       "       [189.1, 100. ],\n",
       "       [160.5,  80. ],\n",
       "       [159.7, 150. ],\n",
       "       [140.4, 190. ],\n",
       "       [ 40.2,  60. ],\n",
       "       [120.3,  60. ],\n",
       "       [120.4, 100. ],\n",
       "       [ 10. , 190. ],\n",
       "       [150.5, 170. ],\n",
       "       [150.4,  70. ],\n",
       "       [140.4, 130. ],\n",
       "       [ 60.3, 180. ],\n",
       "       [130.5, 190. ],\n",
       "       [190.4,  10. ],\n",
       "       [ 60.2,  80. ],\n",
       "       [ 40.2,  80. ],\n",
       "       [120.4, 150. ],\n",
       "       [130.5,  70. ],\n",
       "       [180.1, 150. ],\n",
       "       [120.5,  30. ],\n",
       "       [ 60.3,  70. ],\n",
       "       [160.2, 110. ],\n",
       "       [140.4, 110. ],\n",
       "       [160.3, 130. ],\n",
       "       [110.4, 190. ],\n",
       "       [ 50.2, 160. ],\n",
       "       [179.4, 110. ],\n",
       "       [197.9,  80. ],\n",
       "       [ 10. , 120. ],\n",
       "       [180.1,  80. ],\n",
       "       [ 30.1,  20. ],\n",
       "       [100.5,  10. ],\n",
       "       [ 70.4, 130. ],\n",
       "       [120.4,  50. ]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "843f3241",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([190.,  60.,  90., 200., 130.,  30., 180., 120., 100.,  50., 150.,\n",
       "       130.,  90.,  50.,  20.,  80., 120.,  60., 190.,  40.,  30.,  30.,\n",
       "        30., 150., 180., 130.,  70.,  20.,  20.,  80.,  20.,  50.,  60.,\n",
       "       160.,  70.,  10., 190.,  30., 180., 200.,  90.,  50., 120.,  30.,\n",
       "        60.,  60., 100.,  80., 150., 190.,  60.,  60., 100., 190., 170.,\n",
       "        70., 130., 180., 190.,  10.,  80.,  80., 150.,  70., 150.,  30.,\n",
       "        70., 110., 110., 130., 190., 160., 110.,  80., 120.,  80.,  20.,\n",
       "        10., 130.,  50.])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest1[:, 1].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "34ce44ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(Xtest1_tensor)\n",
    "y = pd.concat([pd.Series(Xtest1[:, 0].reshape(-1), name='send rates'), pd.Series(Xtest1[:, 1].reshape(-1), name='block size'), \n",
    "               pd.Series(Ytest1_tensor.numpy().reshape(-1), name='latency_true'), pd.Series(y_pred.numpy().reshape(-1), name='latency_pred')], \n",
    "              axis=1)\n",
    "y.to_csv('../../Data/Result/Related3/latency_true_pred_related3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3d7e4365",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>block size</th>\n",
       "      <th>latency_true</th>\n",
       "      <th>latency_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.488863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70.3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.488265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170.6</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.930942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170.7</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.061628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.3</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.819647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   send rates  block size  latency_true  latency_pred\n",
       "0       200.6       190.0          2.11      2.488863\n",
       "1        70.3        60.0          0.55      0.488265\n",
       "2       170.6        90.0          0.97      0.930942\n",
       "3       170.7       200.0          1.22      1.061628\n",
       "4        90.3       130.0          0.88      0.819647"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute test set MAE RMSE MAPE\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../../Data/Result/Related3/latency_true_pred_related3.csv')\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "354b4d47",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.11, 0.55, 0.97, 1.22, 0.88, 0.9 , 0.79, 0.8 , 1.17, 0.41, 0.9 ,\n",
       "       1.2 , 0.68, 0.48, 0.39, 0.67, 1.22, 1.12, 1.15, 0.34, 0.25, 0.22,\n",
       "       1.34, 0.89, 1.18, 1.24, 1.33, 0.13, 1.44, 0.66, 0.2 , 1.14, 0.48,\n",
       "       0.88, 1.16, 0.09, 1.14, 0.28, 2.01, 1.02, 0.59, 0.31, 1.01, 0.39,\n",
       "       1.55, 0.68, 2.7 , 1.61, 2.89, 0.82, 0.81, 0.44, 0.72, 1.18, 0.95,\n",
       "       0.46, 0.74, 1.23, 0.81, 1.59, 0.8 , 1.08, 0.94, 0.49, 1.42, 0.2 ,\n",
       "       0.68, 1.96, 1.12, 0.8 , 0.85, 1.21, 2.44, 2.26, 1.14, 1.89, 0.31,\n",
       "       0.06, 1.07, 0.41])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['latency_true'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8eb18263",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.4888635 , 0.48826516, 0.9309422 , 1.061628  , 0.8196467 ,\n",
       "       0.49261189, 0.7921686 , 0.88242006, 1.1601468 , 0.36275733,\n",
       "       0.8674686 , 0.961465  , 0.6638267 , 0.40813863, 0.39843118,\n",
       "       0.6143881 , 1.4676718 , 1.1679631 , 1.1207069 , 0.2473098 ,\n",
       "       0.23394537, 0.18397391, 1.2577022 , 1.0170275 , 1.1276613 ,\n",
       "       1.191578  , 0.9024142 , 0.11868835, 0.9672638 , 0.6110817 ,\n",
       "       0.17260742, 1.1529206 , 0.42975926, 0.9646293 , 1.205774  ,\n",
       "       0.0339303 , 1.0693284 , 0.23420823, 2.3376565 , 1.0458857 ,\n",
       "       0.71694016, 0.31065834, 0.78859067, 0.36313593, 1.2846214 ,\n",
       "       0.6578646 , 2.5964231 , 1.3108565 , 1.3520206 , 0.8444942 ,\n",
       "       0.7475821 , 0.37735236, 0.71026146, 1.1155001 , 0.8627765 ,\n",
       "       0.64402103, 0.8625984 , 1.1830331 , 0.8179724 , 1.5967647 ,\n",
       "       0.7885654 , 1.0265604 , 1.0052754 , 0.45508647, 1.523527  ,\n",
       "       0.18332613, 0.71137476, 1.7175158 , 0.8795042 , 1.0714923 ,\n",
       "       1.0676185 , 1.1860782 , 2.6192908 , 2.3683414 , 1.0927199 ,\n",
       "       2.8530908 , 0.24175417, 0.06326878, 1.1128689 , 0.3112998 ])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = data['latency_pred'].values\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a18748f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "818e234b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.1283142071875\n",
      "RMSE:  0.2496037791682217\n",
      "MAPE:  0.12628273082676367\n"
     ]
    }
   ],
   "source": [
    "MAE = metrics.mean_absolute_error(y, y_hat)\n",
    "RMSE = metrics.mean_squared_error(y, y_hat) ** 0.5\n",
    "MAPE = metrics.mean_absolute_percentage_error(y, y_hat)\n",
    "print('MAE: ', MAE)\n",
    "print('RMSE: ', RMSE)\n",
    "print('MAPE: ', MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c0cfe2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MLP2(send rates, block size) = throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "157b7764",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split training set and test set\n",
    "Xtrain2, Xtest2, Ytrain2, Ytest2 = train_test_split(X, Y2, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "de8a9bf9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200.6, 190. ],\n",
       "       [ 70.3,  60. ],\n",
       "       [170.6,  90. ],\n",
       "       [170.7, 200. ],\n",
       "       [ 90.3, 130. ],\n",
       "       [180.6,  30. ],\n",
       "       [100.4, 180. ],\n",
       "       [150.3, 120. ],\n",
       "       [ 20.1, 100. ],\n",
       "       [150.7,  50. ],\n",
       "       [ 90.2, 150. ],\n",
       "       [187.7, 130. ],\n",
       "       [100.4,  90. ],\n",
       "       [170.6,  50. ],\n",
       "       [169.5,  20. ],\n",
       "       [120.4,  80. ],\n",
       "       [190.5, 120. ],\n",
       "       [ 10. ,  60. ],\n",
       "       [ 40.2, 190. ],\n",
       "       [120.4,  40. ],\n",
       "       [ 70.4,  30. ],\n",
       "       [130.6,  30. ],\n",
       "       [200.6,  30. ],\n",
       "       [110.5, 150. ],\n",
       "       [ 10. , 180. ],\n",
       "       [ 50.2, 130. ],\n",
       "       [190.5,  70. ],\n",
       "       [110.4,  20. ],\n",
       "       [189.9,  20. ],\n",
       "       [110.2,  80. ],\n",
       "       [ 50.2,  20. ],\n",
       "       [ 10. ,  50. ],\n",
       "       [ 90.4,  60. ],\n",
       "       [140.4, 160. ],\n",
       "       [ 10. ,  70. ],\n",
       "       [ 50.2,  10. ],\n",
       "       [ 30.1, 190. ],\n",
       "       [ 60.3,  30. ],\n",
       "       [199.8, 180. ],\n",
       "       [ 70.3, 200. ],\n",
       "       [140.5,  90. ],\n",
       "       [110.4,  50. ],\n",
       "       [ 90.4, 120. ],\n",
       "       [ 40.2,  30. ],\n",
       "       [200.4,  60. ],\n",
       "       [ 50.2,  60. ],\n",
       "       [189.1, 100. ],\n",
       "       [160.5,  80. ],\n",
       "       [159.7, 150. ],\n",
       "       [140.4, 190. ],\n",
       "       [ 40.2,  60. ],\n",
       "       [120.3,  60. ],\n",
       "       [120.4, 100. ],\n",
       "       [ 10. , 190. ],\n",
       "       [150.5, 170. ],\n",
       "       [150.4,  70. ],\n",
       "       [140.4, 130. ],\n",
       "       [ 60.3, 180. ],\n",
       "       [130.5, 190. ],\n",
       "       [190.4,  10. ],\n",
       "       [ 60.2,  80. ],\n",
       "       [ 40.2,  80. ],\n",
       "       [120.4, 150. ],\n",
       "       [130.5,  70. ],\n",
       "       [180.1, 150. ],\n",
       "       [120.5,  30. ],\n",
       "       [ 60.3,  70. ],\n",
       "       [160.2, 110. ],\n",
       "       [140.4, 110. ],\n",
       "       [160.3, 130. ],\n",
       "       [110.4, 190. ],\n",
       "       [ 50.2, 160. ],\n",
       "       [179.4, 110. ],\n",
       "       [197.9,  80. ],\n",
       "       [ 10. , 120. ],\n",
       "       [180.1,  80. ],\n",
       "       [ 30.1,  20. ],\n",
       "       [100.5,  10. ],\n",
       "       [ 70.4, 130. ],\n",
       "       [120.4,  50. ]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f14b9110",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[160.4],\n",
       "       [ 63.3],\n",
       "       [124.9],\n",
       "       [160.4],\n",
       "       [ 81.6],\n",
       "       [131.3],\n",
       "       [ 97.1],\n",
       "       [117.7],\n",
       "       [ 20. ],\n",
       "       [118.7],\n",
       "       [ 81.1],\n",
       "       [162.5],\n",
       "       [ 83.4],\n",
       "       [129.8],\n",
       "       [126.2],\n",
       "       [ 98.8],\n",
       "       [135.4],\n",
       "       [ 10. ],\n",
       "       [ 37.3],\n",
       "       [ 99.2],\n",
       "       [ 61.8],\n",
       "       [103.4],\n",
       "       [140.4],\n",
       "       [ 96.2],\n",
       "       [  9.9],\n",
       "       [ 48.2],\n",
       "       [135.2],\n",
       "       [110. ],\n",
       "       [135.9],\n",
       "       [ 92.4],\n",
       "       [ 50.1],\n",
       "       [  9.9],\n",
       "       [ 78.3],\n",
       "       [122.1],\n",
       "       [  9.8],\n",
       "       [ 50.1],\n",
       "       [ 28.8],\n",
       "       [ 54. ],\n",
       "       [157.2],\n",
       "       [ 69.9],\n",
       "       [109.2],\n",
       "       [109.6],\n",
       "       [ 77.9],\n",
       "       [ 37.3],\n",
       "       [139.4],\n",
       "       [ 47. ],\n",
       "       [134.6],\n",
       "       [119.5],\n",
       "       [119.5],\n",
       "       [122.1],\n",
       "       [ 38.3],\n",
       "       [ 99.1],\n",
       "       [118.9],\n",
       "       [  9.9],\n",
       "       [120.4],\n",
       "       [116.4],\n",
       "       [121.3],\n",
       "       [ 59.3],\n",
       "       [121. ],\n",
       "       [135.1],\n",
       "       [ 55.2],\n",
       "       [ 38.3],\n",
       "       [ 96.6],\n",
       "       [104.2],\n",
       "       [158.9],\n",
       "       [ 96.9],\n",
       "       [ 54.3],\n",
       "       [118.1],\n",
       "       [109.4],\n",
       "       [122. ],\n",
       "       [ 96.6],\n",
       "       [ 48.6],\n",
       "       [125.5],\n",
       "       [135.2],\n",
       "       [  9.9],\n",
       "       [128.2],\n",
       "       [ 30.1],\n",
       "       [100.1],\n",
       "       [ 61.5],\n",
       "       [ 99.8]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6cd3ae3d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# property scaling\n",
    "min_max_scaler2 = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "05f08d18",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21058146, 0.68421053],\n",
       "       [0.05290728, 0.15789474],\n",
       "       [0.73598743, 1.        ],\n",
       "       [0.        , 0.68421053],\n",
       "       [0.21058146, 0.52631579],\n",
       "       [0.15819801, 0.05263158],\n",
       "       [0.99738083, 0.05263158],\n",
       "       [0.52592981, 0.68421053],\n",
       "       [0.99685699, 0.57894737],\n",
       "       [0.31587218, 0.36842105],\n",
       "       [0.36877947, 1.        ],\n",
       "       [0.89313777, 0.78947368],\n",
       "       [0.73598743, 0.15789474],\n",
       "       [0.83970665, 0.94736842],\n",
       "       [0.42116291, 0.05263158],\n",
       "       [0.78837087, 0.        ],\n",
       "       [0.68412782, 0.89473684],\n",
       "       [0.73493976, 0.47368421],\n",
       "       [0.68360398, 0.        ],\n",
       "       [0.42063908, 0.68421053],\n",
       "       [0.21058146, 0.10526316],\n",
       "       [0.94342588, 0.36842105],\n",
       "       [0.42063908, 1.        ],\n",
       "       [0.05290728, 0.36842105],\n",
       "       [0.21058146, 0.47368421],\n",
       "       [0.78837087, 0.57894737],\n",
       "       [0.15819801, 0.68421053],\n",
       "       [0.57726558, 0.68421053],\n",
       "       [0.73546359, 0.94736842],\n",
       "       [0.6291252 , 0.52631579],\n",
       "       [0.26348874, 1.        ],\n",
       "       [0.94394971, 0.89473684],\n",
       "       [0.21058146, 0.94736842],\n",
       "       [0.6306967 , 0.21052632],\n",
       "       [0.36877947, 0.31578947],\n",
       "       [0.21058146, 0.57894737],\n",
       "       [0.83813515, 0.10526316],\n",
       "       [0.6306967 , 0.42105263],\n",
       "       [0.47354636, 0.31578947],\n",
       "       [0.95966475, 0.78947368],\n",
       "       [0.05290728, 1.        ],\n",
       "       [0.42116291, 0.36842105],\n",
       "       [0.99214248, 0.15789474],\n",
       "       [0.57883709, 0.05263158],\n",
       "       [0.89470927, 0.57894737],\n",
       "       [0.10529073, 1.        ],\n",
       "       [0.31639602, 0.        ],\n",
       "       [0.        , 0.63157895],\n",
       "       [0.7857517 , 0.47368421],\n",
       "       [0.97171294, 1.        ],\n",
       "       [0.84232583, 0.68421053],\n",
       "       [0.73546359, 0.68421053],\n",
       "       [0.92194866, 0.52631579],\n",
       "       [0.73546359, 0.36842105],\n",
       "       [0.89313777, 0.94736842],\n",
       "       [0.73651126, 0.78947368],\n",
       "       [0.21058146, 0.42105263],\n",
       "       [0.31587218, 0.57894737],\n",
       "       [0.31587218, 0.21052632],\n",
       "       [0.94237821, 0.84210526],\n",
       "       [0.        , 0.73684211],\n",
       "       [0.42063908, 0.10526316],\n",
       "       [0.31587218, 0.15789474],\n",
       "       [0.31587218, 0.94736842],\n",
       "       [0.26348874, 0.57894737],\n",
       "       [0.10529073, 0.57894737],\n",
       "       [0.2629649 , 0.63157895],\n",
       "       [0.        , 0.47368421],\n",
       "       [0.47302252, 0.26315789],\n",
       "       [0.        , 0.84210526],\n",
       "       [0.94552122, 0.21052632],\n",
       "       [0.        , 0.        ],\n",
       "       [0.26348874, 0.26315789],\n",
       "       [0.47354636, 0.47368421],\n",
       "       [0.52592981, 0.57894737],\n",
       "       [0.8920901 , 0.        ],\n",
       "       [0.15819801, 0.52631579],\n",
       "       [0.05290728, 0.94736842],\n",
       "       [0.36825563, 0.52631579],\n",
       "       [0.52488214, 0.42105263],\n",
       "       [0.42116291, 0.15789474],\n",
       "       [0.62964903, 0.36842105],\n",
       "       [0.87742273, 0.94736842],\n",
       "       [0.36825563, 0.26315789],\n",
       "       [0.63122053, 0.57894737],\n",
       "       [0.10529073, 0.10526316],\n",
       "       [0.42116291, 0.        ],\n",
       "       [0.57726558, 0.52631579],\n",
       "       [0.47302252, 0.57894737],\n",
       "       [0.57831325, 0.94736842],\n",
       "       [0.88947093, 0.21052632],\n",
       "       [0.99738083, 0.63157895],\n",
       "       [0.83603981, 0.52631579],\n",
       "       [0.73651126, 0.26315789],\n",
       "       [0.47354636, 0.63157895],\n",
       "       [0.36825563, 0.36842105],\n",
       "       [0.78837087, 0.21052632],\n",
       "       [0.26348874, 0.        ],\n",
       "       [0.05290728, 0.73684211],\n",
       "       [0.2629649 , 0.52631579],\n",
       "       [0.42116291, 0.78947368],\n",
       "       [0.63122053, 0.26315789],\n",
       "       [0.31587218, 0.78947368],\n",
       "       [0.94552122, 0.73684211],\n",
       "       [0.36877947, 0.73684211],\n",
       "       [0.        , 1.        ],\n",
       "       [0.3677318 , 0.10526316],\n",
       "       [0.36877947, 0.94736842],\n",
       "       [0.68308015, 0.68421053],\n",
       "       [0.10529073, 0.21052632],\n",
       "       [0.68360398, 0.57894737],\n",
       "       [0.57831325, 0.84210526],\n",
       "       [0.63122053, 0.47368421],\n",
       "       [0.2629649 , 0.42105263],\n",
       "       [0.89313777, 0.26315789],\n",
       "       [0.42063908, 0.47368421],\n",
       "       [0.15819801, 1.        ],\n",
       "       [0.93137768, 1.        ],\n",
       "       [0.21058146, 0.21052632],\n",
       "       [0.        , 0.42105263],\n",
       "       [0.94552122, 0.26315789],\n",
       "       [0.21058146, 1.        ],\n",
       "       [0.8920901 , 0.42105263],\n",
       "       [0.84075432, 0.47368421],\n",
       "       [0.84232583, 0.        ],\n",
       "       [0.05290728, 0.42105263],\n",
       "       [0.05290728, 0.57894737],\n",
       "       [0.10529073, 0.78947368],\n",
       "       [0.57831325, 0.42105263],\n",
       "       [0.05290728, 0.63157895],\n",
       "       [0.68255631, 0.73684211],\n",
       "       [0.10529073, 0.42105263],\n",
       "       [0.84023049, 0.57894737],\n",
       "       [0.6306967 , 0.89473684],\n",
       "       [0.8936616 , 0.05263158],\n",
       "       [0.05290728, 0.68421053],\n",
       "       [0.68308015, 0.36842105],\n",
       "       [0.05290728, 0.78947368],\n",
       "       [0.78732321, 0.15789474],\n",
       "       [0.15819801, 0.15789474],\n",
       "       [0.36877947, 0.89473684],\n",
       "       [0.31587218, 0.42105263],\n",
       "       [0.10529073, 0.31578947],\n",
       "       [0.57883709, 0.63157895],\n",
       "       [0.15819801, 0.31578947],\n",
       "       [0.63122053, 0.84210526],\n",
       "       [0.89156627, 0.47368421],\n",
       "       [0.68360398, 0.05263158],\n",
       "       [0.99947617, 0.        ],\n",
       "       [0.92771084, 0.52631579],\n",
       "       [0.84075432, 0.15789474],\n",
       "       [0.10529073, 0.26315789],\n",
       "       [0.52645364, 0.31578947],\n",
       "       [0.42063908, 0.42105263],\n",
       "       [0.83918282, 0.63157895],\n",
       "       [0.10529073, 0.36842105],\n",
       "       [0.2629649 , 0.68421053],\n",
       "       [0.42063908, 0.89473684],\n",
       "       [0.52645364, 1.        ],\n",
       "       [0.78941854, 0.26315789],\n",
       "       [0.10529073, 0.        ],\n",
       "       [0.05290728, 0.05263158],\n",
       "       [0.26348874, 0.05263158],\n",
       "       [0.42116291, 0.21052632],\n",
       "       [0.15819801, 0.47368421],\n",
       "       [0.10529073, 0.68421053],\n",
       "       [0.47354636, 1.        ],\n",
       "       [0.99633316, 0.21052632],\n",
       "       [0.31587218, 0.89473684],\n",
       "       [0.05290728, 0.21052632],\n",
       "       [0.84127816, 0.26315789],\n",
       "       [0.47302252, 0.36842105],\n",
       "       [0.47302252, 0.21052632],\n",
       "       [0.52592981, 0.26315789],\n",
       "       [0.31587218, 0.73684211],\n",
       "       [0.78313253, 0.94736842],\n",
       "       [0.26348874, 0.84210526],\n",
       "       [0.5243583 , 0.63157895],\n",
       "       [0.94552122, 0.10526316],\n",
       "       [0.89470927, 0.15789474],\n",
       "       [0.84023049, 0.89473684],\n",
       "       [0.57726558, 0.57894737],\n",
       "       [0.52540597, 0.89473684],\n",
       "       [0.10529073, 0.84210526],\n",
       "       [0.78941854, 0.78947368],\n",
       "       [0.73598743, 0.73684211],\n",
       "       [0.7370351 , 0.63157895],\n",
       "       [0.47302252, 0.05263158],\n",
       "       [0.21058146, 0.89473684],\n",
       "       [0.2629649 , 0.73684211],\n",
       "       [0.2629649 , 0.94736842],\n",
       "       [0.78837087, 0.05263158],\n",
       "       [0.42011524, 0.52631579],\n",
       "       [0.8936616 , 0.31578947],\n",
       "       [0.10529073, 0.73684211],\n",
       "       [0.42116291, 0.84210526],\n",
       "       [0.47354636, 0.73684211],\n",
       "       [0.47354636, 0.94736842],\n",
       "       [0.47354636, 0.10526316],\n",
       "       [0.83237297, 0.36842105],\n",
       "       [0.26348874, 0.15789474],\n",
       "       [0.57831325, 0.        ],\n",
       "       [0.31587218, 0.52631579],\n",
       "       [0.73651126, 0.52631579],\n",
       "       [0.15819801, 0.        ],\n",
       "       [0.21058146, 0.73684211],\n",
       "       [0.47354636, 0.68421053],\n",
       "       [0.36877947, 0.        ],\n",
       "       [0.3677318 , 0.42105263],\n",
       "       [0.52592981, 0.10526316],\n",
       "       [0.36877947, 0.63157895],\n",
       "       [0.74332111, 0.31578947],\n",
       "       [0.        , 0.52631579],\n",
       "       [0.68308015, 0.47368421],\n",
       "       [0.21058146, 0.84210526],\n",
       "       [0.52592981, 0.52631579],\n",
       "       [0.63122053, 0.        ],\n",
       "       [0.10529073, 0.89473684],\n",
       "       [0.78837087, 0.31578947],\n",
       "       [0.05290728, 0.84210526],\n",
       "       [0.89261393, 1.        ],\n",
       "       [0.05290728, 0.        ],\n",
       "       [0.89313777, 0.68421053],\n",
       "       [0.15819801, 0.78947368],\n",
       "       [0.52645364, 0.        ],\n",
       "       [0.15819801, 0.89473684],\n",
       "       [0.68360398, 0.15789474],\n",
       "       [0.97904662, 0.73684211],\n",
       "       [0.36825563, 0.47368421],\n",
       "       [0.84337349, 0.78947368],\n",
       "       [0.        , 0.10526316],\n",
       "       [0.10529073, 0.63157895],\n",
       "       [0.68412782, 0.10526316],\n",
       "       [0.62964903, 0.68421053],\n",
       "       [0.6322682 , 1.        ],\n",
       "       [0.47354636, 0.15789474],\n",
       "       [0.36825563, 0.57894737],\n",
       "       [0.94133054, 0.15789474],\n",
       "       [0.78889471, 0.10526316],\n",
       "       [0.47354636, 0.52631579],\n",
       "       [0.15819801, 0.73684211],\n",
       "       [0.62964903, 0.15789474],\n",
       "       [0.99895233, 0.31578947],\n",
       "       [0.21058146, 0.36842105],\n",
       "       [0.57831325, 1.        ],\n",
       "       [0.36825563, 0.15789474],\n",
       "       [0.84023049, 0.73684211],\n",
       "       [0.68360398, 0.31578947],\n",
       "       [0.73441592, 0.42105263],\n",
       "       [0.31587218, 0.84210526],\n",
       "       [0.42063908, 0.31578947],\n",
       "       [0.21058146, 0.15789474],\n",
       "       [0.88894709, 0.84210526],\n",
       "       [0.36825563, 0.78947368],\n",
       "       [0.68308015, 1.        ],\n",
       "       [0.31587218, 0.31578947],\n",
       "       [0.47302252, 0.78947368],\n",
       "       [0.68412782, 0.26315789],\n",
       "       [0.924044  , 0.47368421],\n",
       "       [0.2629649 , 0.21052632],\n",
       "       [0.36825563, 0.68421053],\n",
       "       [0.73651126, 0.10526316],\n",
       "       [0.57883709, 0.31578947],\n",
       "       [0.05290728, 0.26315789],\n",
       "       [0.47354636, 0.84210526],\n",
       "       [0.15819801, 0.21052632],\n",
       "       [0.        , 0.78947368],\n",
       "       [0.73598743, 0.89473684],\n",
       "       [0.2629649 , 0.47368421],\n",
       "       [0.84127816, 0.84210526],\n",
       "       [0.78784704, 0.42105263],\n",
       "       [0.05290728, 0.31578947],\n",
       "       [0.15819801, 0.84210526],\n",
       "       [0.10529073, 0.15789474],\n",
       "       [0.73598743, 0.        ],\n",
       "       [0.        , 0.15789474],\n",
       "       [0.94604505, 0.42105263],\n",
       "       [0.10529073, 0.47368421],\n",
       "       [0.99109481, 0.42105263],\n",
       "       [0.05290728, 0.52631579],\n",
       "       [0.31587218, 0.05263158],\n",
       "       [0.2629649 , 0.78947368],\n",
       "       [0.94342588, 0.68421053],\n",
       "       [0.52592981, 0.84210526],\n",
       "       [0.68412782, 0.21052632],\n",
       "       [0.52592981, 0.47368421],\n",
       "       [0.78837087, 0.84210526],\n",
       "       [0.        , 0.05263158],\n",
       "       [0.78889471, 0.68421053],\n",
       "       [0.05290728, 0.10526316],\n",
       "       [0.78837087, 0.89473684],\n",
       "       [0.78889471, 1.        ],\n",
       "       [0.        , 0.36842105],\n",
       "       [0.36877947, 0.05263158],\n",
       "       [0.21058146, 0.31578947],\n",
       "       [0.89313777, 0.63157895],\n",
       "       [0.63122053, 0.05263158],\n",
       "       [0.52592981, 0.78947368],\n",
       "       [0.15819801, 0.42105263],\n",
       "       [0.10529073, 0.52631579],\n",
       "       [0.36825563, 0.84210526],\n",
       "       [0.63122053, 0.63157895],\n",
       "       [0.89470927, 0.89473684],\n",
       "       [0.63017287, 0.73684211],\n",
       "       [0.68360398, 0.84210526],\n",
       "       [0.42116291, 0.94736842],\n",
       "       [0.73651126, 0.05263158],\n",
       "       [0.57831325, 0.89473684],\n",
       "       [0.15819801, 0.57894737],\n",
       "       [0.31587218, 0.47368421],\n",
       "       [0.36825563, 0.21052632],\n",
       "       [0.78103719, 0.78947368],\n",
       "       [0.31587218, 0.68421053],\n",
       "       [0.52592981, 0.15789474],\n",
       "       [0.9984285 , 0.68421053],\n",
       "       [0.63122053, 0.78947368],\n",
       "       [0.15819801, 0.63157895],\n",
       "       [1.        , 0.84210526],\n",
       "       [0.57831325, 0.78947368],\n",
       "       [0.05290728, 0.89473684]])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling training set data\n",
    "Xtrain2_minmax = min_max_scaler2.fit_transform(Xtrain2)\n",
    "Xtrain2_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "461721dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9984285 , 0.94736842],\n",
       "       [0.31587218, 0.26315789],\n",
       "       [0.84127816, 0.42105263],\n",
       "       [0.84180199, 1.        ],\n",
       "       [0.42063908, 0.63157895],\n",
       "       [0.8936616 , 0.10526316],\n",
       "       [0.47354636, 0.89473684],\n",
       "       [0.73493976, 0.57894737],\n",
       "       [0.05290728, 0.47368421],\n",
       "       [0.7370351 , 0.21052632],\n",
       "       [0.42011524, 0.73684211],\n",
       "       [0.93085385, 0.63157895],\n",
       "       [0.47354636, 0.42105263],\n",
       "       [0.84127816, 0.21052632],\n",
       "       [0.83551598, 0.05263158],\n",
       "       [0.57831325, 0.36842105],\n",
       "       [0.94552122, 0.57894737],\n",
       "       [0.        , 0.26315789],\n",
       "       [0.15819801, 0.94736842],\n",
       "       [0.57831325, 0.15789474],\n",
       "       [0.31639602, 0.10526316],\n",
       "       [0.63174437, 0.10526316],\n",
       "       [0.9984285 , 0.10526316],\n",
       "       [0.52645364, 0.73684211],\n",
       "       [0.        , 0.89473684],\n",
       "       [0.21058146, 0.63157895],\n",
       "       [0.94552122, 0.31578947],\n",
       "       [0.52592981, 0.05263158],\n",
       "       [0.94237821, 0.05263158],\n",
       "       [0.52488214, 0.36842105],\n",
       "       [0.21058146, 0.05263158],\n",
       "       [0.        , 0.21052632],\n",
       "       [0.42116291, 0.26315789],\n",
       "       [0.68308015, 0.78947368],\n",
       "       [0.        , 0.31578947],\n",
       "       [0.21058146, 0.        ],\n",
       "       [0.10529073, 0.94736842],\n",
       "       [0.26348874, 0.10526316],\n",
       "       [0.99423782, 0.89473684],\n",
       "       [0.31587218, 1.        ],\n",
       "       [0.68360398, 0.42105263],\n",
       "       [0.52592981, 0.21052632],\n",
       "       [0.42116291, 0.57894737],\n",
       "       [0.15819801, 0.10526316],\n",
       "       [0.99738083, 0.26315789],\n",
       "       [0.21058146, 0.26315789],\n",
       "       [0.93818753, 0.47368421],\n",
       "       [0.78837087, 0.36842105],\n",
       "       [0.7841802 , 0.73684211],\n",
       "       [0.68308015, 0.94736842],\n",
       "       [0.15819801, 0.26315789],\n",
       "       [0.57778942, 0.26315789],\n",
       "       [0.57831325, 0.47368421],\n",
       "       [0.        , 0.94736842],\n",
       "       [0.73598743, 0.84210526],\n",
       "       [0.73546359, 0.31578947],\n",
       "       [0.68308015, 0.63157895],\n",
       "       [0.26348874, 0.89473684],\n",
       "       [0.63122053, 0.94736842],\n",
       "       [0.94499738, 0.        ],\n",
       "       [0.2629649 , 0.36842105],\n",
       "       [0.15819801, 0.36842105],\n",
       "       [0.57831325, 0.73684211],\n",
       "       [0.63122053, 0.31578947],\n",
       "       [0.89104243, 0.73684211],\n",
       "       [0.57883709, 0.10526316],\n",
       "       [0.26348874, 0.31578947],\n",
       "       [0.78679937, 0.52631579],\n",
       "       [0.68308015, 0.52631579],\n",
       "       [0.78732321, 0.63157895],\n",
       "       [0.52592981, 0.94736842],\n",
       "       [0.21058146, 0.78947368],\n",
       "       [0.88737559, 0.52631579],\n",
       "       [0.98428497, 0.36842105],\n",
       "       [0.        , 0.57894737],\n",
       "       [0.89104243, 0.36842105],\n",
       "       [0.10529073, 0.05263158],\n",
       "       [0.47407019, 0.        ],\n",
       "       [0.31639602, 0.63157895],\n",
       "       [0.57831325, 0.21052632]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the same scaling to the test set data\n",
    "Xtest2_minmax = min_max_scaler2.transform(Xtest2)\n",
    "Xtest2_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "f07479a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Xtest2_tensor = torch.from_numpy(Xtest2_minmax).type(torch.float32)\n",
    "Ytest2_tensor = torch.from_numpy(Ytest2).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "badda5a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = BlockChainDataset(Xtrain2_minmax, Ytrain2)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e3238dff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# design model using class\n",
    "class Thr(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Thr, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "model1 = Thr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "cab7f50f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# construct loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "68714a7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# traning cycle forward, backward, update\n",
    "def train1(epoch):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        y_pred = model1(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * labels.shape[0]\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('epoch:', epoch + 1, 'train_loss:', train_loss / len(Xtrain2))\n",
    "        \n",
    "\n",
    "def test1():\n",
    "    with torch.no_grad():\n",
    "        y_pred = model1(Xtest2_tensor)\n",
    "        loss = criterion(y_pred, Ytest2_tensor)\n",
    "        print('test_loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "380e16d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 train_loss: 82.28491439819337\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 20 train_loss: 82.28492431640625\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 30 train_loss: 82.28492317199706\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 40 train_loss: 82.28492736816406\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 50 train_loss: 82.28490333557129\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 60 train_loss: 82.28489685058594\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 70 train_loss: 82.28491020202637\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 80 train_loss: 82.2849277496338\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 90 train_loss: 82.28492889404296\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 100 train_loss: 82.28492469787598\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 110 train_loss: 82.28492088317871\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 120 train_loss: 82.28490409851074\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 130 train_loss: 82.28491172790527\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 140 train_loss: 82.28493728637696\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 150 train_loss: 82.28489608764649\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 160 train_loss: 82.28490600585937\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 170 train_loss: 82.28489799499512\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 180 train_loss: 82.28491020202637\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 190 train_loss: 82.2848964691162\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 200 train_loss: 82.28490524291992\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 210 train_loss: 82.28488883972167\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 220 train_loss: 82.28489570617675\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 230 train_loss: 82.284907913208\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 240 train_loss: 82.28490600585937\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 250 train_loss: 82.2848949432373\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 260 train_loss: 82.28489379882812\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 270 train_loss: 82.2848934173584\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 280 train_loss: 82.28490676879883\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 290 train_loss: 82.2848876953125\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 300 train_loss: 82.2848991394043\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 310 train_loss: 82.28489875793457\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 320 train_loss: 82.28489723205567\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 330 train_loss: 82.28490447998047\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 340 train_loss: 82.2848949432373\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 350 train_loss: 82.28488349914551\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 360 train_loss: 82.28489837646484\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 370 train_loss: 82.2848934173584\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 380 train_loss: 82.28489875793457\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 390 train_loss: 82.28489570617675\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 400 train_loss: 82.28489418029785\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 410 train_loss: 82.28488960266114\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 420 train_loss: 82.28488883972167\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 430 train_loss: 82.28488998413086\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 440 train_loss: 82.28487167358398\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 450 train_loss: 82.28488235473633\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 460 train_loss: 82.28488121032714\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 470 train_loss: 82.28487854003906\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 480 train_loss: 82.28487434387208\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 490 train_loss: 82.28487167358398\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 500 train_loss: 82.28486595153808\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 510 train_loss: 82.28488693237304\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 520 train_loss: 82.28489761352539\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 530 train_loss: 82.28487243652344\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 540 train_loss: 82.28487625122071\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 550 train_loss: 82.28487091064453\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 560 train_loss: 82.28487129211426\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 570 train_loss: 82.28486671447754\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 580 train_loss: 82.28486938476563\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 590 train_loss: 82.28488044738769\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 600 train_loss: 82.2848747253418\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 610 train_loss: 82.28488388061524\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 620 train_loss: 82.28487548828124\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 630 train_loss: 82.28484764099122\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 640 train_loss: 82.28487167358398\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 650 train_loss: 82.28486022949218\n",
      "test_loss: tensor(85.7230)\n",
      "epoch: 660 train_loss: 82.2848747253418\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 670 train_loss: 82.28485221862793\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 680 train_loss: 82.28486976623535\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 690 train_loss: 82.28485679626465\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 700 train_loss: 82.28485565185547\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 710 train_loss: 82.28485221862793\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 720 train_loss: 82.28484954833985\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 730 train_loss: 82.28486251831055\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 740 train_loss: 82.2848617553711\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 750 train_loss: 82.28485412597657\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 760 train_loss: 82.28486213684081\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 770 train_loss: 82.28485946655273\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 780 train_loss: 82.28486137390136\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 790 train_loss: 82.2848445892334\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 800 train_loss: 82.28483543395996\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 810 train_loss: 82.28485946655273\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 820 train_loss: 82.28485870361328\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 830 train_loss: 82.28485069274902\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 840 train_loss: 82.28485298156738\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 850 train_loss: 82.28485527038575\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 860 train_loss: 82.28485450744628\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 870 train_loss: 82.28487014770508\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 880 train_loss: 82.28483963012695\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 890 train_loss: 82.28484382629395\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 900 train_loss: 82.28483772277832\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 910 train_loss: 82.28484191894532\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 920 train_loss: 82.28485832214355\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 930 train_loss: 82.28490715026855\n",
      "test_loss: tensor(85.7221)\n",
      "epoch: 940 train_loss: 82.2848518371582\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 950 train_loss: 82.28485641479492\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 960 train_loss: 82.28484077453614\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 970 train_loss: 82.28483734130859\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 980 train_loss: 82.28482093811036\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 990 train_loss: 82.28484344482422\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1000 train_loss: 82.2848346710205\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1010 train_loss: 82.28483505249024\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1020 train_loss: 82.28482818603516\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1030 train_loss: 82.28482933044434\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1040 train_loss: 82.28482322692871\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1050 train_loss: 82.28482971191406\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1060 train_loss: 82.28483047485352\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 1070 train_loss: 82.2848289489746\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1080 train_loss: 82.28482856750489\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1090 train_loss: 82.2848461151123\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1100 train_loss: 82.28482322692871\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1110 train_loss: 82.28482055664062\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1120 train_loss: 82.28482437133789\n",
      "test_loss: tensor(85.7233)\n",
      "epoch: 1130 train_loss: 82.28481674194336\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1140 train_loss: 82.28481025695801\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1150 train_loss: 82.28480987548828\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1160 train_loss: 82.28481597900391\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1170 train_loss: 82.28482284545899\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1180 train_loss: 82.28481330871583\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1190 train_loss: 82.28482360839844\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1200 train_loss: 82.28480834960938\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1210 train_loss: 82.28481750488281\n",
      "test_loss: tensor(85.7236)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1220 train_loss: 82.28479461669922\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 1230 train_loss: 82.28480262756348\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1240 train_loss: 82.2848129272461\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1250 train_loss: 82.2847999572754\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1260 train_loss: 82.28480949401856\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1270 train_loss: 82.28481979370117\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1280 train_loss: 82.28480377197266\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1290 train_loss: 82.28480262756348\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1300 train_loss: 82.28480529785156\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1310 train_loss: 82.28481788635254\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1320 train_loss: 82.28480682373046\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1330 train_loss: 82.28480377197266\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1340 train_loss: 82.28475303649903\n",
      "test_loss: tensor(85.7238)\n",
      "epoch: 1350 train_loss: 82.28479232788087\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1360 train_loss: 82.28480224609375\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1370 train_loss: 82.28480377197266\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1380 train_loss: 82.28480339050293\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1390 train_loss: 82.28480072021485\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1400 train_loss: 82.28479537963867\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1410 train_loss: 82.28479652404785\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1420 train_loss: 82.28479690551758\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1430 train_loss: 82.28476905822754\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 1440 train_loss: 82.2847942352295\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1450 train_loss: 82.2847885131836\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1460 train_loss: 82.28477668762207\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 1470 train_loss: 82.28478927612305\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1480 train_loss: 82.28478317260742\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1490 train_loss: 82.28477897644044\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1500 train_loss: 82.28478164672852\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1510 train_loss: 82.28477745056152\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1520 train_loss: 82.28478622436523\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1530 train_loss: 82.28478660583497\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1540 train_loss: 82.28479194641113\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1550 train_loss: 82.28477554321289\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1560 train_loss: 82.2847999572754\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1570 train_loss: 82.28477478027344\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1580 train_loss: 82.28478088378907\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1590 train_loss: 82.2847999572754\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1600 train_loss: 82.2848014831543\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1610 train_loss: 82.28478164672852\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1620 train_loss: 82.28478202819824\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1630 train_loss: 82.28475875854492\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1640 train_loss: 82.28478889465332\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1650 train_loss: 82.28477592468262\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1660 train_loss: 82.2847686767578\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1670 train_loss: 82.28476676940917\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1680 train_loss: 82.28476600646972\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1690 train_loss: 82.2847728729248\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1700 train_loss: 82.2847396850586\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 1710 train_loss: 82.28478202819824\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1720 train_loss: 82.2847743988037\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1730 train_loss: 82.28475646972656\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1740 train_loss: 82.28477363586425\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1750 train_loss: 82.2847785949707\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1760 train_loss: 82.28476676940917\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1770 train_loss: 82.28475379943848\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1780 train_loss: 82.28475914001464\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1790 train_loss: 82.28476753234864\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1800 train_loss: 82.28476028442383\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1810 train_loss: 82.28477668762207\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1820 train_loss: 82.284716796875\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 1830 train_loss: 82.28477325439454\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1840 train_loss: 82.2847511291504\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1850 train_loss: 82.28475990295411\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1860 train_loss: 82.28475914001464\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1870 train_loss: 82.28476829528809\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1880 train_loss: 82.28476829528809\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1890 train_loss: 82.28474960327148\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1900 train_loss: 82.28476333618164\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1910 train_loss: 82.28476409912109\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1920 train_loss: 82.28474006652831\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1930 train_loss: 82.28475532531738\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 1940 train_loss: 82.284765625\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1950 train_loss: 82.28475341796874\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1960 train_loss: 82.28473854064941\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1970 train_loss: 82.28473052978515\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 1980 train_loss: 82.28474426269531\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 1990 train_loss: 82.2847526550293\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2000 train_loss: 82.28474998474121\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2010 train_loss: 82.28473243713378\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2020 train_loss: 82.28474578857421\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2030 train_loss: 82.2847354888916\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2040 train_loss: 82.28476676940917\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2050 train_loss: 82.28472175598145\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2060 train_loss: 82.28476257324219\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 2070 train_loss: 82.2847354888916\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2080 train_loss: 82.28475379943848\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2090 train_loss: 82.28459739685059\n",
      "test_loss: tensor(85.7241)\n",
      "epoch: 2100 train_loss: 82.28473358154297\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 2110 train_loss: 82.28474273681641\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2120 train_loss: 82.28473739624023\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2130 train_loss: 82.28472785949707\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2140 train_loss: 82.28472747802735\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2150 train_loss: 82.28473167419433\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2160 train_loss: 82.28474731445313\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2170 train_loss: 82.28473892211915\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2180 train_loss: 82.28472023010254\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2190 train_loss: 82.2847240447998\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2200 train_loss: 82.28470344543457\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2210 train_loss: 82.28473281860352\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2220 train_loss: 82.28472366333008\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2230 train_loss: 82.28472061157227\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2240 train_loss: 82.28471984863282\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2250 train_loss: 82.28473243713378\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2260 train_loss: 82.28471450805664\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2270 train_loss: 82.28471794128419\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2280 train_loss: 82.28471603393555\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2290 train_loss: 82.2847282409668\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2300 train_loss: 82.28471374511719\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2310 train_loss: 82.2847297668457\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2320 train_loss: 82.28470916748047\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2330 train_loss: 82.28471527099609\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2340 train_loss: 82.28471069335937\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2350 train_loss: 82.28468170166016\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 2360 train_loss: 82.2846752166748\n",
      "test_loss: tensor(85.7217)\n",
      "epoch: 2370 train_loss: 82.284716796875\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2380 train_loss: 82.28470840454102\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2390 train_loss: 82.28472862243652\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2400 train_loss: 82.28467750549316\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 2410 train_loss: 82.28471488952637\n",
      "test_loss: tensor(85.7236)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2420 train_loss: 82.28471794128419\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2430 train_loss: 82.28470153808594\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2440 train_loss: 82.28470115661621\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2450 train_loss: 82.28471336364746\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2460 train_loss: 82.28470230102539\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2470 train_loss: 82.28470573425292\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2480 train_loss: 82.28471183776855\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2490 train_loss: 82.28470878601074\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2500 train_loss: 82.28471412658692\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2510 train_loss: 82.28467636108398\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2520 train_loss: 82.28469734191894\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 2530 train_loss: 82.28470573425292\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2540 train_loss: 82.28470268249512\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2550 train_loss: 82.28471565246582\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2560 train_loss: 82.28469848632812\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2570 train_loss: 82.28467750549316\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 2580 train_loss: 82.28470039367676\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2590 train_loss: 82.28470573425292\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2600 train_loss: 82.28467407226563\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2610 train_loss: 82.28469924926758\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2620 train_loss: 82.28468818664551\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2630 train_loss: 82.28468627929688\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2640 train_loss: 82.2846694946289\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2650 train_loss: 82.28468704223633\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2660 train_loss: 82.28468284606933\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2670 train_loss: 82.28467826843261\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2680 train_loss: 82.28469047546386\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2690 train_loss: 82.28468399047851\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2700 train_loss: 82.28468627929688\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2710 train_loss: 82.28466491699218\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2720 train_loss: 82.28469161987304\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2730 train_loss: 82.2846851348877\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2740 train_loss: 82.28468284606933\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2750 train_loss: 82.2846836090088\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2760 train_loss: 82.28468551635743\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2770 train_loss: 82.28468818664551\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2780 train_loss: 82.28468132019043\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2790 train_loss: 82.28467140197753\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2800 train_loss: 82.28466033935547\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2810 train_loss: 82.28467826843261\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2820 train_loss: 82.28468055725098\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2830 train_loss: 82.2846809387207\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2840 train_loss: 82.28467445373535\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2850 train_loss: 82.28467254638672\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2860 train_loss: 82.28468322753906\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2870 train_loss: 82.28466987609863\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2880 train_loss: 82.28466606140137\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2890 train_loss: 82.28464813232422\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2900 train_loss: 82.28466453552247\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2910 train_loss: 82.28467445373535\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2920 train_loss: 82.28466682434082\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2930 train_loss: 82.28466033935547\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2940 train_loss: 82.28465461730957\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 2950 train_loss: 82.28466758728027\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 2960 train_loss: 82.28467330932617\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2970 train_loss: 82.28466606140137\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2980 train_loss: 82.28467826843261\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 2990 train_loss: 82.28464736938477\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3000 train_loss: 82.28465309143067\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3010 train_loss: 82.28465156555175\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3020 train_loss: 82.28466339111328\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3030 train_loss: 82.28466339111328\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3040 train_loss: 82.28463668823242\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3050 train_loss: 82.28466148376465\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3060 train_loss: 82.28465919494629\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3070 train_loss: 82.28465423583984\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3080 train_loss: 82.28465538024902\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3090 train_loss: 82.28463783264161\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3100 train_loss: 82.28465766906739\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 3110 train_loss: 82.28466491699218\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3120 train_loss: 82.2846466064453\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3130 train_loss: 82.28464698791504\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3140 train_loss: 82.28463706970214\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3150 train_loss: 82.28464317321777\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3160 train_loss: 82.28464851379394\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3170 train_loss: 82.28464393615722\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3180 train_loss: 82.28460578918457\n",
      "test_loss: tensor(85.7230)\n",
      "epoch: 3190 train_loss: 82.28464317321777\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3200 train_loss: 82.28462982177734\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3210 train_loss: 82.28463973999024\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3220 train_loss: 82.2846248626709\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3230 train_loss: 82.28463325500488\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3240 train_loss: 82.28464622497559\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3250 train_loss: 82.2846466064453\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3260 train_loss: 82.28463478088379\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3270 train_loss: 82.28463287353516\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3280 train_loss: 82.28464050292969\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3290 train_loss: 82.28464164733887\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3300 train_loss: 82.28462600708008\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3310 train_loss: 82.28463096618653\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3320 train_loss: 82.28459777832032\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3330 train_loss: 82.28464546203614\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3340 train_loss: 82.28463897705078\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3350 train_loss: 82.28462409973145\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3360 train_loss: 82.28460731506348\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3370 train_loss: 82.2846176147461\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3380 train_loss: 82.28463516235351\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3390 train_loss: 82.28462448120118\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3400 train_loss: 82.28462181091308\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3410 train_loss: 82.28463401794434\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3420 train_loss: 82.28463020324708\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3430 train_loss: 82.28462867736816\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3440 train_loss: 82.28462677001953\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3450 train_loss: 82.28461265563965\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3460 train_loss: 82.28461990356445\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3470 train_loss: 82.28465156555175\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3480 train_loss: 82.2846290588379\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3490 train_loss: 82.28462409973145\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3500 train_loss: 82.28460311889648\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3510 train_loss: 82.28460578918457\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3520 train_loss: 82.28461647033691\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3530 train_loss: 82.28461570739746\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3540 train_loss: 82.28461532592773\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3550 train_loss: 82.28460121154785\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3560 train_loss: 82.28460121154785\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3570 train_loss: 82.28460464477538\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3580 train_loss: 82.28460464477538\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3590 train_loss: 82.28461494445801\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3600 train_loss: 82.28459510803222\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3610 train_loss: 82.28459815979004\n",
      "test_loss: tensor(85.7233)\n",
      "epoch: 3620 train_loss: 82.28460502624512\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3630 train_loss: 82.28460426330567\n",
      "test_loss: tensor(85.7236)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3640 train_loss: 82.28458404541016\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3650 train_loss: 82.28460121154785\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3660 train_loss: 82.28460273742675\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3670 train_loss: 82.28460311889648\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3680 train_loss: 82.28458862304687\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3690 train_loss: 82.28454322814942\n",
      "test_loss: tensor(85.7239)\n",
      "epoch: 3700 train_loss: 82.28460884094238\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3710 train_loss: 82.28459663391114\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3720 train_loss: 82.28459014892579\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3730 train_loss: 82.28459358215332\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3740 train_loss: 82.2845848083496\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3750 train_loss: 82.28459281921387\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3760 train_loss: 82.28459243774414\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3770 train_loss: 82.28458251953126\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3780 train_loss: 82.28459129333496\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3790 train_loss: 82.28457374572754\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3800 train_loss: 82.28458595275879\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3810 train_loss: 82.28458595275879\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3820 train_loss: 82.28458366394042\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3830 train_loss: 82.28459320068359\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3840 train_loss: 82.28457641601562\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 3850 train_loss: 82.28457756042481\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3860 train_loss: 82.28456954956054\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3870 train_loss: 82.28459205627442\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3880 train_loss: 82.28458442687989\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3890 train_loss: 82.28458290100097\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3900 train_loss: 82.28458938598632\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3910 train_loss: 82.28458251953126\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3920 train_loss: 82.28456344604493\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3930 train_loss: 82.28457298278809\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 3940 train_loss: 82.28457984924316\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3950 train_loss: 82.2845615386963\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3960 train_loss: 82.28456573486328\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3970 train_loss: 82.2845672607422\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3980 train_loss: 82.28457412719726\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 3990 train_loss: 82.28457832336426\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4000 train_loss: 82.28456954956054\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4010 train_loss: 82.2845630645752\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4020 train_loss: 82.28455963134766\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4030 train_loss: 82.2845474243164\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4040 train_loss: 82.28454132080078\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 4050 train_loss: 82.28456764221191\n",
      "test_loss: tensor(85.7233)\n",
      "epoch: 4060 train_loss: 82.28457107543946\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4070 train_loss: 82.28456420898438\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4080 train_loss: 82.2845516204834\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4090 train_loss: 82.28455619812011\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4100 train_loss: 82.28455429077148\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4110 train_loss: 82.28456268310546\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4120 train_loss: 82.28455352783203\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4130 train_loss: 82.28455276489258\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4140 train_loss: 82.2845474243164\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4150 train_loss: 82.2845573425293\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4160 train_loss: 82.28454246520997\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4170 train_loss: 82.28456802368164\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4180 train_loss: 82.28454856872558\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4190 train_loss: 82.2845443725586\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4200 train_loss: 82.28455390930176\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4210 train_loss: 82.28454704284668\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4220 train_loss: 82.2845531463623\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4230 train_loss: 82.28455505371093\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4240 train_loss: 82.2845272064209\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4250 train_loss: 82.28454933166503\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4260 train_loss: 82.2845458984375\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4270 train_loss: 82.28440704345704\n",
      "test_loss: tensor(85.7240)\n",
      "epoch: 4280 train_loss: 82.28453674316407\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 4290 train_loss: 82.28453063964844\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4300 train_loss: 82.2845474243164\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4310 train_loss: 82.28453102111817\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4320 train_loss: 82.28452529907227\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4330 train_loss: 82.2845417022705\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4340 train_loss: 82.28452758789062\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4350 train_loss: 82.2845443725586\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4360 train_loss: 82.28453712463379\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4370 train_loss: 82.28453063964844\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4380 train_loss: 82.2845329284668\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4390 train_loss: 82.28453254699707\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4400 train_loss: 82.2845401763916\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4410 train_loss: 82.28452568054199\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4420 train_loss: 82.2845474243164\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4430 train_loss: 82.284521484375\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4440 train_loss: 82.28453636169434\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4450 train_loss: 82.2845443725586\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4460 train_loss: 82.2845230102539\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4470 train_loss: 82.28450202941895\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4480 train_loss: 82.28451690673828\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4490 train_loss: 82.28452835083007\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4500 train_loss: 82.28452072143554\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 4510 train_loss: 82.2845230102539\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4520 train_loss: 82.28452835083007\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4530 train_loss: 82.28451805114746\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4540 train_loss: 82.28452339172364\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4550 train_loss: 82.28451004028321\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4560 train_loss: 82.28451614379883\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4570 train_loss: 82.28451538085938\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4580 train_loss: 82.28451385498047\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4590 train_loss: 82.28451232910156\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4600 train_loss: 82.28451919555664\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4610 train_loss: 82.28450508117676\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4620 train_loss: 82.28450164794921\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4630 train_loss: 82.28451652526856\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4640 train_loss: 82.28450050354004\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4650 train_loss: 82.28451805114746\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4660 train_loss: 82.28450508117676\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4670 train_loss: 82.28449745178223\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4680 train_loss: 82.28450775146484\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4690 train_loss: 82.28450813293458\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4700 train_loss: 82.2845085144043\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4710 train_loss: 82.28448143005372\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 4720 train_loss: 82.28449783325195\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4730 train_loss: 82.28450813293458\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 4740 train_loss: 82.28451118469238\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4750 train_loss: 82.28450088500976\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4760 train_loss: 82.28449020385742\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4770 train_loss: 82.28449859619141\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4780 train_loss: 82.2844841003418\n",
      "test_loss: tensor(85.7229)\n",
      "epoch: 4790 train_loss: 82.28449440002441\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4800 train_loss: 82.28449935913086\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4810 train_loss: 82.28449783325195\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4820 train_loss: 82.28451118469238\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4830 train_loss: 82.28449745178223\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4840 train_loss: 82.28447914123535\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 4850 train_loss: 82.28448600769043\n",
      "test_loss: tensor(85.7236)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4860 train_loss: 82.28451995849609\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4870 train_loss: 82.28449478149415\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4880 train_loss: 82.28448867797852\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4890 train_loss: 82.28449020385742\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4900 train_loss: 82.28448486328125\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4910 train_loss: 82.28448143005372\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4920 train_loss: 82.28447341918945\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4930 train_loss: 82.28449172973633\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4940 train_loss: 82.28446960449219\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4950 train_loss: 82.28447608947754\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4960 train_loss: 82.28448638916015\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 4970 train_loss: 82.28450012207031\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4980 train_loss: 82.28446464538574\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 4990 train_loss: 82.28447875976562\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5000 train_loss: 82.28446350097656\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5010 train_loss: 82.28448486328125\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5020 train_loss: 82.2844783782959\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5030 train_loss: 82.28448638916015\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5040 train_loss: 82.28446960449219\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5050 train_loss: 82.28446311950684\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5060 train_loss: 82.28447875976562\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5070 train_loss: 82.28448371887207\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 5080 train_loss: 82.28447036743164\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5090 train_loss: 82.2844669342041\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5100 train_loss: 82.28445625305176\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5110 train_loss: 82.28446311950684\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5120 train_loss: 82.28447494506835\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5130 train_loss: 82.28447761535645\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5140 train_loss: 82.28445663452149\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5150 train_loss: 82.28445816040039\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5160 train_loss: 82.28445892333984\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5170 train_loss: 82.28447914123535\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5180 train_loss: 82.28446769714355\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5190 train_loss: 82.28446159362792\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5200 train_loss: 82.28446006774902\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5210 train_loss: 82.28447151184082\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5220 train_loss: 82.2844669342041\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5230 train_loss: 82.28445777893066\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5240 train_loss: 82.28444900512696\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5250 train_loss: 82.28444938659668\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5260 train_loss: 82.28446960449219\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5270 train_loss: 82.28446006774902\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5280 train_loss: 82.2844627380371\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5290 train_loss: 82.28444976806641\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5300 train_loss: 82.28445320129394\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5310 train_loss: 82.28445053100586\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5320 train_loss: 82.28444175720215\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5330 train_loss: 82.28443336486816\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5340 train_loss: 82.28443756103516\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5350 train_loss: 82.28444633483886\n",
      "test_loss: tensor(85.7231)\n",
      "epoch: 5360 train_loss: 82.28444442749023\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5370 train_loss: 82.28444213867188\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5380 train_loss: 82.28444595336914\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5390 train_loss: 82.28445358276367\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5400 train_loss: 82.28444633483886\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5410 train_loss: 82.28445091247559\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5420 train_loss: 82.284428024292\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5430 train_loss: 82.28444442749023\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5440 train_loss: 82.2844425201416\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5450 train_loss: 82.28443756103516\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5460 train_loss: 82.28443489074706\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5470 train_loss: 82.28443450927735\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5480 train_loss: 82.28444328308106\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5490 train_loss: 82.2844482421875\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5500 train_loss: 82.28442764282227\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5510 train_loss: 82.28442993164063\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5520 train_loss: 82.2844139099121\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5530 train_loss: 82.2844165802002\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5540 train_loss: 82.2844394683838\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5550 train_loss: 82.28443260192871\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5560 train_loss: 82.28443222045898\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5570 train_loss: 82.28442916870117\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 5580 train_loss: 82.28444900512696\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5590 train_loss: 82.2844295501709\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5600 train_loss: 82.28442764282227\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5610 train_loss: 82.28440895080567\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5620 train_loss: 82.28441009521484\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5630 train_loss: 82.28441162109375\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5640 train_loss: 82.28439903259277\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5650 train_loss: 82.2844181060791\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5660 train_loss: 82.28440208435059\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5670 train_loss: 82.2844181060791\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5680 train_loss: 82.2844165802002\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5690 train_loss: 82.28443489074706\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5700 train_loss: 82.28442497253418\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5710 train_loss: 82.28440170288086\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5720 train_loss: 82.28441200256347\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5730 train_loss: 82.28440361022949\n",
      "test_loss: tensor(85.7231)\n",
      "epoch: 5740 train_loss: 82.28442344665527\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5750 train_loss: 82.28441734313965\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5760 train_loss: 82.28441543579102\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5770 train_loss: 82.28442420959473\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5780 train_loss: 82.28442611694337\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5790 train_loss: 82.28440170288086\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5800 train_loss: 82.28440475463867\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5810 train_loss: 82.28440895080567\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5820 train_loss: 82.28441696166992\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5830 train_loss: 82.2844009399414\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5840 train_loss: 82.2844051361084\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5850 train_loss: 82.28437614440918\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 5860 train_loss: 82.28439559936524\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5870 train_loss: 82.28440589904785\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5880 train_loss: 82.28440399169922\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5890 train_loss: 82.28439483642578\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5900 train_loss: 82.28439865112304\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 5910 train_loss: 82.28440284729004\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5920 train_loss: 82.28439712524414\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5930 train_loss: 82.28438720703124\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5940 train_loss: 82.28436164855957\n",
      "test_loss: tensor(85.7225)\n",
      "epoch: 5950 train_loss: 82.28439712524414\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5960 train_loss: 82.2844024658203\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5970 train_loss: 82.2843849182129\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 5980 train_loss: 82.28447189331055\n",
      "test_loss: tensor(85.7218)\n",
      "epoch: 5990 train_loss: 82.28437004089355\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 6000 train_loss: 82.28438911437988\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6010 train_loss: 82.28439750671387\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6020 train_loss: 82.28440017700196\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6030 train_loss: 82.28439216613769\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6040 train_loss: 82.28438415527344\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6050 train_loss: 82.28437004089355\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6060 train_loss: 82.28438301086426\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 6070 train_loss: 82.28439064025879\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6080 train_loss: 82.2843807220459\n",
      "test_loss: tensor(85.7235)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6090 train_loss: 82.28440704345704\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 6100 train_loss: 82.28437843322754\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6110 train_loss: 82.28438262939453\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6120 train_loss: 82.28438377380371\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6130 train_loss: 82.28440017700196\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 6140 train_loss: 82.28436889648438\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6150 train_loss: 82.28436279296875\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6160 train_loss: 82.28438186645508\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6170 train_loss: 82.28437194824218\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6180 train_loss: 82.28436584472657\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6190 train_loss: 82.28435859680175\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6200 train_loss: 82.28439254760742\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6210 train_loss: 82.28437309265136\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6220 train_loss: 82.28436813354492\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6230 train_loss: 82.28435745239258\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6240 train_loss: 82.28434448242187\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6250 train_loss: 82.28437080383301\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6260 train_loss: 82.28435668945312\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6270 train_loss: 82.28439445495606\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 6280 train_loss: 82.28434753417969\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6290 train_loss: 82.28437385559081\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6300 train_loss: 82.2843578338623\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6310 train_loss: 82.28436164855957\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6320 train_loss: 82.28435020446777\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6330 train_loss: 82.28435592651367\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6340 train_loss: 82.28435249328614\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 6350 train_loss: 82.28436431884765\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6360 train_loss: 82.28436813354492\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6370 train_loss: 82.28434600830079\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6380 train_loss: 82.28435592651367\n",
      "test_loss: tensor(85.7230)\n",
      "epoch: 6390 train_loss: 82.2843521118164\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6400 train_loss: 82.28436965942383\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6410 train_loss: 82.28435592651367\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6420 train_loss: 82.28434295654297\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6430 train_loss: 82.28434867858887\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6440 train_loss: 82.28436012268067\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6450 train_loss: 82.28435173034669\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6460 train_loss: 82.28435707092285\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6470 train_loss: 82.28433189392089\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6480 train_loss: 82.28433074951172\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6490 train_loss: 82.28435935974122\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6500 train_loss: 82.28435096740722\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6510 train_loss: 82.2843448638916\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6520 train_loss: 82.28432388305664\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6530 train_loss: 82.2843391418457\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6540 train_loss: 82.28436546325683\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 6550 train_loss: 82.28434371948242\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6560 train_loss: 82.28433036804199\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6570 train_loss: 82.28434028625489\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6580 train_loss: 82.28433418273926\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6590 train_loss: 82.28433990478516\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6600 train_loss: 82.284326171875\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6610 train_loss: 82.28432273864746\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6620 train_loss: 82.28433799743652\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6630 train_loss: 82.28431968688965\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6640 train_loss: 82.28433456420899\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6650 train_loss: 82.28433570861816\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6660 train_loss: 82.28433074951172\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6670 train_loss: 82.28431777954101\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6680 train_loss: 82.28432807922363\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6690 train_loss: 82.28433303833008\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6700 train_loss: 82.2843189239502\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6710 train_loss: 82.28430862426758\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6720 train_loss: 82.28432922363281\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6730 train_loss: 82.28433036804199\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6740 train_loss: 82.28432579040528\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6750 train_loss: 82.28432083129883\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6760 train_loss: 82.28431625366211\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6770 train_loss: 82.28430976867676\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6780 train_loss: 82.28431510925293\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6790 train_loss: 82.28432350158691\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6800 train_loss: 82.2843116760254\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6810 train_loss: 82.2843090057373\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6820 train_loss: 82.28430213928223\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6830 train_loss: 82.28431434631348\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 6840 train_loss: 82.28431015014648\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6850 train_loss: 82.28430519104003\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6860 train_loss: 82.2843132019043\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6870 train_loss: 82.28431625366211\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6880 train_loss: 82.28430290222168\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6890 train_loss: 82.28430633544922\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6900 train_loss: 82.28430137634277\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6910 train_loss: 82.28430061340332\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6920 train_loss: 82.28429908752442\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 6930 train_loss: 82.28430366516113\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 6940 train_loss: 82.28431129455566\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6950 train_loss: 82.28429870605468\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6960 train_loss: 82.28429069519044\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6970 train_loss: 82.28430061340332\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6980 train_loss: 82.2843002319336\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 6990 train_loss: 82.2842861175537\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 7000 train_loss: 82.28427658081054\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7010 train_loss: 82.28429260253907\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7020 train_loss: 82.28429069519044\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7030 train_loss: 82.28429679870605\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7040 train_loss: 82.28430252075195\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7050 train_loss: 82.28429794311523\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7060 train_loss: 82.28428764343262\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7070 train_loss: 82.28429412841797\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7080 train_loss: 82.28440322875977\n",
      "test_loss: tensor(85.7228)\n",
      "epoch: 7090 train_loss: 82.28428344726562\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 7100 train_loss: 82.28426361083984\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7110 train_loss: 82.28429527282715\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7120 train_loss: 82.28429489135742\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7130 train_loss: 82.28428535461425\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7140 train_loss: 82.28428535461425\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7150 train_loss: 82.28428535461425\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7160 train_loss: 82.2842887878418\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7170 train_loss: 82.2842845916748\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7180 train_loss: 82.2842975616455\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7190 train_loss: 82.28428115844727\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7200 train_loss: 82.28427963256836\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7210 train_loss: 82.28428382873535\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7220 train_loss: 82.28429374694824\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7230 train_loss: 82.28427162170411\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7240 train_loss: 82.28428955078125\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7250 train_loss: 82.28427963256836\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7260 train_loss: 82.28428115844727\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7270 train_loss: 82.28426475524903\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7280 train_loss: 82.28427352905274\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7290 train_loss: 82.28428802490234\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7300 train_loss: 82.28426780700684\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7310 train_loss: 82.28426780700684\n",
      "test_loss: tensor(85.7235)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7320 train_loss: 82.28426246643066\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7330 train_loss: 82.28427162170411\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7340 train_loss: 82.28427124023438\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7350 train_loss: 82.28427352905274\n",
      "test_loss: tensor(85.7231)\n",
      "epoch: 7360 train_loss: 82.28427314758301\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7370 train_loss: 82.28425445556641\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7380 train_loss: 82.28425636291504\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7390 train_loss: 82.28426055908203\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7400 train_loss: 82.28427162170411\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7410 train_loss: 82.2841251373291\n",
      "test_loss: tensor(85.7240)\n",
      "epoch: 7420 train_loss: 82.28425827026368\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 7430 train_loss: 82.2842945098877\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 7440 train_loss: 82.28424835205078\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 7450 train_loss: 82.28424644470215\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7460 train_loss: 82.28425216674805\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7470 train_loss: 82.28426017761231\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7480 train_loss: 82.2842571258545\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7490 train_loss: 82.2842803955078\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7500 train_loss: 82.28424034118652\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7510 train_loss: 82.28425369262695\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7520 train_loss: 82.28423919677735\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7530 train_loss: 82.28427581787109\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7540 train_loss: 82.28426246643066\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7550 train_loss: 82.28423652648925\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7560 train_loss: 82.28424797058105\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7570 train_loss: 82.2842643737793\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7580 train_loss: 82.28425636291504\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7590 train_loss: 82.2842357635498\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7600 train_loss: 82.28423461914062\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7610 train_loss: 82.28423805236817\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7620 train_loss: 82.28424377441407\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7630 train_loss: 82.2842357635498\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7640 train_loss: 82.28424530029297\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7650 train_loss: 82.28424491882325\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7660 train_loss: 82.28424606323242\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7670 train_loss: 82.28423156738282\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7680 train_loss: 82.28425331115723\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7690 train_loss: 82.28423843383788\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7700 train_loss: 82.28424186706543\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7710 train_loss: 82.2842170715332\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7720 train_loss: 82.28424339294433\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7730 train_loss: 82.28424911499023\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7740 train_loss: 82.28423309326172\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7750 train_loss: 82.28422012329102\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7760 train_loss: 82.28422355651855\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7770 train_loss: 82.2842227935791\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7780 train_loss: 82.28424186706543\n",
      "test_loss: tensor(85.7233)\n",
      "epoch: 7790 train_loss: 82.28423767089843\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7800 train_loss: 82.28423194885254\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7810 train_loss: 82.28422660827637\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7820 train_loss: 82.28422813415527\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7830 train_loss: 82.28422966003419\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7840 train_loss: 82.2842342376709\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7850 train_loss: 82.28424110412598\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7860 train_loss: 82.28421058654786\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7870 train_loss: 82.28422393798829\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7880 train_loss: 82.28421325683594\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 7890 train_loss: 82.28422546386719\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7900 train_loss: 82.28422203063965\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7910 train_loss: 82.28421325683594\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7920 train_loss: 82.28422241210937\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7930 train_loss: 82.28422698974609\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7940 train_loss: 82.28422317504882\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7950 train_loss: 82.28420639038086\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7960 train_loss: 82.28419876098633\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7970 train_loss: 82.28421096801758\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 7980 train_loss: 82.28421173095703\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 7990 train_loss: 82.28421592712402\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 8000 train_loss: 82.28421554565429\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8010 train_loss: 82.28421401977539\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8020 train_loss: 82.28422660827637\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8030 train_loss: 82.28422203063965\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8040 train_loss: 82.28421211242676\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8050 train_loss: 82.28421020507812\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 8060 train_loss: 82.28420600891113\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 8070 train_loss: 82.28419570922851\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8080 train_loss: 82.28420753479004\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8090 train_loss: 82.28420066833496\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8100 train_loss: 82.28420867919922\n",
      "test_loss: tensor(85.7233)\n",
      "epoch: 8110 train_loss: 82.28420219421386\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8120 train_loss: 82.28420906066894\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8130 train_loss: 82.28420219421386\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8140 train_loss: 82.28420028686523\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8150 train_loss: 82.2841926574707\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8160 train_loss: 82.2842025756836\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 8170 train_loss: 82.28420219421386\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 8180 train_loss: 82.28420028686523\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8190 train_loss: 82.28430976867676\n",
      "test_loss: tensor(85.7228)\n",
      "epoch: 8200 train_loss: 82.28418006896973\n",
      "test_loss: tensor(85.7233)\n",
      "epoch: 8210 train_loss: 82.28419876098633\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8220 train_loss: 82.28419303894043\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8230 train_loss: 82.28417472839355\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8240 train_loss: 82.28416938781739\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8250 train_loss: 82.2841854095459\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8260 train_loss: 82.28418769836426\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8270 train_loss: 82.284183883667\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8280 train_loss: 82.28417778015137\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8290 train_loss: 82.28418083190918\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8300 train_loss: 82.284183883667\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8310 train_loss: 82.28419189453125\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8320 train_loss: 82.28417739868163\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 8330 train_loss: 82.28416938781739\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8340 train_loss: 82.28417091369629\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 8350 train_loss: 82.28418197631837\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8360 train_loss: 82.284183883667\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8370 train_loss: 82.28417053222657\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8380 train_loss: 82.28417472839355\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8390 train_loss: 82.28417282104492\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8400 train_loss: 82.28417930603027\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8410 train_loss: 82.2841781616211\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8420 train_loss: 82.28417510986328\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8430 train_loss: 82.28416557312012\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8440 train_loss: 82.28416175842285\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8450 train_loss: 82.28419075012206\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 8460 train_loss: 82.284175491333\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8470 train_loss: 82.28416557312012\n",
      "test_loss: tensor(85.7232)\n",
      "epoch: 8480 train_loss: 82.2841567993164\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8490 train_loss: 82.284183883667\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8500 train_loss: 82.28417205810547\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8510 train_loss: 82.28415451049804\n",
      "test_loss: tensor(85.7235)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8520 train_loss: 82.28417434692383\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8530 train_loss: 82.28416519165039\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 8540 train_loss: 82.28417358398437\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 8550 train_loss: 82.28416481018067\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8560 train_loss: 82.28417282104492\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8570 train_loss: 82.28415870666504\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8580 train_loss: 82.28416442871094\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8590 train_loss: 82.28416213989257\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8600 train_loss: 82.28416595458984\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8610 train_loss: 82.28415222167969\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8620 train_loss: 82.28415756225586\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 8630 train_loss: 82.28414115905761\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8640 train_loss: 82.28417053222657\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8650 train_loss: 82.28416519165039\n",
      "test_loss: tensor(85.7233)\n",
      "epoch: 8660 train_loss: 82.28414840698242\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8670 train_loss: 82.28414573669434\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8680 train_loss: 82.28415451049804\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8690 train_loss: 82.28415718078614\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8700 train_loss: 82.28414611816406\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8710 train_loss: 82.28414039611816\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8720 train_loss: 82.2841567993164\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8730 train_loss: 82.28415107727051\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8740 train_loss: 82.28415374755859\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8750 train_loss: 82.28414802551269\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8760 train_loss: 82.28413887023926\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8770 train_loss: 82.28414382934571\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8780 train_loss: 82.28412437438965\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 8790 train_loss: 82.28414382934571\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8800 train_loss: 82.28413619995118\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8810 train_loss: 82.28412704467773\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8820 train_loss: 82.28413543701171\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8830 train_loss: 82.28413848876953\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8840 train_loss: 82.28413696289063\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8850 train_loss: 82.28413047790528\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 8860 train_loss: 82.28412895202636\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8870 train_loss: 82.28412780761718\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8880 train_loss: 82.28413696289063\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8890 train_loss: 82.28413467407226\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8900 train_loss: 82.28412055969238\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8910 train_loss: 82.28412094116212\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8920 train_loss: 82.28413619995118\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8930 train_loss: 82.28412475585938\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8940 train_loss: 82.28413887023926\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8950 train_loss: 82.28411865234375\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8960 train_loss: 82.28412094116212\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8970 train_loss: 82.28412132263183\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 8980 train_loss: 82.28424301147462\n",
      "test_loss: tensor(85.7227)\n",
      "epoch: 8990 train_loss: 82.28412780761718\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9000 train_loss: 82.2841121673584\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9010 train_loss: 82.28413581848145\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9020 train_loss: 82.28413124084473\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9030 train_loss: 82.28411254882812\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9040 train_loss: 82.28412284851075\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9050 train_loss: 82.28410911560059\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9060 train_loss: 82.28412742614746\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9070 train_loss: 82.28411750793457\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9080 train_loss: 82.28408813476562\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 9090 train_loss: 82.28410148620605\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9100 train_loss: 82.28411178588867\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9110 train_loss: 82.28412780761718\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9120 train_loss: 82.28409767150879\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9130 train_loss: 82.28409767150879\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9140 train_loss: 82.28410568237305\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9150 train_loss: 82.28410835266114\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9160 train_loss: 82.28411750793457\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9170 train_loss: 82.28410530090332\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9180 train_loss: 82.28411178588867\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9190 train_loss: 82.2840534210205\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 9200 train_loss: 82.28408775329589\n",
      "test_loss: tensor(85.7231)\n",
      "epoch: 9210 train_loss: 82.28409767150879\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9220 train_loss: 82.28410148620605\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9230 train_loss: 82.28411140441895\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9240 train_loss: 82.28410606384277\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9250 train_loss: 82.28409347534179\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9260 train_loss: 82.28408813476562\n",
      "test_loss: tensor(85.7232)\n",
      "epoch: 9270 train_loss: 82.2841022491455\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9280 train_loss: 82.28410110473632\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9290 train_loss: 82.28410873413085\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9300 train_loss: 82.28409194946289\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9310 train_loss: 82.28408470153809\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9320 train_loss: 82.28409385681152\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9330 train_loss: 82.28410720825195\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9340 train_loss: 82.28408470153809\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9350 train_loss: 82.28408889770508\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9360 train_loss: 82.28408317565918\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9370 train_loss: 82.28408851623536\n",
      "test_loss: tensor(85.7233)\n",
      "epoch: 9380 train_loss: 82.28409423828126\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9390 train_loss: 82.28409385681152\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9400 train_loss: 82.2840805053711\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9410 train_loss: 82.28408622741699\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9420 train_loss: 82.28408889770508\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9430 train_loss: 82.28409004211426\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9440 train_loss: 82.28407859802246\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9450 train_loss: 82.28407402038575\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9460 train_loss: 82.28408393859863\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9470 train_loss: 82.28408279418946\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9480 train_loss: 82.28407402038575\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9490 train_loss: 82.2840747833252\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9500 train_loss: 82.28409538269042\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9510 train_loss: 82.28405151367187\n",
      "test_loss: tensor(85.7237)\n",
      "epoch: 9520 train_loss: 82.28418617248535\n",
      "test_loss: tensor(85.7228)\n",
      "epoch: 9530 train_loss: 82.28406982421875\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9540 train_loss: 82.28406028747558\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9550 train_loss: 82.28407096862793\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9560 train_loss: 82.28408088684083\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9570 train_loss: 82.28407440185546\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9580 train_loss: 82.28406524658203\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9590 train_loss: 82.28405876159668\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9600 train_loss: 82.2840705871582\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9610 train_loss: 82.28407516479493\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9620 train_loss: 82.28405570983887\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9630 train_loss: 82.28405876159668\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9640 train_loss: 82.28406219482422\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9650 train_loss: 82.28406410217285\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9660 train_loss: 82.28405876159668\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9670 train_loss: 82.28406066894532\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9680 train_loss: 82.28406066894532\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9690 train_loss: 82.28405876159668\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9700 train_loss: 82.2840732574463\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9710 train_loss: 82.28406066894532\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9720 train_loss: 82.28405380249023\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9730 train_loss: 82.28405456542968\n",
      "test_loss: tensor(85.7235)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9740 train_loss: 82.28406295776367\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9750 train_loss: 82.28405838012695\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9760 train_loss: 82.28405113220215\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9770 train_loss: 82.28404273986817\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9780 train_loss: 82.28403282165527\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9790 train_loss: 82.28405532836913\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9800 train_loss: 82.2840503692627\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9810 train_loss: 82.28406257629395\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9820 train_loss: 82.2840560913086\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9830 train_loss: 82.2840503692627\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9840 train_loss: 82.28404426574707\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9850 train_loss: 82.28404006958007\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9860 train_loss: 82.28404006958007\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9870 train_loss: 82.28404579162597\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9880 train_loss: 82.2840389251709\n",
      "test_loss: tensor(85.7232)\n",
      "epoch: 9890 train_loss: 82.28403854370117\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9900 train_loss: 82.28404541015625\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9910 train_loss: 82.28403739929199\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9920 train_loss: 82.28405265808105\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 9930 train_loss: 82.28404273986817\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9940 train_loss: 82.2840576171875\n",
      "test_loss: tensor(85.7234)\n",
      "epoch: 9950 train_loss: 82.2840187072754\n",
      "test_loss: tensor(85.7236)\n",
      "epoch: 9960 train_loss: 82.28404541015625\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9970 train_loss: 82.28402366638184\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9980 train_loss: 82.28402633666992\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 9990 train_loss: 82.28403167724609\n",
      "test_loss: tensor(85.7235)\n",
      "epoch: 10000 train_loss: 82.28402519226074\n",
      "test_loss: tensor(85.7235)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for epoch in range(10000):\n",
    "        train1(epoch)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model1.eval()\n",
    "            test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "dd55e74f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model1(Xtest2_tensor)\n",
    "y = pd.concat([pd.Series(Xtest2[:, 0].reshape(-1), name='send rates'), pd.Series(Xtest2[:, 1].reshape(-1), name='block size'), \n",
    "               pd.Series(Ytest2_tensor.numpy().reshape(-1), name='throughput_true'), \n",
    "               pd.Series(y_pred.numpy().reshape(-1), name='throughput_pred')], axis=1)\n",
    "y.to_csv('../../Data/Result/Related3/throughput_true_pred_related3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "5557c67f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>block size</th>\n",
       "      <th>throughput_true</th>\n",
       "      <th>throughput_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>160.4</td>\n",
       "      <td>161.16898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70.3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>63.3</td>\n",
       "      <td>61.12558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170.6</td>\n",
       "      <td>90.0</td>\n",
       "      <td>124.9</td>\n",
       "      <td>136.47790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170.7</td>\n",
       "      <td>200.0</td>\n",
       "      <td>160.4</td>\n",
       "      <td>139.15407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.3</td>\n",
       "      <td>130.0</td>\n",
       "      <td>81.6</td>\n",
       "      <td>77.66512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   send rates  block size  throughput_true  throughput_pred\n",
       "0       200.6       190.0            160.4        161.16898\n",
       "1        70.3        60.0             63.3         61.12558\n",
       "2       170.6        90.0            124.9        136.47790\n",
       "3       170.7       200.0            160.4        139.15407\n",
       "4        90.3       130.0             81.6         77.66512"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the test set MAE RMSE MAPE\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../../Data/Result/Related3/throughput_true_pred_related3.csv')\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "275fbb3c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([160.4,  63.3, 124.9, 160.4,  81.6, 131.3,  97.1, 117.7,  20. ,\n",
       "       118.7,  81.1, 162.5,  83.4, 129.8, 126.2,  98.8, 135.4,  10. ,\n",
       "        37.3,  99.2,  61.8, 103.4, 140.4,  96.2,   9.9,  48.2, 135.2,\n",
       "       110. , 135.9,  92.4,  50.1,   9.9,  78.3, 122.1,   9.8,  50.1,\n",
       "        28.8,  54. , 157.2,  69.9, 109.2, 109.6,  77.9,  37.3, 139.4,\n",
       "        47. , 134.6, 119.5, 119.5, 122.1,  38.3,  99.1, 118.9,   9.9,\n",
       "       120.4, 116.4, 121.3,  59.3, 121. , 135.1,  55.2,  38.3,  96.6,\n",
       "       104.2, 158.9,  96.9,  54.3, 118.1, 109.4, 122. ,  96.6,  48.6,\n",
       "       125.5, 135.2,   9.9, 128.2,  30.1, 100.1,  61.5,  99.8])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['throughput_true'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "a5823ab8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([161.16898 ,  61.12558 , 136.4779  , 139.15407 ,  77.66512 ,\n",
       "       142.50072 ,  86.3641  , 122.0803  ,  24.713081, 120.72233 ,\n",
       "        78.06376 , 150.14973 ,  84.235405, 135.53183 , 134.00365 ,\n",
       "        98.88279 , 151.99696 ,  16.250626,  41.8001  ,  97.93671 ,\n",
       "        60.490433, 105.29097 , 157.38463 ,  93.170906,  19.088888,\n",
       "        47.82291 , 150.81435 ,  90.02171 , 149.18523 ,  91.29201 ,\n",
       "        45.221176,  16.014105,  76.08389 , 115.658875,  16.487146,\n",
       "        44.984653,  34.28373 ,  52.974064, 160.3371  ,  64.43689 ,\n",
       "       114.07763 ,  90.73128 ,  77.50302 ,  38.01575 , 157.94536 ,\n",
       "        46.167263, 150.48204 , 128.72502 , 129.78532 , 116.36843 ,\n",
       "        38.725315,  98.335335,  99.35583 ,  19.325409, 123.41176 ,\n",
       "       120.97213 , 114.94929 ,  56.521893, 109.00089 , 149.32079 ,\n",
       "        54.082256,  39.198357, 100.538445, 106.16264 , 144.96687 ,\n",
       "        97.77461 ,  53.920155, 129.21132 , 114.47626 , 129.75877 ,\n",
       "        94.04258 ,  48.532475, 143.49986 , 156.5579  ,  17.669756,\n",
       "       143.31123 ,  30.262856,  82.417656,  62.855656,  98.173225])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = data['throughput_pred'].values\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5633d8da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "7946ad5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  6.8966246249999985\n",
      "RMSE:  9.258698981420968\n",
      "MAPE:  0.11773896126117259\n"
     ]
    }
   ],
   "source": [
    "MAE = metrics.mean_absolute_error(y, y_hat)\n",
    "RMSE = metrics.mean_squared_error(y, y_hat) ** 0.5\n",
    "MAPE = metrics.mean_absolute_percentage_error(y, y_hat)\n",
    "print('MAE: ', MAE)\n",
    "print('RMSE: ', RMSE)\n",
    "print('MAPE: ', MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099e69e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Choose the optimal block size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3dc70",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MLP1 to predict latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "4f7783b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TPS = []\n",
    "for i in range(10,210,10):\n",
    "    for j in range(20):\n",
    "        TPS.append(i)\n",
    "TPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "cf0d1fdd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200,\n",
       " 10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BS = list(range(10, 210, 10)) * 20\n",
    "BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "d289723b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "4915dbef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Xp': [10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  20,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  30,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  60,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  80,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  90,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  110,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  120,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  130,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  140,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  150,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  160,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  170,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  180,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  190,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  200],\n",
       " 'BS': [10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200,\n",
       "  10,\n",
       "  20,\n",
       "  30,\n",
       "  40,\n",
       "  50,\n",
       "  60,\n",
       "  70,\n",
       "  80,\n",
       "  90,\n",
       "  100,\n",
       "  110,\n",
       "  120,\n",
       "  130,\n",
       "  140,\n",
       "  150,\n",
       "  160,\n",
       "  170,\n",
       "  180,\n",
       "  190,\n",
       "  200]}"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XpBS = {\"Xp\": TPS, \"BS\": BS}\n",
    "XpBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "aa6a6f04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Xp</th>\n",
       "      <th>BS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>200</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>200</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>200</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Xp   BS\n",
       "0     10   10\n",
       "1     10   20\n",
       "2     10   30\n",
       "3     10   40\n",
       "4     10   50\n",
       "..   ...  ...\n",
       "395  200  160\n",
       "396  200  170\n",
       "397  200  180\n",
       "398  200  190\n",
       "399  200  200\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XpBS = pd.DataFrame(XpBS)\n",
    "XpBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "262e54c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10,  10],\n",
       "       [ 10,  20],\n",
       "       [ 10,  30],\n",
       "       [ 10,  40],\n",
       "       [ 10,  50],\n",
       "       [ 10,  60],\n",
       "       [ 10,  70],\n",
       "       [ 10,  80],\n",
       "       [ 10,  90],\n",
       "       [ 10, 100],\n",
       "       [ 10, 110],\n",
       "       [ 10, 120],\n",
       "       [ 10, 130],\n",
       "       [ 10, 140],\n",
       "       [ 10, 150],\n",
       "       [ 10, 160],\n",
       "       [ 10, 170],\n",
       "       [ 10, 180],\n",
       "       [ 10, 190],\n",
       "       [ 10, 200],\n",
       "       [ 20,  10],\n",
       "       [ 20,  20],\n",
       "       [ 20,  30],\n",
       "       [ 20,  40],\n",
       "       [ 20,  50],\n",
       "       [ 20,  60],\n",
       "       [ 20,  70],\n",
       "       [ 20,  80],\n",
       "       [ 20,  90],\n",
       "       [ 20, 100],\n",
       "       [ 20, 110],\n",
       "       [ 20, 120],\n",
       "       [ 20, 130],\n",
       "       [ 20, 140],\n",
       "       [ 20, 150],\n",
       "       [ 20, 160],\n",
       "       [ 20, 170],\n",
       "       [ 20, 180],\n",
       "       [ 20, 190],\n",
       "       [ 20, 200],\n",
       "       [ 30,  10],\n",
       "       [ 30,  20],\n",
       "       [ 30,  30],\n",
       "       [ 30,  40],\n",
       "       [ 30,  50],\n",
       "       [ 30,  60],\n",
       "       [ 30,  70],\n",
       "       [ 30,  80],\n",
       "       [ 30,  90],\n",
       "       [ 30, 100],\n",
       "       [ 30, 110],\n",
       "       [ 30, 120],\n",
       "       [ 30, 130],\n",
       "       [ 30, 140],\n",
       "       [ 30, 150],\n",
       "       [ 30, 160],\n",
       "       [ 30, 170],\n",
       "       [ 30, 180],\n",
       "       [ 30, 190],\n",
       "       [ 30, 200],\n",
       "       [ 40,  10],\n",
       "       [ 40,  20],\n",
       "       [ 40,  30],\n",
       "       [ 40,  40],\n",
       "       [ 40,  50],\n",
       "       [ 40,  60],\n",
       "       [ 40,  70],\n",
       "       [ 40,  80],\n",
       "       [ 40,  90],\n",
       "       [ 40, 100],\n",
       "       [ 40, 110],\n",
       "       [ 40, 120],\n",
       "       [ 40, 130],\n",
       "       [ 40, 140],\n",
       "       [ 40, 150],\n",
       "       [ 40, 160],\n",
       "       [ 40, 170],\n",
       "       [ 40, 180],\n",
       "       [ 40, 190],\n",
       "       [ 40, 200],\n",
       "       [ 50,  10],\n",
       "       [ 50,  20],\n",
       "       [ 50,  30],\n",
       "       [ 50,  40],\n",
       "       [ 50,  50],\n",
       "       [ 50,  60],\n",
       "       [ 50,  70],\n",
       "       [ 50,  80],\n",
       "       [ 50,  90],\n",
       "       [ 50, 100],\n",
       "       [ 50, 110],\n",
       "       [ 50, 120],\n",
       "       [ 50, 130],\n",
       "       [ 50, 140],\n",
       "       [ 50, 150],\n",
       "       [ 50, 160],\n",
       "       [ 50, 170],\n",
       "       [ 50, 180],\n",
       "       [ 50, 190],\n",
       "       [ 50, 200],\n",
       "       [ 60,  10],\n",
       "       [ 60,  20],\n",
       "       [ 60,  30],\n",
       "       [ 60,  40],\n",
       "       [ 60,  50],\n",
       "       [ 60,  60],\n",
       "       [ 60,  70],\n",
       "       [ 60,  80],\n",
       "       [ 60,  90],\n",
       "       [ 60, 100],\n",
       "       [ 60, 110],\n",
       "       [ 60, 120],\n",
       "       [ 60, 130],\n",
       "       [ 60, 140],\n",
       "       [ 60, 150],\n",
       "       [ 60, 160],\n",
       "       [ 60, 170],\n",
       "       [ 60, 180],\n",
       "       [ 60, 190],\n",
       "       [ 60, 200],\n",
       "       [ 70,  10],\n",
       "       [ 70,  20],\n",
       "       [ 70,  30],\n",
       "       [ 70,  40],\n",
       "       [ 70,  50],\n",
       "       [ 70,  60],\n",
       "       [ 70,  70],\n",
       "       [ 70,  80],\n",
       "       [ 70,  90],\n",
       "       [ 70, 100],\n",
       "       [ 70, 110],\n",
       "       [ 70, 120],\n",
       "       [ 70, 130],\n",
       "       [ 70, 140],\n",
       "       [ 70, 150],\n",
       "       [ 70, 160],\n",
       "       [ 70, 170],\n",
       "       [ 70, 180],\n",
       "       [ 70, 190],\n",
       "       [ 70, 200],\n",
       "       [ 80,  10],\n",
       "       [ 80,  20],\n",
       "       [ 80,  30],\n",
       "       [ 80,  40],\n",
       "       [ 80,  50],\n",
       "       [ 80,  60],\n",
       "       [ 80,  70],\n",
       "       [ 80,  80],\n",
       "       [ 80,  90],\n",
       "       [ 80, 100],\n",
       "       [ 80, 110],\n",
       "       [ 80, 120],\n",
       "       [ 80, 130],\n",
       "       [ 80, 140],\n",
       "       [ 80, 150],\n",
       "       [ 80, 160],\n",
       "       [ 80, 170],\n",
       "       [ 80, 180],\n",
       "       [ 80, 190],\n",
       "       [ 80, 200],\n",
       "       [ 90,  10],\n",
       "       [ 90,  20],\n",
       "       [ 90,  30],\n",
       "       [ 90,  40],\n",
       "       [ 90,  50],\n",
       "       [ 90,  60],\n",
       "       [ 90,  70],\n",
       "       [ 90,  80],\n",
       "       [ 90,  90],\n",
       "       [ 90, 100],\n",
       "       [ 90, 110],\n",
       "       [ 90, 120],\n",
       "       [ 90, 130],\n",
       "       [ 90, 140],\n",
       "       [ 90, 150],\n",
       "       [ 90, 160],\n",
       "       [ 90, 170],\n",
       "       [ 90, 180],\n",
       "       [ 90, 190],\n",
       "       [ 90, 200],\n",
       "       [100,  10],\n",
       "       [100,  20],\n",
       "       [100,  30],\n",
       "       [100,  40],\n",
       "       [100,  50],\n",
       "       [100,  60],\n",
       "       [100,  70],\n",
       "       [100,  80],\n",
       "       [100,  90],\n",
       "       [100, 100],\n",
       "       [100, 110],\n",
       "       [100, 120],\n",
       "       [100, 130],\n",
       "       [100, 140],\n",
       "       [100, 150],\n",
       "       [100, 160],\n",
       "       [100, 170],\n",
       "       [100, 180],\n",
       "       [100, 190],\n",
       "       [100, 200],\n",
       "       [110,  10],\n",
       "       [110,  20],\n",
       "       [110,  30],\n",
       "       [110,  40],\n",
       "       [110,  50],\n",
       "       [110,  60],\n",
       "       [110,  70],\n",
       "       [110,  80],\n",
       "       [110,  90],\n",
       "       [110, 100],\n",
       "       [110, 110],\n",
       "       [110, 120],\n",
       "       [110, 130],\n",
       "       [110, 140],\n",
       "       [110, 150],\n",
       "       [110, 160],\n",
       "       [110, 170],\n",
       "       [110, 180],\n",
       "       [110, 190],\n",
       "       [110, 200],\n",
       "       [120,  10],\n",
       "       [120,  20],\n",
       "       [120,  30],\n",
       "       [120,  40],\n",
       "       [120,  50],\n",
       "       [120,  60],\n",
       "       [120,  70],\n",
       "       [120,  80],\n",
       "       [120,  90],\n",
       "       [120, 100],\n",
       "       [120, 110],\n",
       "       [120, 120],\n",
       "       [120, 130],\n",
       "       [120, 140],\n",
       "       [120, 150],\n",
       "       [120, 160],\n",
       "       [120, 170],\n",
       "       [120, 180],\n",
       "       [120, 190],\n",
       "       [120, 200],\n",
       "       [130,  10],\n",
       "       [130,  20],\n",
       "       [130,  30],\n",
       "       [130,  40],\n",
       "       [130,  50],\n",
       "       [130,  60],\n",
       "       [130,  70],\n",
       "       [130,  80],\n",
       "       [130,  90],\n",
       "       [130, 100],\n",
       "       [130, 110],\n",
       "       [130, 120],\n",
       "       [130, 130],\n",
       "       [130, 140],\n",
       "       [130, 150],\n",
       "       [130, 160],\n",
       "       [130, 170],\n",
       "       [130, 180],\n",
       "       [130, 190],\n",
       "       [130, 200],\n",
       "       [140,  10],\n",
       "       [140,  20],\n",
       "       [140,  30],\n",
       "       [140,  40],\n",
       "       [140,  50],\n",
       "       [140,  60],\n",
       "       [140,  70],\n",
       "       [140,  80],\n",
       "       [140,  90],\n",
       "       [140, 100],\n",
       "       [140, 110],\n",
       "       [140, 120],\n",
       "       [140, 130],\n",
       "       [140, 140],\n",
       "       [140, 150],\n",
       "       [140, 160],\n",
       "       [140, 170],\n",
       "       [140, 180],\n",
       "       [140, 190],\n",
       "       [140, 200],\n",
       "       [150,  10],\n",
       "       [150,  20],\n",
       "       [150,  30],\n",
       "       [150,  40],\n",
       "       [150,  50],\n",
       "       [150,  60],\n",
       "       [150,  70],\n",
       "       [150,  80],\n",
       "       [150,  90],\n",
       "       [150, 100],\n",
       "       [150, 110],\n",
       "       [150, 120],\n",
       "       [150, 130],\n",
       "       [150, 140],\n",
       "       [150, 150],\n",
       "       [150, 160],\n",
       "       [150, 170],\n",
       "       [150, 180],\n",
       "       [150, 190],\n",
       "       [150, 200],\n",
       "       [160,  10],\n",
       "       [160,  20],\n",
       "       [160,  30],\n",
       "       [160,  40],\n",
       "       [160,  50],\n",
       "       [160,  60],\n",
       "       [160,  70],\n",
       "       [160,  80],\n",
       "       [160,  90],\n",
       "       [160, 100],\n",
       "       [160, 110],\n",
       "       [160, 120],\n",
       "       [160, 130],\n",
       "       [160, 140],\n",
       "       [160, 150],\n",
       "       [160, 160],\n",
       "       [160, 170],\n",
       "       [160, 180],\n",
       "       [160, 190],\n",
       "       [160, 200],\n",
       "       [170,  10],\n",
       "       [170,  20],\n",
       "       [170,  30],\n",
       "       [170,  40],\n",
       "       [170,  50],\n",
       "       [170,  60],\n",
       "       [170,  70],\n",
       "       [170,  80],\n",
       "       [170,  90],\n",
       "       [170, 100],\n",
       "       [170, 110],\n",
       "       [170, 120],\n",
       "       [170, 130],\n",
       "       [170, 140],\n",
       "       [170, 150],\n",
       "       [170, 160],\n",
       "       [170, 170],\n",
       "       [170, 180],\n",
       "       [170, 190],\n",
       "       [170, 200],\n",
       "       [180,  10],\n",
       "       [180,  20],\n",
       "       [180,  30],\n",
       "       [180,  40],\n",
       "       [180,  50],\n",
       "       [180,  60],\n",
       "       [180,  70],\n",
       "       [180,  80],\n",
       "       [180,  90],\n",
       "       [180, 100],\n",
       "       [180, 110],\n",
       "       [180, 120],\n",
       "       [180, 130],\n",
       "       [180, 140],\n",
       "       [180, 150],\n",
       "       [180, 160],\n",
       "       [180, 170],\n",
       "       [180, 180],\n",
       "       [180, 190],\n",
       "       [180, 200],\n",
       "       [190,  10],\n",
       "       [190,  20],\n",
       "       [190,  30],\n",
       "       [190,  40],\n",
       "       [190,  50],\n",
       "       [190,  60],\n",
       "       [190,  70],\n",
       "       [190,  80],\n",
       "       [190,  90],\n",
       "       [190, 100],\n",
       "       [190, 110],\n",
       "       [190, 120],\n",
       "       [190, 130],\n",
       "       [190, 140],\n",
       "       [190, 150],\n",
       "       [190, 160],\n",
       "       [190, 170],\n",
       "       [190, 180],\n",
       "       [190, 190],\n",
       "       [190, 200],\n",
       "       [200,  10],\n",
       "       [200,  20],\n",
       "       [200,  30],\n",
       "       [200,  40],\n",
       "       [200,  50],\n",
       "       [200,  60],\n",
       "       [200,  70],\n",
       "       [200,  80],\n",
       "       [200,  90],\n",
       "       [200, 100],\n",
       "       [200, 110],\n",
       "       [200, 120],\n",
       "       [200, 130],\n",
       "       [200, 140],\n",
       "       [200, 150],\n",
       "       [200, 160],\n",
       "       [200, 170],\n",
       "       [200, 180],\n",
       "       [200, 190],\n",
       "       [200, 200]], dtype=int64)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XpBS_values = XpBS.values\n",
    "XpBS_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "e7ba194c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.        , 0.05263158],\n",
       "       [0.        , 0.10526316],\n",
       "       [0.        , 0.15789474],\n",
       "       [0.        , 0.21052632],\n",
       "       [0.        , 0.26315789],\n",
       "       [0.        , 0.31578947],\n",
       "       [0.        , 0.36842105],\n",
       "       [0.        , 0.42105263],\n",
       "       [0.        , 0.47368421],\n",
       "       [0.        , 0.52631579],\n",
       "       [0.        , 0.57894737],\n",
       "       [0.        , 0.63157895],\n",
       "       [0.        , 0.68421053],\n",
       "       [0.        , 0.73684211],\n",
       "       [0.        , 0.78947368],\n",
       "       [0.        , 0.84210526],\n",
       "       [0.        , 0.89473684],\n",
       "       [0.        , 0.94736842],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05238345, 0.        ],\n",
       "       [0.05238345, 0.05263158],\n",
       "       [0.05238345, 0.10526316],\n",
       "       [0.05238345, 0.15789474],\n",
       "       [0.05238345, 0.21052632],\n",
       "       [0.05238345, 0.26315789],\n",
       "       [0.05238345, 0.31578947],\n",
       "       [0.05238345, 0.36842105],\n",
       "       [0.05238345, 0.42105263],\n",
       "       [0.05238345, 0.47368421],\n",
       "       [0.05238345, 0.52631579],\n",
       "       [0.05238345, 0.57894737],\n",
       "       [0.05238345, 0.63157895],\n",
       "       [0.05238345, 0.68421053],\n",
       "       [0.05238345, 0.73684211],\n",
       "       [0.05238345, 0.78947368],\n",
       "       [0.05238345, 0.84210526],\n",
       "       [0.05238345, 0.89473684],\n",
       "       [0.05238345, 0.94736842],\n",
       "       [0.05238345, 1.        ],\n",
       "       [0.10476689, 0.        ],\n",
       "       [0.10476689, 0.05263158],\n",
       "       [0.10476689, 0.10526316],\n",
       "       [0.10476689, 0.15789474],\n",
       "       [0.10476689, 0.21052632],\n",
       "       [0.10476689, 0.26315789],\n",
       "       [0.10476689, 0.31578947],\n",
       "       [0.10476689, 0.36842105],\n",
       "       [0.10476689, 0.42105263],\n",
       "       [0.10476689, 0.47368421],\n",
       "       [0.10476689, 0.52631579],\n",
       "       [0.10476689, 0.57894737],\n",
       "       [0.10476689, 0.63157895],\n",
       "       [0.10476689, 0.68421053],\n",
       "       [0.10476689, 0.73684211],\n",
       "       [0.10476689, 0.78947368],\n",
       "       [0.10476689, 0.84210526],\n",
       "       [0.10476689, 0.89473684],\n",
       "       [0.10476689, 0.94736842],\n",
       "       [0.10476689, 1.        ],\n",
       "       [0.15715034, 0.        ],\n",
       "       [0.15715034, 0.05263158],\n",
       "       [0.15715034, 0.10526316],\n",
       "       [0.15715034, 0.15789474],\n",
       "       [0.15715034, 0.21052632],\n",
       "       [0.15715034, 0.26315789],\n",
       "       [0.15715034, 0.31578947],\n",
       "       [0.15715034, 0.36842105],\n",
       "       [0.15715034, 0.42105263],\n",
       "       [0.15715034, 0.47368421],\n",
       "       [0.15715034, 0.52631579],\n",
       "       [0.15715034, 0.57894737],\n",
       "       [0.15715034, 0.63157895],\n",
       "       [0.15715034, 0.68421053],\n",
       "       [0.15715034, 0.73684211],\n",
       "       [0.15715034, 0.78947368],\n",
       "       [0.15715034, 0.84210526],\n",
       "       [0.15715034, 0.89473684],\n",
       "       [0.15715034, 0.94736842],\n",
       "       [0.15715034, 1.        ],\n",
       "       [0.20953379, 0.        ],\n",
       "       [0.20953379, 0.05263158],\n",
       "       [0.20953379, 0.10526316],\n",
       "       [0.20953379, 0.15789474],\n",
       "       [0.20953379, 0.21052632],\n",
       "       [0.20953379, 0.26315789],\n",
       "       [0.20953379, 0.31578947],\n",
       "       [0.20953379, 0.36842105],\n",
       "       [0.20953379, 0.42105263],\n",
       "       [0.20953379, 0.47368421],\n",
       "       [0.20953379, 0.52631579],\n",
       "       [0.20953379, 0.57894737],\n",
       "       [0.20953379, 0.63157895],\n",
       "       [0.20953379, 0.68421053],\n",
       "       [0.20953379, 0.73684211],\n",
       "       [0.20953379, 0.78947368],\n",
       "       [0.20953379, 0.84210526],\n",
       "       [0.20953379, 0.89473684],\n",
       "       [0.20953379, 0.94736842],\n",
       "       [0.20953379, 1.        ],\n",
       "       [0.26191723, 0.        ],\n",
       "       [0.26191723, 0.05263158],\n",
       "       [0.26191723, 0.10526316],\n",
       "       [0.26191723, 0.15789474],\n",
       "       [0.26191723, 0.21052632],\n",
       "       [0.26191723, 0.26315789],\n",
       "       [0.26191723, 0.31578947],\n",
       "       [0.26191723, 0.36842105],\n",
       "       [0.26191723, 0.42105263],\n",
       "       [0.26191723, 0.47368421],\n",
       "       [0.26191723, 0.52631579],\n",
       "       [0.26191723, 0.57894737],\n",
       "       [0.26191723, 0.63157895],\n",
       "       [0.26191723, 0.68421053],\n",
       "       [0.26191723, 0.73684211],\n",
       "       [0.26191723, 0.78947368],\n",
       "       [0.26191723, 0.84210526],\n",
       "       [0.26191723, 0.89473684],\n",
       "       [0.26191723, 0.94736842],\n",
       "       [0.26191723, 1.        ],\n",
       "       [0.31430068, 0.        ],\n",
       "       [0.31430068, 0.05263158],\n",
       "       [0.31430068, 0.10526316],\n",
       "       [0.31430068, 0.15789474],\n",
       "       [0.31430068, 0.21052632],\n",
       "       [0.31430068, 0.26315789],\n",
       "       [0.31430068, 0.31578947],\n",
       "       [0.31430068, 0.36842105],\n",
       "       [0.31430068, 0.42105263],\n",
       "       [0.31430068, 0.47368421],\n",
       "       [0.31430068, 0.52631579],\n",
       "       [0.31430068, 0.57894737],\n",
       "       [0.31430068, 0.63157895],\n",
       "       [0.31430068, 0.68421053],\n",
       "       [0.31430068, 0.73684211],\n",
       "       [0.31430068, 0.78947368],\n",
       "       [0.31430068, 0.84210526],\n",
       "       [0.31430068, 0.89473684],\n",
       "       [0.31430068, 0.94736842],\n",
       "       [0.31430068, 1.        ],\n",
       "       [0.36668413, 0.        ],\n",
       "       [0.36668413, 0.05263158],\n",
       "       [0.36668413, 0.10526316],\n",
       "       [0.36668413, 0.15789474],\n",
       "       [0.36668413, 0.21052632],\n",
       "       [0.36668413, 0.26315789],\n",
       "       [0.36668413, 0.31578947],\n",
       "       [0.36668413, 0.36842105],\n",
       "       [0.36668413, 0.42105263],\n",
       "       [0.36668413, 0.47368421],\n",
       "       [0.36668413, 0.52631579],\n",
       "       [0.36668413, 0.57894737],\n",
       "       [0.36668413, 0.63157895],\n",
       "       [0.36668413, 0.68421053],\n",
       "       [0.36668413, 0.73684211],\n",
       "       [0.36668413, 0.78947368],\n",
       "       [0.36668413, 0.84210526],\n",
       "       [0.36668413, 0.89473684],\n",
       "       [0.36668413, 0.94736842],\n",
       "       [0.36668413, 1.        ],\n",
       "       [0.41906757, 0.        ],\n",
       "       [0.41906757, 0.05263158],\n",
       "       [0.41906757, 0.10526316],\n",
       "       [0.41906757, 0.15789474],\n",
       "       [0.41906757, 0.21052632],\n",
       "       [0.41906757, 0.26315789],\n",
       "       [0.41906757, 0.31578947],\n",
       "       [0.41906757, 0.36842105],\n",
       "       [0.41906757, 0.42105263],\n",
       "       [0.41906757, 0.47368421],\n",
       "       [0.41906757, 0.52631579],\n",
       "       [0.41906757, 0.57894737],\n",
       "       [0.41906757, 0.63157895],\n",
       "       [0.41906757, 0.68421053],\n",
       "       [0.41906757, 0.73684211],\n",
       "       [0.41906757, 0.78947368],\n",
       "       [0.41906757, 0.84210526],\n",
       "       [0.41906757, 0.89473684],\n",
       "       [0.41906757, 0.94736842],\n",
       "       [0.41906757, 1.        ],\n",
       "       [0.47145102, 0.        ],\n",
       "       [0.47145102, 0.05263158],\n",
       "       [0.47145102, 0.10526316],\n",
       "       [0.47145102, 0.15789474],\n",
       "       [0.47145102, 0.21052632],\n",
       "       [0.47145102, 0.26315789],\n",
       "       [0.47145102, 0.31578947],\n",
       "       [0.47145102, 0.36842105],\n",
       "       [0.47145102, 0.42105263],\n",
       "       [0.47145102, 0.47368421],\n",
       "       [0.47145102, 0.52631579],\n",
       "       [0.47145102, 0.57894737],\n",
       "       [0.47145102, 0.63157895],\n",
       "       [0.47145102, 0.68421053],\n",
       "       [0.47145102, 0.73684211],\n",
       "       [0.47145102, 0.78947368],\n",
       "       [0.47145102, 0.84210526],\n",
       "       [0.47145102, 0.89473684],\n",
       "       [0.47145102, 0.94736842],\n",
       "       [0.47145102, 1.        ],\n",
       "       [0.52383447, 0.        ],\n",
       "       [0.52383447, 0.05263158],\n",
       "       [0.52383447, 0.10526316],\n",
       "       [0.52383447, 0.15789474],\n",
       "       [0.52383447, 0.21052632],\n",
       "       [0.52383447, 0.26315789],\n",
       "       [0.52383447, 0.31578947],\n",
       "       [0.52383447, 0.36842105],\n",
       "       [0.52383447, 0.42105263],\n",
       "       [0.52383447, 0.47368421],\n",
       "       [0.52383447, 0.52631579],\n",
       "       [0.52383447, 0.57894737],\n",
       "       [0.52383447, 0.63157895],\n",
       "       [0.52383447, 0.68421053],\n",
       "       [0.52383447, 0.73684211],\n",
       "       [0.52383447, 0.78947368],\n",
       "       [0.52383447, 0.84210526],\n",
       "       [0.52383447, 0.89473684],\n",
       "       [0.52383447, 0.94736842],\n",
       "       [0.52383447, 1.        ],\n",
       "       [0.57621792, 0.        ],\n",
       "       [0.57621792, 0.05263158],\n",
       "       [0.57621792, 0.10526316],\n",
       "       [0.57621792, 0.15789474],\n",
       "       [0.57621792, 0.21052632],\n",
       "       [0.57621792, 0.26315789],\n",
       "       [0.57621792, 0.31578947],\n",
       "       [0.57621792, 0.36842105],\n",
       "       [0.57621792, 0.42105263],\n",
       "       [0.57621792, 0.47368421],\n",
       "       [0.57621792, 0.52631579],\n",
       "       [0.57621792, 0.57894737],\n",
       "       [0.57621792, 0.63157895],\n",
       "       [0.57621792, 0.68421053],\n",
       "       [0.57621792, 0.73684211],\n",
       "       [0.57621792, 0.78947368],\n",
       "       [0.57621792, 0.84210526],\n",
       "       [0.57621792, 0.89473684],\n",
       "       [0.57621792, 0.94736842],\n",
       "       [0.57621792, 1.        ],\n",
       "       [0.62860136, 0.        ],\n",
       "       [0.62860136, 0.05263158],\n",
       "       [0.62860136, 0.10526316],\n",
       "       [0.62860136, 0.15789474],\n",
       "       [0.62860136, 0.21052632],\n",
       "       [0.62860136, 0.26315789],\n",
       "       [0.62860136, 0.31578947],\n",
       "       [0.62860136, 0.36842105],\n",
       "       [0.62860136, 0.42105263],\n",
       "       [0.62860136, 0.47368421],\n",
       "       [0.62860136, 0.52631579],\n",
       "       [0.62860136, 0.57894737],\n",
       "       [0.62860136, 0.63157895],\n",
       "       [0.62860136, 0.68421053],\n",
       "       [0.62860136, 0.73684211],\n",
       "       [0.62860136, 0.78947368],\n",
       "       [0.62860136, 0.84210526],\n",
       "       [0.62860136, 0.89473684],\n",
       "       [0.62860136, 0.94736842],\n",
       "       [0.62860136, 1.        ],\n",
       "       [0.68098481, 0.        ],\n",
       "       [0.68098481, 0.05263158],\n",
       "       [0.68098481, 0.10526316],\n",
       "       [0.68098481, 0.15789474],\n",
       "       [0.68098481, 0.21052632],\n",
       "       [0.68098481, 0.26315789],\n",
       "       [0.68098481, 0.31578947],\n",
       "       [0.68098481, 0.36842105],\n",
       "       [0.68098481, 0.42105263],\n",
       "       [0.68098481, 0.47368421],\n",
       "       [0.68098481, 0.52631579],\n",
       "       [0.68098481, 0.57894737],\n",
       "       [0.68098481, 0.63157895],\n",
       "       [0.68098481, 0.68421053],\n",
       "       [0.68098481, 0.73684211],\n",
       "       [0.68098481, 0.78947368],\n",
       "       [0.68098481, 0.84210526],\n",
       "       [0.68098481, 0.89473684],\n",
       "       [0.68098481, 0.94736842],\n",
       "       [0.68098481, 1.        ],\n",
       "       [0.73336826, 0.        ],\n",
       "       [0.73336826, 0.05263158],\n",
       "       [0.73336826, 0.10526316],\n",
       "       [0.73336826, 0.15789474],\n",
       "       [0.73336826, 0.21052632],\n",
       "       [0.73336826, 0.26315789],\n",
       "       [0.73336826, 0.31578947],\n",
       "       [0.73336826, 0.36842105],\n",
       "       [0.73336826, 0.42105263],\n",
       "       [0.73336826, 0.47368421],\n",
       "       [0.73336826, 0.52631579],\n",
       "       [0.73336826, 0.57894737],\n",
       "       [0.73336826, 0.63157895],\n",
       "       [0.73336826, 0.68421053],\n",
       "       [0.73336826, 0.73684211],\n",
       "       [0.73336826, 0.78947368],\n",
       "       [0.73336826, 0.84210526],\n",
       "       [0.73336826, 0.89473684],\n",
       "       [0.73336826, 0.94736842],\n",
       "       [0.73336826, 1.        ],\n",
       "       [0.7857517 , 0.        ],\n",
       "       [0.7857517 , 0.05263158],\n",
       "       [0.7857517 , 0.10526316],\n",
       "       [0.7857517 , 0.15789474],\n",
       "       [0.7857517 , 0.21052632],\n",
       "       [0.7857517 , 0.26315789],\n",
       "       [0.7857517 , 0.31578947],\n",
       "       [0.7857517 , 0.36842105],\n",
       "       [0.7857517 , 0.42105263],\n",
       "       [0.7857517 , 0.47368421],\n",
       "       [0.7857517 , 0.52631579],\n",
       "       [0.7857517 , 0.57894737],\n",
       "       [0.7857517 , 0.63157895],\n",
       "       [0.7857517 , 0.68421053],\n",
       "       [0.7857517 , 0.73684211],\n",
       "       [0.7857517 , 0.78947368],\n",
       "       [0.7857517 , 0.84210526],\n",
       "       [0.7857517 , 0.89473684],\n",
       "       [0.7857517 , 0.94736842],\n",
       "       [0.7857517 , 1.        ],\n",
       "       [0.83813515, 0.        ],\n",
       "       [0.83813515, 0.05263158],\n",
       "       [0.83813515, 0.10526316],\n",
       "       [0.83813515, 0.15789474],\n",
       "       [0.83813515, 0.21052632],\n",
       "       [0.83813515, 0.26315789],\n",
       "       [0.83813515, 0.31578947],\n",
       "       [0.83813515, 0.36842105],\n",
       "       [0.83813515, 0.42105263],\n",
       "       [0.83813515, 0.47368421],\n",
       "       [0.83813515, 0.52631579],\n",
       "       [0.83813515, 0.57894737],\n",
       "       [0.83813515, 0.63157895],\n",
       "       [0.83813515, 0.68421053],\n",
       "       [0.83813515, 0.73684211],\n",
       "       [0.83813515, 0.78947368],\n",
       "       [0.83813515, 0.84210526],\n",
       "       [0.83813515, 0.89473684],\n",
       "       [0.83813515, 0.94736842],\n",
       "       [0.83813515, 1.        ],\n",
       "       [0.8905186 , 0.        ],\n",
       "       [0.8905186 , 0.05263158],\n",
       "       [0.8905186 , 0.10526316],\n",
       "       [0.8905186 , 0.15789474],\n",
       "       [0.8905186 , 0.21052632],\n",
       "       [0.8905186 , 0.26315789],\n",
       "       [0.8905186 , 0.31578947],\n",
       "       [0.8905186 , 0.36842105],\n",
       "       [0.8905186 , 0.42105263],\n",
       "       [0.8905186 , 0.47368421],\n",
       "       [0.8905186 , 0.52631579],\n",
       "       [0.8905186 , 0.57894737],\n",
       "       [0.8905186 , 0.63157895],\n",
       "       [0.8905186 , 0.68421053],\n",
       "       [0.8905186 , 0.73684211],\n",
       "       [0.8905186 , 0.78947368],\n",
       "       [0.8905186 , 0.84210526],\n",
       "       [0.8905186 , 0.89473684],\n",
       "       [0.8905186 , 0.94736842],\n",
       "       [0.8905186 , 1.        ],\n",
       "       [0.94290204, 0.        ],\n",
       "       [0.94290204, 0.05263158],\n",
       "       [0.94290204, 0.10526316],\n",
       "       [0.94290204, 0.15789474],\n",
       "       [0.94290204, 0.21052632],\n",
       "       [0.94290204, 0.26315789],\n",
       "       [0.94290204, 0.31578947],\n",
       "       [0.94290204, 0.36842105],\n",
       "       [0.94290204, 0.42105263],\n",
       "       [0.94290204, 0.47368421],\n",
       "       [0.94290204, 0.52631579],\n",
       "       [0.94290204, 0.57894737],\n",
       "       [0.94290204, 0.63157895],\n",
       "       [0.94290204, 0.68421053],\n",
       "       [0.94290204, 0.73684211],\n",
       "       [0.94290204, 0.78947368],\n",
       "       [0.94290204, 0.84210526],\n",
       "       [0.94290204, 0.89473684],\n",
       "       [0.94290204, 0.94736842],\n",
       "       [0.94290204, 1.        ],\n",
       "       [0.99528549, 0.        ],\n",
       "       [0.99528549, 0.05263158],\n",
       "       [0.99528549, 0.10526316],\n",
       "       [0.99528549, 0.15789474],\n",
       "       [0.99528549, 0.21052632],\n",
       "       [0.99528549, 0.26315789],\n",
       "       [0.99528549, 0.31578947],\n",
       "       [0.99528549, 0.36842105],\n",
       "       [0.99528549, 0.42105263],\n",
       "       [0.99528549, 0.47368421],\n",
       "       [0.99528549, 0.52631579],\n",
       "       [0.99528549, 0.57894737],\n",
       "       [0.99528549, 0.63157895],\n",
       "       [0.99528549, 0.68421053],\n",
       "       [0.99528549, 0.73684211],\n",
       "       [0.99528549, 0.78947368],\n",
       "       [0.99528549, 0.84210526],\n",
       "       [0.99528549, 0.89473684],\n",
       "       [0.99528549, 0.94736842],\n",
       "       [0.99528549, 1.        ]])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization\n",
    "XpBS_minmax = min_max_scaler1.transform(XpBS_values)\n",
    "XpBS_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "938a9638",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "XpBS_tensor = torch.from_numpy(XpBS_minmax).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b84bc8e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    latency_pred = model(XpBS_tensor)\n",
    "y = pd.concat([pd.Series(XpBS_values[:, 0].reshape(-1), name='send rates'), pd.Series(XpBS_values[:, 1].reshape(-1), name='block size'), \n",
    "               pd.Series(latency_pred.numpy().reshape(-1), name='latency_pred')], axis=1)\n",
    "y.index = XpBS.index\n",
    "y.to_csv('../../Data/Result/Related3/latency_pred_related3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "b1104954",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>block size</th>\n",
       "      <th>latency_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.263789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.761386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>1.080978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>1.097761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>1.152921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>2.706232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>200</td>\n",
       "      <td>170</td>\n",
       "      <td>2.351660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>200</td>\n",
       "      <td>180</td>\n",
       "      <td>2.354682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>200</td>\n",
       "      <td>190</td>\n",
       "      <td>2.437658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>2.620018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     send rates  block size  latency_pred\n",
       "0            10          10      0.263789\n",
       "1            10          20      0.761386\n",
       "2            10          30      1.080978\n",
       "3            10          40      1.097761\n",
       "4            10          50      1.152921\n",
       "..          ...         ...           ...\n",
       "395         200         160      2.706232\n",
       "396         200         170      2.351660\n",
       "397         200         180      2.354682\n",
       "398         200         190      2.437658\n",
       "399         200         200      2.620018\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344286e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MLP2 to predict throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "dc743354",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.        , 0.05263158],\n",
       "       [0.        , 0.10526316],\n",
       "       [0.        , 0.15789474],\n",
       "       [0.        , 0.21052632],\n",
       "       [0.        , 0.26315789],\n",
       "       [0.        , 0.31578947],\n",
       "       [0.        , 0.36842105],\n",
       "       [0.        , 0.42105263],\n",
       "       [0.        , 0.47368421],\n",
       "       [0.        , 0.52631579],\n",
       "       [0.        , 0.57894737],\n",
       "       [0.        , 0.63157895],\n",
       "       [0.        , 0.68421053],\n",
       "       [0.        , 0.73684211],\n",
       "       [0.        , 0.78947368],\n",
       "       [0.        , 0.84210526],\n",
       "       [0.        , 0.89473684],\n",
       "       [0.        , 0.94736842],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05238345, 0.        ],\n",
       "       [0.05238345, 0.05263158],\n",
       "       [0.05238345, 0.10526316],\n",
       "       [0.05238345, 0.15789474],\n",
       "       [0.05238345, 0.21052632],\n",
       "       [0.05238345, 0.26315789],\n",
       "       [0.05238345, 0.31578947],\n",
       "       [0.05238345, 0.36842105],\n",
       "       [0.05238345, 0.42105263],\n",
       "       [0.05238345, 0.47368421],\n",
       "       [0.05238345, 0.52631579],\n",
       "       [0.05238345, 0.57894737],\n",
       "       [0.05238345, 0.63157895],\n",
       "       [0.05238345, 0.68421053],\n",
       "       [0.05238345, 0.73684211],\n",
       "       [0.05238345, 0.78947368],\n",
       "       [0.05238345, 0.84210526],\n",
       "       [0.05238345, 0.89473684],\n",
       "       [0.05238345, 0.94736842],\n",
       "       [0.05238345, 1.        ],\n",
       "       [0.10476689, 0.        ],\n",
       "       [0.10476689, 0.05263158],\n",
       "       [0.10476689, 0.10526316],\n",
       "       [0.10476689, 0.15789474],\n",
       "       [0.10476689, 0.21052632],\n",
       "       [0.10476689, 0.26315789],\n",
       "       [0.10476689, 0.31578947],\n",
       "       [0.10476689, 0.36842105],\n",
       "       [0.10476689, 0.42105263],\n",
       "       [0.10476689, 0.47368421],\n",
       "       [0.10476689, 0.52631579],\n",
       "       [0.10476689, 0.57894737],\n",
       "       [0.10476689, 0.63157895],\n",
       "       [0.10476689, 0.68421053],\n",
       "       [0.10476689, 0.73684211],\n",
       "       [0.10476689, 0.78947368],\n",
       "       [0.10476689, 0.84210526],\n",
       "       [0.10476689, 0.89473684],\n",
       "       [0.10476689, 0.94736842],\n",
       "       [0.10476689, 1.        ],\n",
       "       [0.15715034, 0.        ],\n",
       "       [0.15715034, 0.05263158],\n",
       "       [0.15715034, 0.10526316],\n",
       "       [0.15715034, 0.15789474],\n",
       "       [0.15715034, 0.21052632],\n",
       "       [0.15715034, 0.26315789],\n",
       "       [0.15715034, 0.31578947],\n",
       "       [0.15715034, 0.36842105],\n",
       "       [0.15715034, 0.42105263],\n",
       "       [0.15715034, 0.47368421],\n",
       "       [0.15715034, 0.52631579],\n",
       "       [0.15715034, 0.57894737],\n",
       "       [0.15715034, 0.63157895],\n",
       "       [0.15715034, 0.68421053],\n",
       "       [0.15715034, 0.73684211],\n",
       "       [0.15715034, 0.78947368],\n",
       "       [0.15715034, 0.84210526],\n",
       "       [0.15715034, 0.89473684],\n",
       "       [0.15715034, 0.94736842],\n",
       "       [0.15715034, 1.        ],\n",
       "       [0.20953379, 0.        ],\n",
       "       [0.20953379, 0.05263158],\n",
       "       [0.20953379, 0.10526316],\n",
       "       [0.20953379, 0.15789474],\n",
       "       [0.20953379, 0.21052632],\n",
       "       [0.20953379, 0.26315789],\n",
       "       [0.20953379, 0.31578947],\n",
       "       [0.20953379, 0.36842105],\n",
       "       [0.20953379, 0.42105263],\n",
       "       [0.20953379, 0.47368421],\n",
       "       [0.20953379, 0.52631579],\n",
       "       [0.20953379, 0.57894737],\n",
       "       [0.20953379, 0.63157895],\n",
       "       [0.20953379, 0.68421053],\n",
       "       [0.20953379, 0.73684211],\n",
       "       [0.20953379, 0.78947368],\n",
       "       [0.20953379, 0.84210526],\n",
       "       [0.20953379, 0.89473684],\n",
       "       [0.20953379, 0.94736842],\n",
       "       [0.20953379, 1.        ],\n",
       "       [0.26191723, 0.        ],\n",
       "       [0.26191723, 0.05263158],\n",
       "       [0.26191723, 0.10526316],\n",
       "       [0.26191723, 0.15789474],\n",
       "       [0.26191723, 0.21052632],\n",
       "       [0.26191723, 0.26315789],\n",
       "       [0.26191723, 0.31578947],\n",
       "       [0.26191723, 0.36842105],\n",
       "       [0.26191723, 0.42105263],\n",
       "       [0.26191723, 0.47368421],\n",
       "       [0.26191723, 0.52631579],\n",
       "       [0.26191723, 0.57894737],\n",
       "       [0.26191723, 0.63157895],\n",
       "       [0.26191723, 0.68421053],\n",
       "       [0.26191723, 0.73684211],\n",
       "       [0.26191723, 0.78947368],\n",
       "       [0.26191723, 0.84210526],\n",
       "       [0.26191723, 0.89473684],\n",
       "       [0.26191723, 0.94736842],\n",
       "       [0.26191723, 1.        ],\n",
       "       [0.31430068, 0.        ],\n",
       "       [0.31430068, 0.05263158],\n",
       "       [0.31430068, 0.10526316],\n",
       "       [0.31430068, 0.15789474],\n",
       "       [0.31430068, 0.21052632],\n",
       "       [0.31430068, 0.26315789],\n",
       "       [0.31430068, 0.31578947],\n",
       "       [0.31430068, 0.36842105],\n",
       "       [0.31430068, 0.42105263],\n",
       "       [0.31430068, 0.47368421],\n",
       "       [0.31430068, 0.52631579],\n",
       "       [0.31430068, 0.57894737],\n",
       "       [0.31430068, 0.63157895],\n",
       "       [0.31430068, 0.68421053],\n",
       "       [0.31430068, 0.73684211],\n",
       "       [0.31430068, 0.78947368],\n",
       "       [0.31430068, 0.84210526],\n",
       "       [0.31430068, 0.89473684],\n",
       "       [0.31430068, 0.94736842],\n",
       "       [0.31430068, 1.        ],\n",
       "       [0.36668413, 0.        ],\n",
       "       [0.36668413, 0.05263158],\n",
       "       [0.36668413, 0.10526316],\n",
       "       [0.36668413, 0.15789474],\n",
       "       [0.36668413, 0.21052632],\n",
       "       [0.36668413, 0.26315789],\n",
       "       [0.36668413, 0.31578947],\n",
       "       [0.36668413, 0.36842105],\n",
       "       [0.36668413, 0.42105263],\n",
       "       [0.36668413, 0.47368421],\n",
       "       [0.36668413, 0.52631579],\n",
       "       [0.36668413, 0.57894737],\n",
       "       [0.36668413, 0.63157895],\n",
       "       [0.36668413, 0.68421053],\n",
       "       [0.36668413, 0.73684211],\n",
       "       [0.36668413, 0.78947368],\n",
       "       [0.36668413, 0.84210526],\n",
       "       [0.36668413, 0.89473684],\n",
       "       [0.36668413, 0.94736842],\n",
       "       [0.36668413, 1.        ],\n",
       "       [0.41906757, 0.        ],\n",
       "       [0.41906757, 0.05263158],\n",
       "       [0.41906757, 0.10526316],\n",
       "       [0.41906757, 0.15789474],\n",
       "       [0.41906757, 0.21052632],\n",
       "       [0.41906757, 0.26315789],\n",
       "       [0.41906757, 0.31578947],\n",
       "       [0.41906757, 0.36842105],\n",
       "       [0.41906757, 0.42105263],\n",
       "       [0.41906757, 0.47368421],\n",
       "       [0.41906757, 0.52631579],\n",
       "       [0.41906757, 0.57894737],\n",
       "       [0.41906757, 0.63157895],\n",
       "       [0.41906757, 0.68421053],\n",
       "       [0.41906757, 0.73684211],\n",
       "       [0.41906757, 0.78947368],\n",
       "       [0.41906757, 0.84210526],\n",
       "       [0.41906757, 0.89473684],\n",
       "       [0.41906757, 0.94736842],\n",
       "       [0.41906757, 1.        ],\n",
       "       [0.47145102, 0.        ],\n",
       "       [0.47145102, 0.05263158],\n",
       "       [0.47145102, 0.10526316],\n",
       "       [0.47145102, 0.15789474],\n",
       "       [0.47145102, 0.21052632],\n",
       "       [0.47145102, 0.26315789],\n",
       "       [0.47145102, 0.31578947],\n",
       "       [0.47145102, 0.36842105],\n",
       "       [0.47145102, 0.42105263],\n",
       "       [0.47145102, 0.47368421],\n",
       "       [0.47145102, 0.52631579],\n",
       "       [0.47145102, 0.57894737],\n",
       "       [0.47145102, 0.63157895],\n",
       "       [0.47145102, 0.68421053],\n",
       "       [0.47145102, 0.73684211],\n",
       "       [0.47145102, 0.78947368],\n",
       "       [0.47145102, 0.84210526],\n",
       "       [0.47145102, 0.89473684],\n",
       "       [0.47145102, 0.94736842],\n",
       "       [0.47145102, 1.        ],\n",
       "       [0.52383447, 0.        ],\n",
       "       [0.52383447, 0.05263158],\n",
       "       [0.52383447, 0.10526316],\n",
       "       [0.52383447, 0.15789474],\n",
       "       [0.52383447, 0.21052632],\n",
       "       [0.52383447, 0.26315789],\n",
       "       [0.52383447, 0.31578947],\n",
       "       [0.52383447, 0.36842105],\n",
       "       [0.52383447, 0.42105263],\n",
       "       [0.52383447, 0.47368421],\n",
       "       [0.52383447, 0.52631579],\n",
       "       [0.52383447, 0.57894737],\n",
       "       [0.52383447, 0.63157895],\n",
       "       [0.52383447, 0.68421053],\n",
       "       [0.52383447, 0.73684211],\n",
       "       [0.52383447, 0.78947368],\n",
       "       [0.52383447, 0.84210526],\n",
       "       [0.52383447, 0.89473684],\n",
       "       [0.52383447, 0.94736842],\n",
       "       [0.52383447, 1.        ],\n",
       "       [0.57621792, 0.        ],\n",
       "       [0.57621792, 0.05263158],\n",
       "       [0.57621792, 0.10526316],\n",
       "       [0.57621792, 0.15789474],\n",
       "       [0.57621792, 0.21052632],\n",
       "       [0.57621792, 0.26315789],\n",
       "       [0.57621792, 0.31578947],\n",
       "       [0.57621792, 0.36842105],\n",
       "       [0.57621792, 0.42105263],\n",
       "       [0.57621792, 0.47368421],\n",
       "       [0.57621792, 0.52631579],\n",
       "       [0.57621792, 0.57894737],\n",
       "       [0.57621792, 0.63157895],\n",
       "       [0.57621792, 0.68421053],\n",
       "       [0.57621792, 0.73684211],\n",
       "       [0.57621792, 0.78947368],\n",
       "       [0.57621792, 0.84210526],\n",
       "       [0.57621792, 0.89473684],\n",
       "       [0.57621792, 0.94736842],\n",
       "       [0.57621792, 1.        ],\n",
       "       [0.62860136, 0.        ],\n",
       "       [0.62860136, 0.05263158],\n",
       "       [0.62860136, 0.10526316],\n",
       "       [0.62860136, 0.15789474],\n",
       "       [0.62860136, 0.21052632],\n",
       "       [0.62860136, 0.26315789],\n",
       "       [0.62860136, 0.31578947],\n",
       "       [0.62860136, 0.36842105],\n",
       "       [0.62860136, 0.42105263],\n",
       "       [0.62860136, 0.47368421],\n",
       "       [0.62860136, 0.52631579],\n",
       "       [0.62860136, 0.57894737],\n",
       "       [0.62860136, 0.63157895],\n",
       "       [0.62860136, 0.68421053],\n",
       "       [0.62860136, 0.73684211],\n",
       "       [0.62860136, 0.78947368],\n",
       "       [0.62860136, 0.84210526],\n",
       "       [0.62860136, 0.89473684],\n",
       "       [0.62860136, 0.94736842],\n",
       "       [0.62860136, 1.        ],\n",
       "       [0.68098481, 0.        ],\n",
       "       [0.68098481, 0.05263158],\n",
       "       [0.68098481, 0.10526316],\n",
       "       [0.68098481, 0.15789474],\n",
       "       [0.68098481, 0.21052632],\n",
       "       [0.68098481, 0.26315789],\n",
       "       [0.68098481, 0.31578947],\n",
       "       [0.68098481, 0.36842105],\n",
       "       [0.68098481, 0.42105263],\n",
       "       [0.68098481, 0.47368421],\n",
       "       [0.68098481, 0.52631579],\n",
       "       [0.68098481, 0.57894737],\n",
       "       [0.68098481, 0.63157895],\n",
       "       [0.68098481, 0.68421053],\n",
       "       [0.68098481, 0.73684211],\n",
       "       [0.68098481, 0.78947368],\n",
       "       [0.68098481, 0.84210526],\n",
       "       [0.68098481, 0.89473684],\n",
       "       [0.68098481, 0.94736842],\n",
       "       [0.68098481, 1.        ],\n",
       "       [0.73336826, 0.        ],\n",
       "       [0.73336826, 0.05263158],\n",
       "       [0.73336826, 0.10526316],\n",
       "       [0.73336826, 0.15789474],\n",
       "       [0.73336826, 0.21052632],\n",
       "       [0.73336826, 0.26315789],\n",
       "       [0.73336826, 0.31578947],\n",
       "       [0.73336826, 0.36842105],\n",
       "       [0.73336826, 0.42105263],\n",
       "       [0.73336826, 0.47368421],\n",
       "       [0.73336826, 0.52631579],\n",
       "       [0.73336826, 0.57894737],\n",
       "       [0.73336826, 0.63157895],\n",
       "       [0.73336826, 0.68421053],\n",
       "       [0.73336826, 0.73684211],\n",
       "       [0.73336826, 0.78947368],\n",
       "       [0.73336826, 0.84210526],\n",
       "       [0.73336826, 0.89473684],\n",
       "       [0.73336826, 0.94736842],\n",
       "       [0.73336826, 1.        ],\n",
       "       [0.7857517 , 0.        ],\n",
       "       [0.7857517 , 0.05263158],\n",
       "       [0.7857517 , 0.10526316],\n",
       "       [0.7857517 , 0.15789474],\n",
       "       [0.7857517 , 0.21052632],\n",
       "       [0.7857517 , 0.26315789],\n",
       "       [0.7857517 , 0.31578947],\n",
       "       [0.7857517 , 0.36842105],\n",
       "       [0.7857517 , 0.42105263],\n",
       "       [0.7857517 , 0.47368421],\n",
       "       [0.7857517 , 0.52631579],\n",
       "       [0.7857517 , 0.57894737],\n",
       "       [0.7857517 , 0.63157895],\n",
       "       [0.7857517 , 0.68421053],\n",
       "       [0.7857517 , 0.73684211],\n",
       "       [0.7857517 , 0.78947368],\n",
       "       [0.7857517 , 0.84210526],\n",
       "       [0.7857517 , 0.89473684],\n",
       "       [0.7857517 , 0.94736842],\n",
       "       [0.7857517 , 1.        ],\n",
       "       [0.83813515, 0.        ],\n",
       "       [0.83813515, 0.05263158],\n",
       "       [0.83813515, 0.10526316],\n",
       "       [0.83813515, 0.15789474],\n",
       "       [0.83813515, 0.21052632],\n",
       "       [0.83813515, 0.26315789],\n",
       "       [0.83813515, 0.31578947],\n",
       "       [0.83813515, 0.36842105],\n",
       "       [0.83813515, 0.42105263],\n",
       "       [0.83813515, 0.47368421],\n",
       "       [0.83813515, 0.52631579],\n",
       "       [0.83813515, 0.57894737],\n",
       "       [0.83813515, 0.63157895],\n",
       "       [0.83813515, 0.68421053],\n",
       "       [0.83813515, 0.73684211],\n",
       "       [0.83813515, 0.78947368],\n",
       "       [0.83813515, 0.84210526],\n",
       "       [0.83813515, 0.89473684],\n",
       "       [0.83813515, 0.94736842],\n",
       "       [0.83813515, 1.        ],\n",
       "       [0.8905186 , 0.        ],\n",
       "       [0.8905186 , 0.05263158],\n",
       "       [0.8905186 , 0.10526316],\n",
       "       [0.8905186 , 0.15789474],\n",
       "       [0.8905186 , 0.21052632],\n",
       "       [0.8905186 , 0.26315789],\n",
       "       [0.8905186 , 0.31578947],\n",
       "       [0.8905186 , 0.36842105],\n",
       "       [0.8905186 , 0.42105263],\n",
       "       [0.8905186 , 0.47368421],\n",
       "       [0.8905186 , 0.52631579],\n",
       "       [0.8905186 , 0.57894737],\n",
       "       [0.8905186 , 0.63157895],\n",
       "       [0.8905186 , 0.68421053],\n",
       "       [0.8905186 , 0.73684211],\n",
       "       [0.8905186 , 0.78947368],\n",
       "       [0.8905186 , 0.84210526],\n",
       "       [0.8905186 , 0.89473684],\n",
       "       [0.8905186 , 0.94736842],\n",
       "       [0.8905186 , 1.        ],\n",
       "       [0.94290204, 0.        ],\n",
       "       [0.94290204, 0.05263158],\n",
       "       [0.94290204, 0.10526316],\n",
       "       [0.94290204, 0.15789474],\n",
       "       [0.94290204, 0.21052632],\n",
       "       [0.94290204, 0.26315789],\n",
       "       [0.94290204, 0.31578947],\n",
       "       [0.94290204, 0.36842105],\n",
       "       [0.94290204, 0.42105263],\n",
       "       [0.94290204, 0.47368421],\n",
       "       [0.94290204, 0.52631579],\n",
       "       [0.94290204, 0.57894737],\n",
       "       [0.94290204, 0.63157895],\n",
       "       [0.94290204, 0.68421053],\n",
       "       [0.94290204, 0.73684211],\n",
       "       [0.94290204, 0.78947368],\n",
       "       [0.94290204, 0.84210526],\n",
       "       [0.94290204, 0.89473684],\n",
       "       [0.94290204, 0.94736842],\n",
       "       [0.94290204, 1.        ],\n",
       "       [0.99528549, 0.        ],\n",
       "       [0.99528549, 0.05263158],\n",
       "       [0.99528549, 0.10526316],\n",
       "       [0.99528549, 0.15789474],\n",
       "       [0.99528549, 0.21052632],\n",
       "       [0.99528549, 0.26315789],\n",
       "       [0.99528549, 0.31578947],\n",
       "       [0.99528549, 0.36842105],\n",
       "       [0.99528549, 0.42105263],\n",
       "       [0.99528549, 0.47368421],\n",
       "       [0.99528549, 0.52631579],\n",
       "       [0.99528549, 0.57894737],\n",
       "       [0.99528549, 0.63157895],\n",
       "       [0.99528549, 0.68421053],\n",
       "       [0.99528549, 0.73684211],\n",
       "       [0.99528549, 0.78947368],\n",
       "       [0.99528549, 0.84210526],\n",
       "       [0.99528549, 0.89473684],\n",
       "       [0.99528549, 0.94736842],\n",
       "       [0.99528549, 1.        ]])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization\n",
    "XpBS_minmax2 = min_max_scaler2.transform(XpBS_values)\n",
    "XpBS_minmax2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "a2deec8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "XpBS_tensor2 = torch.from_numpy(XpBS_minmax2).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "bafa8d60",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    throughput_pred = model1(XpBS_tensor2)\n",
    "y = pd.concat([pd.Series(XpBS_values[:, 0].reshape(-1), name='send rates'), pd.Series(XpBS_values[:, 1].reshape(-1), name='block size'), \n",
    "               pd.Series(throughput_pred.numpy().reshape(-1), name='throughput_pred')], axis=1)\n",
    "y.index = XpBS.index\n",
    "y.to_csv('../../Data/Result/Related3/throughput_pred_related3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e373ef2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>block size</th>\n",
       "      <th>throughput_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>15.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>15.304539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>15.541059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>15.777582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>16.014105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>160.012894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>200</td>\n",
       "      <td>170</td>\n",
       "      <td>160.249405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>200</td>\n",
       "      <td>180</td>\n",
       "      <td>160.485947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>200</td>\n",
       "      <td>190</td>\n",
       "      <td>160.722458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>160.958969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     send rates  block size  throughput_pred\n",
       "0            10          10        15.068465\n",
       "1            10          20        15.304539\n",
       "2            10          30        15.541059\n",
       "3            10          40        15.777582\n",
       "4            10          50        16.014105\n",
       "..          ...         ...              ...\n",
       "395         200         160       160.012894\n",
       "396         200         170       160.249405\n",
       "397         200         180       160.485947\n",
       "398         200         190       160.722458\n",
       "399         200         200       160.958969\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "4b522fa3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    latency_pred = model(XpBS_tensor)\n",
    "    throughput_pred = model1(XpBS_tensor2)\n",
    "y = pd.concat([pd.Series(XpBS_values[:, 0].reshape(-1), name='send rates'), pd.Series(XpBS_values[:, 1].reshape(-1), name='block size'), \n",
    "               pd.Series(latency_pred.numpy().reshape(-1), name='latency_pred'),\n",
    "               pd.Series(throughput_pred.numpy().reshape(-1), name='throughput_pred')], axis=1)\n",
    "y.index = XpBS.index\n",
    "y.to_csv('../../Data/Result/Related3/latency_throughput_pred_related3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ae3e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set the score function to chose the best block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "497a5f90",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>block size</th>\n",
       "      <th>latency_pred</th>\n",
       "      <th>throughput_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>15.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.761386</td>\n",
       "      <td>15.304539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>1.080978</td>\n",
       "      <td>15.541059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>1.097761</td>\n",
       "      <td>15.777582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>1.152921</td>\n",
       "      <td>16.014105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>2.706232</td>\n",
       "      <td>160.012894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>200</td>\n",
       "      <td>170</td>\n",
       "      <td>2.351660</td>\n",
       "      <td>160.249405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>200</td>\n",
       "      <td>180</td>\n",
       "      <td>2.354682</td>\n",
       "      <td>160.485947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>200</td>\n",
       "      <td>190</td>\n",
       "      <td>2.437658</td>\n",
       "      <td>160.722458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>2.620018</td>\n",
       "      <td>160.958969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     send rates  block size  latency_pred  throughput_pred\n",
       "0            10          10      0.263789        15.068465\n",
       "1            10          20      0.761386        15.304539\n",
       "2            10          30      1.080978        15.541059\n",
       "3            10          40      1.097761        15.777582\n",
       "4            10          50      1.152921        16.014105\n",
       "..          ...         ...           ...              ...\n",
       "395         200         160      2.706232       160.012894\n",
       "396         200         170      2.351660       160.249405\n",
       "397         200         180      2.354682       160.485947\n",
       "398         200         190      2.437658       160.722458\n",
       "399         200         200      2.620018       160.958969\n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "68b3bba8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "    send rates  block size  latency_pred  throughput_pred\n",
      "0           10          10      0.263789        15.068465\n",
      "1           10          20      0.761386        15.304539\n",
      "2           10          30      1.080978        15.541059\n",
      "3           10          40      1.097761        15.777582\n",
      "4           10          50      1.152921        16.014105\n",
      "5           10          60      1.167963        16.250626\n",
      "6           10          70      1.205774        16.487146\n",
      "7           10          80      1.114404        16.723669\n",
      "8           10          90      1.144804        16.960192\n",
      "9           10         100      1.132989        17.196712\n",
      "10          10         110      1.089252        17.433235\n",
      "11          10         120      1.092720        17.669756\n",
      "12          10         130      1.096455        17.906279\n",
      "13          10         140      1.093285        18.142801\n",
      "14          10         150      1.089604        18.379322\n",
      "15          10         160      1.081395        18.615845\n",
      "16          10         170      1.109392        18.852365\n",
      "17          10         180      1.127661        19.088888\n",
      "18          10         190      1.115500        19.325409\n",
      "19          10         200      1.103337        19.562485\n",
      "20\n",
      "    send rates  block size  latency_pred  throughput_pred\n",
      "20          20          10      0.155900        22.509968\n",
      "21          20          20      0.397323        22.746487\n",
      "22          20          30      0.685939        22.983009\n",
      "23          20          40      0.866808        23.219532\n",
      "24          20          50      1.047620        23.456055\n",
      "25          20          60      1.083343        23.692574\n",
      "26          20          70      1.125193        23.929098\n",
      "27          20          80      1.123034        24.165621\n",
      "28          20          90      1.133289        24.402140\n",
      "29          20         100      1.160087        24.638660\n",
      "30          20         110      1.109058        24.875183\n",
      "31          20         120      1.105128        25.111706\n",
      "32          20         130      1.101446        25.348225\n",
      "33          20         140      1.097764        25.584751\n",
      "34          20         150      1.094082        25.821274\n",
      "35          20         160      1.099720        26.057795\n",
      "36          20         170      1.118711        26.294315\n",
      "37          20         180      1.104691        26.530834\n",
      "38          20         190      1.092529        26.767359\n",
      "39          20         200      1.097975        27.003880\n",
      "30\n",
      "    send rates  block size  latency_pred  throughput_pred\n",
      "40          30          10      0.118345        29.951918\n",
      "41          30          20      0.241769        30.188438\n",
      "42          30          30      0.455765        30.424961\n",
      "43          30          40      0.636815        30.661480\n",
      "44          30          50      0.817628        30.898003\n",
      "45          30          60      0.998439        31.134525\n",
      "46          30          70      1.044134        31.371044\n",
      "47          30          80      1.095101        31.607567\n",
      "48          30          90      1.106462        31.844090\n",
      "49          30         100      1.132952        32.080612\n",
      "50          30         110      1.113517        32.317135\n",
      "51          30         120      1.109606        32.553654\n",
      "52          30         130      1.105925        32.790176\n",
      "53          30         140      1.102244        33.026699\n",
      "54          30         150      1.098562        33.263218\n",
      "55          30         160      1.118045        33.499741\n",
      "56          30         170      1.094734        33.736267\n",
      "57          30         180      1.081720        33.972786\n",
      "58          30         190      1.069559        34.209309\n",
      "59          30         200      1.093379        34.445827\n",
      "40\n",
      "    send rates  block size  latency_pred  throughput_pred\n",
      "60          40          10      0.074766        37.393864\n",
      "61          40          20      0.229174        37.630386\n",
      "62          40          30      0.364109        37.866905\n",
      "63          40          40      0.485518        38.103436\n",
      "64          40          50      0.605926        38.339954\n",
      "65          40          60      0.752637        38.576477\n",
      "66          40          70      0.940806        38.812996\n",
      "67          40          80      1.028626        39.049519\n",
      "68          40          90      1.053199        39.286041\n",
      "69          40         100      1.097860        39.522560\n",
      "70          40         110      1.115794        39.759083\n",
      "71          40         120      1.114085        39.995602\n",
      "72          40         130      1.110404        40.232128\n",
      "73          40         140      1.106722        40.468651\n",
      "74          40         150      1.108213        40.705173\n",
      "75          40         160      1.114056        40.941692\n",
      "76          40         170      1.101126        41.178215\n",
      "77          40         180      1.101486        41.414738\n",
      "78          40         190      1.118886        41.651260\n",
      "79          40         200      1.099087        41.887779\n",
      "50\n",
      "    send rates  block size  latency_pred  throughput_pred\n",
      "80          50          10      0.035269        44.835815\n",
      "81          50          20      0.174059        45.072338\n",
      "82          50          30      0.294447        45.308857\n",
      "83          50          40      0.405010        45.545380\n",
      "84          50          50      0.532267        45.781906\n",
      "85          50          60      0.659525        46.018421\n",
      "86          50          70      0.774571        46.254948\n",
      "87          50          80      0.891201        46.491467\n",
      "88          50          90      0.978460        46.727985\n",
      "89          50         100      1.099917        46.964512\n",
      "90          50         110      1.130480        47.201031\n",
      "91          50         120      1.180757        47.437557\n",
      "92          50         130      1.189597        47.674076\n",
      "93          50         140      1.198437        47.910595\n",
      "94          50         150      1.178569        48.147118\n",
      "95          50         160      1.184666        48.383644\n",
      "96          50         170      1.172741        48.620167\n",
      "97          50         180      1.163757        48.856686\n",
      "98          50         190      1.139515        49.093208\n",
      "99          50         200      1.116668        49.329731\n",
      "60\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "100          60          10      0.049518        52.277767\n",
      "101          60          20      0.150596        52.514286\n",
      "102          60          30      0.233558        52.750805\n",
      "103          60          40      0.327275        52.987331\n",
      "104          60          50      0.451204        53.223846\n",
      "105          60          60      0.578811        53.460373\n",
      "106          60          70      0.713216        53.696899\n",
      "107          60          80      0.789792        53.933422\n",
      "108          60          90      0.841445        54.169941\n",
      "109          60         100      0.985645        54.406460\n",
      "110          60         110      1.064084        54.642979\n",
      "111          60         120      1.166176        54.879501\n",
      "112          60         130      1.211983        55.116028\n",
      "113          60         140      1.228073        55.352547\n",
      "114          60         150      1.207975        55.589066\n",
      "115          60         160      1.221464        55.825588\n",
      "116          60         170      1.220241        56.062115\n",
      "117          60         180      1.182472        56.298637\n",
      "118          60         190      1.138895        56.535160\n",
      "119          60         200      1.134129        56.771679\n",
      "70\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "120          70          10      0.063975        59.719715\n",
      "121          70          20      0.158033        59.956242\n",
      "122          70          30      0.234877        60.192760\n",
      "123          70          40      0.310092        60.429279\n",
      "124          70          50      0.395259        60.665806\n",
      "125          70          60      0.489138        60.902325\n",
      "126          70          70      0.625500        61.138844\n",
      "127          70          80      0.700735        61.375362\n",
      "128          70          90      0.775970        61.611889\n",
      "129          70         100      0.818950        61.848412\n",
      "130          70         110      0.939786        62.084938\n",
      "131          70         120      1.040624        62.321461\n",
      "132          70         130      1.118488        62.557980\n",
      "133          70         140      1.097116        62.794491\n",
      "134          70         150      1.075744        63.031017\n",
      "135          70         160      1.032114        63.267540\n",
      "136          70         170      1.055850        63.504070\n",
      "137          70         180      1.054994        63.740585\n",
      "138          70         190      1.019303        63.977108\n",
      "139          70         200      1.051304        64.213623\n",
      "80\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "140          80          10      0.050968        67.161659\n",
      "141          80          20      0.137202        67.398186\n",
      "142          80          30      0.211611        67.634705\n",
      "143          80          40      0.286020        67.871223\n",
      "144          80          50      0.366152        68.107742\n",
      "145          80          60      0.460031        68.344261\n",
      "146          80          70      0.565376        68.580788\n",
      "147          80          80      0.641577        68.817314\n",
      "148          80          90      0.684426        69.053833\n",
      "149          80         100      0.803612        69.290359\n",
      "150          80         110      0.813951        69.526878\n",
      "151          80         120      0.903012        69.763405\n",
      "152          80         130      0.955589        69.999924\n",
      "153          80         140      0.956646        70.236443\n",
      "154          80         150      0.899496        70.472969\n",
      "155          80         160      0.900649        70.709496\n",
      "156          80         170      0.916069        70.946007\n",
      "157          80         180      0.937325        71.182533\n",
      "158          80         190      0.881765        71.419052\n",
      "159          80         200      0.885110        71.655579\n",
      "90\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "160          90          10      0.051824        74.603607\n",
      "161          90          20      0.117380        74.840134\n",
      "162          90          30      0.188346        75.076653\n",
      "163          90          40      0.262754        75.313171\n",
      "164          90          50      0.337773        75.549690\n",
      "165          90          60      0.430924        75.786217\n",
      "166          90          70      0.524803        76.022728\n",
      "167          90          80      0.631479        76.259254\n",
      "168          90          90      0.674328        76.495781\n",
      "169          90         100      0.775931        76.732307\n",
      "170          90         110      0.883040        76.968826\n",
      "171          90         120      0.782240        77.205345\n",
      "172          90         130      0.821437        77.441864\n",
      "173          90         140      0.866943        77.678398\n",
      "174          90         150      0.868096        77.914917\n",
      "175          90         160      0.845203        78.151436\n",
      "176          90         170      0.877164        78.387962\n",
      "177          90         180      0.866264        78.624481\n",
      "178          90         190      0.827027        78.861000\n",
      "179          90         200      0.828437        79.097519\n",
      "100\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "180         100          10      0.062724        82.045555\n",
      "181         100          20      0.118022        82.282082\n",
      "182         100          30      0.182012        82.518608\n",
      "183         100          40      0.246002        82.755127\n",
      "184         100          50      0.313898        82.991646\n",
      "185         100          60      0.401816        83.228172\n",
      "186         100          70      0.495695        83.464691\n",
      "187         100          80      0.621382        83.701210\n",
      "188         100          90      0.664231        83.937737\n",
      "189         100         100      0.758520        84.174255\n",
      "190         100         110      0.874314        84.410774\n",
      "191         100         120      0.882315        84.647301\n",
      "192         100         130      0.841869        84.883820\n",
      "193         100         140      0.859928        85.120346\n",
      "194         100         150      0.852031        85.356865\n",
      "195         100         160      0.767332        85.593384\n",
      "196         100         170      0.815519        85.829910\n",
      "197         100         180      0.795019        86.066429\n",
      "198         100         190      0.772288        86.302956\n",
      "199         100         200      0.771764        86.539467\n",
      "110\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "200         110          10      0.073622        89.487511\n",
      "201         110          20      0.118663        89.724030\n",
      "202         110          30      0.182653        89.960548\n",
      "203         110          40      0.246643        90.197075\n",
      "204         110          50      0.310633        90.433594\n",
      "205         110          60      0.382289        90.670120\n",
      "206         110          70      0.466587        90.906639\n",
      "207         110          80      0.611284        91.143166\n",
      "208         110          90      0.654133        91.379684\n",
      "209         110         100      0.738898        91.616211\n",
      "210         110         110      0.860017        91.852730\n",
      "211         110         120      0.857508        92.089249\n",
      "212         110         130      0.882671        92.325775\n",
      "213         110         140      0.897286        92.562294\n",
      "214         110         150      1.008930        92.798813\n",
      "215         110         160      0.841726        93.035332\n",
      "216         110         170      0.791650        93.271851\n",
      "217         110         180      1.114442        93.508385\n",
      "218         110         190      1.039646        93.744896\n",
      "219         110         200      0.726319        93.981430\n",
      "120\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "220         120          10      0.084523        96.929436\n",
      "221         120          20      0.119304        97.165977\n",
      "222         120          30      0.183294        97.402496\n",
      "223         120          40      0.247284        97.639030\n",
      "224         120          50      0.311274        97.875557\n",
      "225         120          60      0.377484        98.112053\n",
      "226         120          70      0.460548        98.348587\n",
      "227         120          80      0.613835        98.585114\n",
      "228         120          90      0.646538        98.821632\n",
      "229         120         100      0.711627        99.058144\n",
      "230         120         110      0.835819        99.294670\n",
      "231         120         120      0.832703        99.531204\n",
      "232         120         130      0.827875        99.767731\n",
      "233         120         140      1.459007       100.004234\n",
      "234         120         150      1.025172       100.240753\n",
      "235         120         160      0.800259       100.477272\n",
      "236         120         170      0.802888       100.713799\n",
      "237         120         180      0.791528       100.950325\n",
      "238         120         190      0.772733       101.186844\n",
      "239         120         200      0.778814       101.423363\n",
      "130\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "240         130          10      0.098502       104.371407\n",
      "241         130          20      0.119945       104.607948\n",
      "242         130          30      0.183936       104.844460\n",
      "243         130          40      0.247925       105.080978\n",
      "244         130          50      0.311915       105.317497\n",
      "245         130          60      0.375905       105.554024\n",
      "246         130          70      0.455347       105.790535\n",
      "247         130          80      0.627644       106.027061\n",
      "248         130          90      0.650893       106.263588\n",
      "249         130         100      0.695342       106.500099\n",
      "250         130         110      0.815028       106.736626\n",
      "251         130         120      0.819984       106.973152\n",
      "252         130         130      0.821294       107.209679\n",
      "253         130         140      1.018586       107.446190\n",
      "254         130         150      0.903704       107.682716\n",
      "255         130         160      0.836764       107.919235\n",
      "256         130         170      0.825101       108.155769\n",
      "257         130         180      0.853174       108.392296\n",
      "258         130         190      0.816633       108.628799\n",
      "259         130         200      0.766309       108.865326\n",
      "140\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "260         140          10      0.118054       111.813362\n",
      "261         140          20      0.148960       112.049873\n",
      "262         140          30      0.215015       112.286400\n",
      "263         140          40      0.281070       112.522926\n",
      "264         140          50      0.347124       112.759445\n",
      "265         140          60      0.413422       112.995972\n",
      "266         140          70      0.493578       113.232491\n",
      "267         140          80      0.664002       113.469009\n",
      "268         140          90      0.712898       113.705536\n",
      "269         140         100      0.766055       113.942055\n",
      "270         140         110      0.876702       114.178574\n",
      "271         140         120      0.887965       114.415100\n",
      "272         140         130      0.865114       114.651619\n",
      "273         140         140      0.908109       114.888145\n",
      "274         140         150      0.984429       115.124657\n",
      "275         140         160      0.962913       115.361168\n",
      "276         140         170      0.824189       115.597702\n",
      "277         140         180      0.865595       115.834236\n",
      "278         140         190      0.843423       116.070763\n",
      "279         140         200      0.824949       116.307259\n",
      "150\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "280         150          10      0.189969       119.255318\n",
      "281         150          20      0.141362       119.491829\n",
      "282         150          30      0.213657       119.728355\n",
      "283         150          40      0.285951       119.964882\n",
      "284         150          50      0.358246       120.201408\n",
      "285         150          60      0.481801       120.437927\n",
      "286         150          70      0.554579       120.674446\n",
      "287         150          80      1.317163       120.910973\n",
      "288         150          90      0.847866       121.147491\n",
      "289         150         100      0.889090       121.384003\n",
      "290         150         110      0.881981       121.620522\n",
      "291         150         120      0.882913       121.857048\n",
      "292         150         130      0.802239       122.093582\n",
      "293         150         140      0.844332       122.330101\n",
      "294         150         150      1.004015       122.566628\n",
      "295         150         160      0.918344       122.803139\n",
      "296         150         170      0.861700       123.039665\n",
      "297         150         180      0.870888       123.276176\n",
      "298         150         190      0.870212       123.512703\n",
      "299         150         200      0.956028       123.749222\n",
      "160\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "300         160          10      0.383779       126.697258\n",
      "301         160          20      0.208175       126.933777\n",
      "302         160          30      0.272684       127.170303\n",
      "303         160          40      0.291070       127.406822\n",
      "304         160          50      0.273821       127.643333\n",
      "305         160          60      0.434518       127.879875\n",
      "306         160          70      0.657056       128.116409\n",
      "307         160          80      1.326645       128.352921\n",
      "308         160          90      1.084123       128.589432\n",
      "309         160         100      1.309100       128.825958\n",
      "310         160         110      1.684730       129.062485\n",
      "311         160         120      1.171908       129.299011\n",
      "312         160         130      1.073841       129.535538\n",
      "313         160         140      1.158722       129.772049\n",
      "314         160         150      1.341131       130.008560\n",
      "315         160         160      1.868159       130.245102\n",
      "316         160         170      1.066184       130.481613\n",
      "317         160         180      1.117047       130.718140\n",
      "318         160         190      0.902836       130.954666\n",
      "319         160         200      0.931205       131.191177\n",
      "170\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "320         170          10      1.487002       134.139221\n",
      "321         170          20      0.394718       134.375717\n",
      "322         170          30      0.299055       134.612259\n",
      "323         170          40      0.212505       134.848770\n",
      "324         170          50      0.386539       135.085312\n",
      "325         170          60      0.489699       135.321823\n",
      "326         170          70      0.614435       135.558350\n",
      "327         170          80      2.139231       135.794876\n",
      "328         170          90      0.856186       136.031387\n",
      "329         170         100      1.525549       136.267914\n",
      "330         170         110      2.075055       136.504440\n",
      "331         170         120      0.806171       136.740952\n",
      "332         170         130      0.816980       136.977478\n",
      "333         170         140      0.937694       137.213989\n",
      "334         170         150      1.437641       137.450531\n",
      "335         170         160      2.708397       137.687027\n",
      "336         170         170      1.544507       137.923553\n",
      "337         170         180      1.545217       138.160080\n",
      "338         170         190      1.155119       138.396622\n",
      "339         170         200      0.977175       138.633118\n",
      "180\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "340         180          10      1.492652       141.581161\n",
      "341         180          20      0.378550       141.817673\n",
      "342         180          30      0.450190       142.054214\n",
      "343         180          40      0.634388       142.290726\n",
      "344         180          50      0.671767       142.527252\n",
      "345         180          60      0.688444       142.763763\n",
      "346         180          70      0.705122       143.000305\n",
      "347         180          80      2.855116       143.236832\n",
      "348         180          90      1.813832       143.473343\n",
      "349         180         100      2.305609       143.709885\n",
      "350         180         110      2.644391       143.946381\n",
      "351         180         120      1.045426       144.182907\n",
      "352         180         130      0.987322       144.419418\n",
      "353         180         140      1.108630       144.655960\n",
      "354         180         150      1.522662       144.892471\n",
      "355         180         160      2.741954       145.128998\n",
      "356         180         170      2.118181       145.365524\n",
      "357         180         180      1.604608       145.602051\n",
      "358         180         190      1.251604       145.838562\n",
      "359         180         200      1.134738       146.075073\n",
      "190\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "360         190          10      1.576311       149.023117\n",
      "361         190          20      0.972377       149.259644\n",
      "362         190          30      0.875724       149.496155\n",
      "363         190          40      0.930956       149.732666\n",
      "364         190          50      0.935589       149.969193\n",
      "365         190          60      0.909099       150.205719\n",
      "366         190          70      0.882609       150.442245\n",
      "367         190          80      2.546600       150.678757\n",
      "368         190          90      1.884537       150.915298\n",
      "369         190         100      2.613135       151.151810\n",
      "370         190         110      2.880932       151.388336\n",
      "371         190         120      1.421129       151.624847\n",
      "372         190         130      1.141806       151.861374\n",
      "373         190         140      1.314090       152.097885\n",
      "374         190         150      1.574455       152.334427\n",
      "375         190         160      2.742911       152.570938\n",
      "376         190         170      2.504016       152.807465\n",
      "377         190         180      1.972616       153.043991\n",
      "378         190         190      1.619610       153.280502\n",
      "379         190         200      1.767018       153.517029\n",
      "200\n",
      "     send rates  block size  latency_pred  throughput_pred\n",
      "380         200          10      2.087606       156.465073\n",
      "381         200          20      1.483672       156.701584\n",
      "382         200          30      1.238566       156.938126\n",
      "383         200          40      1.249904       157.174622\n",
      "384         200          50      1.261239       157.411148\n",
      "385         200          60      1.272575       157.647675\n",
      "386         200          70      1.277788       157.884201\n",
      "387         200          80      2.331211       158.120728\n",
      "388         200          90      2.076033       158.357254\n",
      "389         200         100      2.599325       158.593765\n",
      "390         200         110      2.815543       158.830292\n",
      "391         200         120      1.697848       159.066803\n",
      "392         200         130      1.895067       159.303329\n",
      "393         200         140      2.090789       159.539841\n",
      "394         200         150      2.396083       159.776382\n",
      "395         200         160      2.706232       160.012894\n",
      "396         200         170      2.351660       160.249405\n",
      "397         200         180      2.354682       160.485947\n",
      "398         200         190      2.437658       160.722458\n",
      "399         200         200      2.620018       160.958969\n"
     ]
    }
   ],
   "source": [
    "for name, group in y.groupby('send rates', sort=False):\n",
    "    print(name)\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "6118b32d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>send rates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.205774</td>\n",
       "      <td>0.263789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.160087</td>\n",
       "      <td>0.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.132952</td>\n",
       "      <td>0.118345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.118886</td>\n",
       "      <td>0.074766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.198437</td>\n",
       "      <td>0.035269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.228073</td>\n",
       "      <td>0.049518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.118488</td>\n",
       "      <td>0.063975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.956646</td>\n",
       "      <td>0.050968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.883040</td>\n",
       "      <td>0.051824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.882315</td>\n",
       "      <td>0.062724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.114442</td>\n",
       "      <td>0.073622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1.459007</td>\n",
       "      <td>0.084523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.018586</td>\n",
       "      <td>0.098502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.984429</td>\n",
       "      <td>0.118054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.317163</td>\n",
       "      <td>0.141362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.868159</td>\n",
       "      <td>0.208175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2.708397</td>\n",
       "      <td>0.212505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2.855116</td>\n",
       "      <td>0.378550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2.880932</td>\n",
       "      <td>0.875724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2.815543</td>\n",
       "      <td>1.238566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 max       min\n",
       "send rates                    \n",
       "10          1.205774  0.263789\n",
       "20          1.160087  0.155900\n",
       "30          1.132952  0.118345\n",
       "40          1.118886  0.074766\n",
       "50          1.198437  0.035269\n",
       "60          1.228073  0.049518\n",
       "70          1.118488  0.063975\n",
       "80          0.956646  0.050968\n",
       "90          0.883040  0.051824\n",
       "100         0.882315  0.062724\n",
       "110         1.114442  0.073622\n",
       "120         1.459007  0.084523\n",
       "130         1.018586  0.098502\n",
       "140         0.984429  0.118054\n",
       "150         1.317163  0.141362\n",
       "160         1.868159  0.208175\n",
       "170         2.708397  0.212505\n",
       "180         2.855116  0.378550\n",
       "190         2.880932  0.875724\n",
       "200         2.815543  1.238566"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_max_min = y.groupby('send rates', sort=False)['latency_pred'].agg(['max', 'min'])\n",
    "la_max_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "1f4f4fc3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latency_max</th>\n",
       "      <th>latency_min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>send rates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.205774</td>\n",
       "      <td>0.263789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.160087</td>\n",
       "      <td>0.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.132952</td>\n",
       "      <td>0.118345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.118886</td>\n",
       "      <td>0.074766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.198437</td>\n",
       "      <td>0.035269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.228073</td>\n",
       "      <td>0.049518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.118488</td>\n",
       "      <td>0.063975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.956646</td>\n",
       "      <td>0.050968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.883040</td>\n",
       "      <td>0.051824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.882315</td>\n",
       "      <td>0.062724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.114442</td>\n",
       "      <td>0.073622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1.459007</td>\n",
       "      <td>0.084523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.018586</td>\n",
       "      <td>0.098502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.984429</td>\n",
       "      <td>0.118054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.317163</td>\n",
       "      <td>0.141362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.868159</td>\n",
       "      <td>0.208175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2.708397</td>\n",
       "      <td>0.212505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2.855116</td>\n",
       "      <td>0.378550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2.880932</td>\n",
       "      <td>0.875724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2.815543</td>\n",
       "      <td>1.238566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            latency_max  latency_min\n",
       "send rates                          \n",
       "10             1.205774     0.263789\n",
       "20             1.160087     0.155900\n",
       "30             1.132952     0.118345\n",
       "40             1.118886     0.074766\n",
       "50             1.198437     0.035269\n",
       "60             1.228073     0.049518\n",
       "70             1.118488     0.063975\n",
       "80             0.956646     0.050968\n",
       "90             0.883040     0.051824\n",
       "100            0.882315     0.062724\n",
       "110            1.114442     0.073622\n",
       "120            1.459007     0.084523\n",
       "130            1.018586     0.098502\n",
       "140            0.984429     0.118054\n",
       "150            1.317163     0.141362\n",
       "160            1.868159     0.208175\n",
       "170            2.708397     0.212505\n",
       "180            2.855116     0.378550\n",
       "190            2.880932     0.875724\n",
       "200            2.815543     1.238566"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_col=['latency_max', 'latency_min']\n",
    "la_max_min.columns = la_col\n",
    "la_max_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "8c0d6f69",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>send rates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19.562485</td>\n",
       "      <td>15.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>27.003880</td>\n",
       "      <td>22.509968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>34.445827</td>\n",
       "      <td>29.951918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41.887779</td>\n",
       "      <td>37.393864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>49.329731</td>\n",
       "      <td>44.835815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>56.771679</td>\n",
       "      <td>52.277767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>64.213623</td>\n",
       "      <td>59.719715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>71.655579</td>\n",
       "      <td>67.161659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>79.097519</td>\n",
       "      <td>74.603607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>86.539467</td>\n",
       "      <td>82.045555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>93.981430</td>\n",
       "      <td>89.487511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>101.423363</td>\n",
       "      <td>96.929436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>108.865326</td>\n",
       "      <td>104.371407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>116.307259</td>\n",
       "      <td>111.813362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>123.749222</td>\n",
       "      <td>119.255318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>131.191177</td>\n",
       "      <td>126.697258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>138.633118</td>\n",
       "      <td>134.139221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>146.075073</td>\n",
       "      <td>141.581161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>153.517029</td>\n",
       "      <td>149.023117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>160.958969</td>\n",
       "      <td>156.465073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   max         min\n",
       "send rates                        \n",
       "10           19.562485   15.068465\n",
       "20           27.003880   22.509968\n",
       "30           34.445827   29.951918\n",
       "40           41.887779   37.393864\n",
       "50           49.329731   44.835815\n",
       "60           56.771679   52.277767\n",
       "70           64.213623   59.719715\n",
       "80           71.655579   67.161659\n",
       "90           79.097519   74.603607\n",
       "100          86.539467   82.045555\n",
       "110          93.981430   89.487511\n",
       "120         101.423363   96.929436\n",
       "130         108.865326  104.371407\n",
       "140         116.307259  111.813362\n",
       "150         123.749222  119.255318\n",
       "160         131.191177  126.697258\n",
       "170         138.633118  134.139221\n",
       "180         146.075073  141.581161\n",
       "190         153.517029  149.023117\n",
       "200         160.958969  156.465073"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_max_min = y.groupby('send rates', sort=False)['throughput_pred'].agg(['max', 'min'])\n",
    "th_max_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "ad506d61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>throughput_max</th>\n",
       "      <th>throughput_min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>send rates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19.562485</td>\n",
       "      <td>15.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>27.003880</td>\n",
       "      <td>22.509968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>34.445827</td>\n",
       "      <td>29.951918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41.887779</td>\n",
       "      <td>37.393864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>49.329731</td>\n",
       "      <td>44.835815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>56.771679</td>\n",
       "      <td>52.277767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>64.213623</td>\n",
       "      <td>59.719715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>71.655579</td>\n",
       "      <td>67.161659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>79.097519</td>\n",
       "      <td>74.603607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>86.539467</td>\n",
       "      <td>82.045555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>93.981430</td>\n",
       "      <td>89.487511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>101.423363</td>\n",
       "      <td>96.929436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>108.865326</td>\n",
       "      <td>104.371407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>116.307259</td>\n",
       "      <td>111.813362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>123.749222</td>\n",
       "      <td>119.255318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>131.191177</td>\n",
       "      <td>126.697258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>138.633118</td>\n",
       "      <td>134.139221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>146.075073</td>\n",
       "      <td>141.581161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>153.517029</td>\n",
       "      <td>149.023117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>160.958969</td>\n",
       "      <td>156.465073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            throughput_max  throughput_min\n",
       "send rates                                \n",
       "10               19.562485       15.068465\n",
       "20               27.003880       22.509968\n",
       "30               34.445827       29.951918\n",
       "40               41.887779       37.393864\n",
       "50               49.329731       44.835815\n",
       "60               56.771679       52.277767\n",
       "70               64.213623       59.719715\n",
       "80               71.655579       67.161659\n",
       "90               79.097519       74.603607\n",
       "100              86.539467       82.045555\n",
       "110              93.981430       89.487511\n",
       "120             101.423363       96.929436\n",
       "130             108.865326      104.371407\n",
       "140             116.307259      111.813362\n",
       "150             123.749222      119.255318\n",
       "160             131.191177      126.697258\n",
       "170             138.633118      134.139221\n",
       "180             146.075073      141.581161\n",
       "190             153.517029      149.023117\n",
       "200             160.958969      156.465073"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_col=['throughput_max', 'throughput_min']\n",
    "th_max_min.columns = th_col\n",
    "th_max_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "5fd6d2d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>latency_max</th>\n",
       "      <th>latency_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>0.263789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>1.160087</td>\n",
       "      <td>0.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>1.132952</td>\n",
       "      <td>0.118345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>1.118886</td>\n",
       "      <td>0.074766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>1.198437</td>\n",
       "      <td>0.035269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>1.228073</td>\n",
       "      <td>0.049518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70</td>\n",
       "      <td>1.118488</td>\n",
       "      <td>0.063975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80</td>\n",
       "      <td>0.956646</td>\n",
       "      <td>0.050968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>90</td>\n",
       "      <td>0.883040</td>\n",
       "      <td>0.051824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>0.882315</td>\n",
       "      <td>0.062724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>110</td>\n",
       "      <td>1.114442</td>\n",
       "      <td>0.073622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>120</td>\n",
       "      <td>1.459007</td>\n",
       "      <td>0.084523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>130</td>\n",
       "      <td>1.018586</td>\n",
       "      <td>0.098502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>140</td>\n",
       "      <td>0.984429</td>\n",
       "      <td>0.118054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150</td>\n",
       "      <td>1.317163</td>\n",
       "      <td>0.141362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>160</td>\n",
       "      <td>1.868159</td>\n",
       "      <td>0.208175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>170</td>\n",
       "      <td>2.708397</td>\n",
       "      <td>0.212505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>180</td>\n",
       "      <td>2.855116</td>\n",
       "      <td>0.378550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>190</td>\n",
       "      <td>2.880932</td>\n",
       "      <td>0.875724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>1.238566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    send rates  latency_max  latency_min\n",
       "0           10     1.205774     0.263789\n",
       "1           20     1.160087     0.155900\n",
       "2           30     1.132952     0.118345\n",
       "3           40     1.118886     0.074766\n",
       "4           50     1.198437     0.035269\n",
       "5           60     1.228073     0.049518\n",
       "6           70     1.118488     0.063975\n",
       "7           80     0.956646     0.050968\n",
       "8           90     0.883040     0.051824\n",
       "9          100     0.882315     0.062724\n",
       "10         110     1.114442     0.073622\n",
       "11         120     1.459007     0.084523\n",
       "12         130     1.018586     0.098502\n",
       "13         140     0.984429     0.118054\n",
       "14         150     1.317163     0.141362\n",
       "15         160     1.868159     0.208175\n",
       "16         170     2.708397     0.212505\n",
       "17         180     2.855116     0.378550\n",
       "18         190     2.880932     0.875724\n",
       "19         200     2.815543     1.238566"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restore the grouping index to the column\n",
    "la_max_min.reset_index(inplace=True)\n",
    "la_max_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "655b55c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>throughput_max</th>\n",
       "      <th>throughput_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>19.562485</td>\n",
       "      <td>15.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>27.003880</td>\n",
       "      <td>22.509968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>34.445827</td>\n",
       "      <td>29.951918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>41.887779</td>\n",
       "      <td>37.393864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>49.329731</td>\n",
       "      <td>44.835815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>56.771679</td>\n",
       "      <td>52.277767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70</td>\n",
       "      <td>64.213623</td>\n",
       "      <td>59.719715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80</td>\n",
       "      <td>71.655579</td>\n",
       "      <td>67.161659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>90</td>\n",
       "      <td>79.097519</td>\n",
       "      <td>74.603607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>86.539467</td>\n",
       "      <td>82.045555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>110</td>\n",
       "      <td>93.981430</td>\n",
       "      <td>89.487511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>120</td>\n",
       "      <td>101.423363</td>\n",
       "      <td>96.929436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>130</td>\n",
       "      <td>108.865326</td>\n",
       "      <td>104.371407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>140</td>\n",
       "      <td>116.307259</td>\n",
       "      <td>111.813362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150</td>\n",
       "      <td>123.749222</td>\n",
       "      <td>119.255318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>160</td>\n",
       "      <td>131.191177</td>\n",
       "      <td>126.697258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>170</td>\n",
       "      <td>138.633118</td>\n",
       "      <td>134.139221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>180</td>\n",
       "      <td>146.075073</td>\n",
       "      <td>141.581161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>190</td>\n",
       "      <td>153.517029</td>\n",
       "      <td>149.023117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200</td>\n",
       "      <td>160.958969</td>\n",
       "      <td>156.465073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    send rates  throughput_max  throughput_min\n",
       "0           10       19.562485       15.068465\n",
       "1           20       27.003880       22.509968\n",
       "2           30       34.445827       29.951918\n",
       "3           40       41.887779       37.393864\n",
       "4           50       49.329731       44.835815\n",
       "5           60       56.771679       52.277767\n",
       "6           70       64.213623       59.719715\n",
       "7           80       71.655579       67.161659\n",
       "8           90       79.097519       74.603607\n",
       "9          100       86.539467       82.045555\n",
       "10         110       93.981430       89.487511\n",
       "11         120      101.423363       96.929436\n",
       "12         130      108.865326      104.371407\n",
       "13         140      116.307259      111.813362\n",
       "14         150      123.749222      119.255318\n",
       "15         160      131.191177      126.697258\n",
       "16         170      138.633118      134.139221\n",
       "17         180      146.075073      141.581161\n",
       "18         190      153.517029      149.023117\n",
       "19         200      160.958969      156.465073"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_max_min.reset_index(inplace=True)\n",
    "th_max_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "e523cacd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_max = la_max_min['latency_max'].values\n",
    "la_max = list(la_max)\n",
    "la_max\n",
    "len(la_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "8242cbc0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.205774,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1600865,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1329519,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1188859,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.1984369,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.2280735,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 1.1184882,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.9566456,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.8830401,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 0.88231456,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.1144425,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.4590067,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 1.0185858,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 0.9844285,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.3171631,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 1.8681587,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.7083974,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8551164,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8809319,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427,\n",
       " 2.8155427]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_max_all = []\n",
    "for i in range(len(la_max)):\n",
    "    la_max_all.extend([la_max[i]]*20)\n",
    "    \n",
    "la_max_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "e5097c54",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26378894,\n",
       " 0.15589988,\n",
       " 0.1183455,\n",
       " 0.07476628,\n",
       " 0.035268545,\n",
       " 0.049518466,\n",
       " 0.06397486,\n",
       " 0.050967813,\n",
       " 0.051823974,\n",
       " 0.062723756,\n",
       " 0.073622465,\n",
       " 0.08452296,\n",
       " 0.09850228,\n",
       " 0.11805391,\n",
       " 0.14136195,\n",
       " 0.2081747,\n",
       " 0.2125045,\n",
       " 0.37855017,\n",
       " 0.87572384,\n",
       " 1.2385656]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_min = la_max_min['latency_min'].values\n",
    "la_min = list(la_min)\n",
    "la_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5bf02006",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.26378894,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.15589988,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.1183455,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.07476628,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.035268545,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.049518466,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.06397486,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.050967813,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.051823974,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.062723756,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.073622465,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.08452296,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.09850228,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.11805391,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.14136195,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2081747,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.2125045,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.37855017,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 0.87572384,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656,\n",
       " 1.2385656]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_min_all = []\n",
    "for i in range(len(la_min)):\n",
    "    la_min_all.extend([la_min[i]]*20)\n",
    "    \n",
    "la_min_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "91503210",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>block size</th>\n",
       "      <th>latency_pred</th>\n",
       "      <th>throughput_pred</th>\n",
       "      <th>latency_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>0.263789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.761386</td>\n",
       "      <td>15.304539</td>\n",
       "      <td>0.263789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>1.080978</td>\n",
       "      <td>15.541059</td>\n",
       "      <td>0.263789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>1.097761</td>\n",
       "      <td>15.777582</td>\n",
       "      <td>0.263789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>1.152921</td>\n",
       "      <td>16.014105</td>\n",
       "      <td>0.263789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>2.706232</td>\n",
       "      <td>160.012894</td>\n",
       "      <td>1.238566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>200</td>\n",
       "      <td>170</td>\n",
       "      <td>2.351660</td>\n",
       "      <td>160.249405</td>\n",
       "      <td>1.238566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>200</td>\n",
       "      <td>180</td>\n",
       "      <td>2.354682</td>\n",
       "      <td>160.485947</td>\n",
       "      <td>1.238566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>200</td>\n",
       "      <td>190</td>\n",
       "      <td>2.437658</td>\n",
       "      <td>160.722458</td>\n",
       "      <td>1.238566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>2.620018</td>\n",
       "      <td>160.958969</td>\n",
       "      <td>1.238566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     send rates  block size  latency_pred  throughput_pred  latency_min\n",
       "0            10          10      0.263789        15.068465     0.263789\n",
       "1            10          20      0.761386        15.304539     0.263789\n",
       "2            10          30      1.080978        15.541059     0.263789\n",
       "3            10          40      1.097761        15.777582     0.263789\n",
       "4            10          50      1.152921        16.014105     0.263789\n",
       "..          ...         ...           ...              ...          ...\n",
       "395         200         160      2.706232       160.012894     1.238566\n",
       "396         200         170      2.351660       160.249405     1.238566\n",
       "397         200         180      2.354682       160.485947     1.238566\n",
       "398         200         190      2.437658       160.722458     1.238566\n",
       "399         200         200      2.620018       160.958969     1.238566\n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['latency_min'] = la_min_all\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "f9dd42bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>block size</th>\n",
       "      <th>latency_pred</th>\n",
       "      <th>throughput_pred</th>\n",
       "      <th>latency_min</th>\n",
       "      <th>latency_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.761386</td>\n",
       "      <td>15.304539</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>1.080978</td>\n",
       "      <td>15.541059</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>1.097761</td>\n",
       "      <td>15.777582</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>1.152921</td>\n",
       "      <td>16.014105</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>2.706232</td>\n",
       "      <td>160.012894</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>200</td>\n",
       "      <td>170</td>\n",
       "      <td>2.351660</td>\n",
       "      <td>160.249405</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>200</td>\n",
       "      <td>180</td>\n",
       "      <td>2.354682</td>\n",
       "      <td>160.485947</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>200</td>\n",
       "      <td>190</td>\n",
       "      <td>2.437658</td>\n",
       "      <td>160.722458</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>2.620018</td>\n",
       "      <td>160.958969</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     send rates  block size  latency_pred  throughput_pred  latency_min  \\\n",
       "0            10          10      0.263789        15.068465     0.263789   \n",
       "1            10          20      0.761386        15.304539     0.263789   \n",
       "2            10          30      1.080978        15.541059     0.263789   \n",
       "3            10          40      1.097761        15.777582     0.263789   \n",
       "4            10          50      1.152921        16.014105     0.263789   \n",
       "..          ...         ...           ...              ...          ...   \n",
       "395         200         160      2.706232       160.012894     1.238566   \n",
       "396         200         170      2.351660       160.249405     1.238566   \n",
       "397         200         180      2.354682       160.485947     1.238566   \n",
       "398         200         190      2.437658       160.722458     1.238566   \n",
       "399         200         200      2.620018       160.958969     1.238566   \n",
       "\n",
       "     latency_max  \n",
       "0       1.205774  \n",
       "1       1.205774  \n",
       "2       1.205774  \n",
       "3       1.205774  \n",
       "4       1.205774  \n",
       "..           ...  \n",
       "395     2.815543  \n",
       "396     2.815543  \n",
       "397     2.815543  \n",
       "398     2.815543  \n",
       "399     2.815543  \n",
       "\n",
       "[400 rows x 6 columns]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['latency_max'] = la_max_all\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "ec57b7d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19.562485,\n",
       " 27.00388,\n",
       " 34.445827,\n",
       " 41.88778,\n",
       " 49.32973,\n",
       " 56.77168,\n",
       " 64.21362,\n",
       " 71.65558,\n",
       " 79.09752,\n",
       " 86.53947,\n",
       " 93.98143,\n",
       " 101.42336,\n",
       " 108.865326,\n",
       " 116.30726,\n",
       " 123.74922,\n",
       " 131.19118,\n",
       " 138.63312,\n",
       " 146.07507,\n",
       " 153.51703,\n",
       " 160.95897]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_max = th_max_min['throughput_max'].values\n",
    "th_max = list(th_max)\n",
    "th_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "819cfce3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 19.562485,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 27.00388,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 34.445827,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 41.88778,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 49.32973,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 56.77168,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 64.21362,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 71.65558,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 79.09752,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 86.53947,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 93.98143,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 101.42336,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 108.865326,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 116.30726,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 123.74922,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 131.19118,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 138.63312,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 146.07507,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 153.51703,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897,\n",
       " 160.95897]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_max_all = []\n",
    "for i in range(len(th_max)):\n",
    "    th_max_all.extend([th_max[i]] * 20)\n",
    "    \n",
    "th_max_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "f45f97b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.068465,\n",
       " 22.509968,\n",
       " 29.951918,\n",
       " 37.393864,\n",
       " 44.835815,\n",
       " 52.277767,\n",
       " 59.719715,\n",
       " 67.16166,\n",
       " 74.60361,\n",
       " 82.045555,\n",
       " 89.48751,\n",
       " 96.929436,\n",
       " 104.37141,\n",
       " 111.81336,\n",
       " 119.25532,\n",
       " 126.69726,\n",
       " 134.13922,\n",
       " 141.58116,\n",
       " 149.02312,\n",
       " 156.46507]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_min = th_max_min['throughput_min'].values\n",
    "th_min = list(th_min)\n",
    "th_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "c714f7f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 15.068465,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 22.509968,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 29.951918,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 37.393864,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 44.835815,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 52.277767,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 59.719715,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 67.16166,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 74.60361,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 82.045555,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 89.48751,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 96.929436,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 104.37141,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 111.81336,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 119.25532,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 126.69726,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 134.13922,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 141.58116,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 149.02312,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507,\n",
       " 156.46507]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_min_all = []\n",
    "for i in range(len(th_min)):\n",
    "    th_min_all.extend([th_min[i]] * 20)\n",
    "    \n",
    "th_min_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "dcf114fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>block size</th>\n",
       "      <th>latency_pred</th>\n",
       "      <th>throughput_pred</th>\n",
       "      <th>latency_min</th>\n",
       "      <th>latency_max</th>\n",
       "      <th>throughput_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.761386</td>\n",
       "      <td>15.304539</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>1.080978</td>\n",
       "      <td>15.541059</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>1.097761</td>\n",
       "      <td>15.777582</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>1.152921</td>\n",
       "      <td>16.014105</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>2.706232</td>\n",
       "      <td>160.012894</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>200</td>\n",
       "      <td>170</td>\n",
       "      <td>2.351660</td>\n",
       "      <td>160.249405</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>200</td>\n",
       "      <td>180</td>\n",
       "      <td>2.354682</td>\n",
       "      <td>160.485947</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>200</td>\n",
       "      <td>190</td>\n",
       "      <td>2.437658</td>\n",
       "      <td>160.722458</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>2.620018</td>\n",
       "      <td>160.958969</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     send rates  block size  latency_pred  throughput_pred  latency_min  \\\n",
       "0            10          10      0.263789        15.068465     0.263789   \n",
       "1            10          20      0.761386        15.304539     0.263789   \n",
       "2            10          30      1.080978        15.541059     0.263789   \n",
       "3            10          40      1.097761        15.777582     0.263789   \n",
       "4            10          50      1.152921        16.014105     0.263789   \n",
       "..          ...         ...           ...              ...          ...   \n",
       "395         200         160      2.706232       160.012894     1.238566   \n",
       "396         200         170      2.351660       160.249405     1.238566   \n",
       "397         200         180      2.354682       160.485947     1.238566   \n",
       "398         200         190      2.437658       160.722458     1.238566   \n",
       "399         200         200      2.620018       160.958969     1.238566   \n",
       "\n",
       "     latency_max  throughput_min  \n",
       "0       1.205774       15.068465  \n",
       "1       1.205774       15.068465  \n",
       "2       1.205774       15.068465  \n",
       "3       1.205774       15.068465  \n",
       "4       1.205774       15.068465  \n",
       "..           ...             ...  \n",
       "395     2.815543      156.465073  \n",
       "396     2.815543      156.465073  \n",
       "397     2.815543      156.465073  \n",
       "398     2.815543      156.465073  \n",
       "399     2.815543      156.465073  \n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['throughput_min'] = th_min_all\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "c6303076",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send rates</th>\n",
       "      <th>block size</th>\n",
       "      <th>latency_pred</th>\n",
       "      <th>throughput_pred</th>\n",
       "      <th>latency_min</th>\n",
       "      <th>latency_max</th>\n",
       "      <th>throughput_min</th>\n",
       "      <th>throughput_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>19.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.761386</td>\n",
       "      <td>15.304539</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>19.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>1.080978</td>\n",
       "      <td>15.541059</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>19.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>1.097761</td>\n",
       "      <td>15.777582</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>19.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>1.152921</td>\n",
       "      <td>16.014105</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>19.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>2.706232</td>\n",
       "      <td>160.012894</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465073</td>\n",
       "      <td>160.958969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>200</td>\n",
       "      <td>170</td>\n",
       "      <td>2.351660</td>\n",
       "      <td>160.249405</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465073</td>\n",
       "      <td>160.958969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>200</td>\n",
       "      <td>180</td>\n",
       "      <td>2.354682</td>\n",
       "      <td>160.485947</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465073</td>\n",
       "      <td>160.958969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>200</td>\n",
       "      <td>190</td>\n",
       "      <td>2.437658</td>\n",
       "      <td>160.722458</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465073</td>\n",
       "      <td>160.958969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>2.620018</td>\n",
       "      <td>160.958969</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465073</td>\n",
       "      <td>160.958969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     send rates  block size  latency_pred  throughput_pred  latency_min  \\\n",
       "0            10          10      0.263789        15.068465     0.263789   \n",
       "1            10          20      0.761386        15.304539     0.263789   \n",
       "2            10          30      1.080978        15.541059     0.263789   \n",
       "3            10          40      1.097761        15.777582     0.263789   \n",
       "4            10          50      1.152921        16.014105     0.263789   \n",
       "..          ...         ...           ...              ...          ...   \n",
       "395         200         160      2.706232       160.012894     1.238566   \n",
       "396         200         170      2.351660       160.249405     1.238566   \n",
       "397         200         180      2.354682       160.485947     1.238566   \n",
       "398         200         190      2.437658       160.722458     1.238566   \n",
       "399         200         200      2.620018       160.958969     1.238566   \n",
       "\n",
       "     latency_max  throughput_min  throughput_max  \n",
       "0       1.205774       15.068465       19.562485  \n",
       "1       1.205774       15.068465       19.562485  \n",
       "2       1.205774       15.068465       19.562485  \n",
       "3       1.205774       15.068465       19.562485  \n",
       "4       1.205774       15.068465       19.562485  \n",
       "..           ...             ...             ...  \n",
       "395     2.815543      156.465073      160.958969  \n",
       "396     2.815543      156.465073      160.958969  \n",
       "397     2.815543      156.465073      160.958969  \n",
       "398     2.815543      156.465073      160.958969  \n",
       "399     2.815543      156.465073      160.958969  \n",
       "\n",
       "[400 rows x 8 columns]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['throughput_max'] = th_max_all\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "08db84db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y.to_csv('../../Data/Result/Related3/latency_throughput_pred_max_min_related3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69f25f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### choose different weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "e15ecc91",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>send rates</th>\n",
       "      <th>block size</th>\n",
       "      <th>latency_pred</th>\n",
       "      <th>throughput_pred</th>\n",
       "      <th>latency_min</th>\n",
       "      <th>latency_max</th>\n",
       "      <th>throughput_min</th>\n",
       "      <th>throughput_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>19.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.761386</td>\n",
       "      <td>15.304539</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>19.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>1.080978</td>\n",
       "      <td>15.541059</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>19.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>1.097761</td>\n",
       "      <td>15.777582</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>19.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>1.152921</td>\n",
       "      <td>16.014105</td>\n",
       "      <td>0.263789</td>\n",
       "      <td>1.205774</td>\n",
       "      <td>15.068465</td>\n",
       "      <td>19.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>395</td>\n",
       "      <td>200</td>\n",
       "      <td>160</td>\n",
       "      <td>2.706232</td>\n",
       "      <td>160.012900</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465070</td>\n",
       "      <td>160.958970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>396</td>\n",
       "      <td>200</td>\n",
       "      <td>170</td>\n",
       "      <td>2.351660</td>\n",
       "      <td>160.249400</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465070</td>\n",
       "      <td>160.958970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>397</td>\n",
       "      <td>200</td>\n",
       "      <td>180</td>\n",
       "      <td>2.354682</td>\n",
       "      <td>160.485950</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465070</td>\n",
       "      <td>160.958970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>398</td>\n",
       "      <td>200</td>\n",
       "      <td>190</td>\n",
       "      <td>2.437658</td>\n",
       "      <td>160.722460</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465070</td>\n",
       "      <td>160.958970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>399</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>2.620018</td>\n",
       "      <td>160.958970</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>2.815543</td>\n",
       "      <td>156.465070</td>\n",
       "      <td>160.958970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  send rates  block size  latency_pred  throughput_pred  \\\n",
       "0             0          10          10      0.263789        15.068465   \n",
       "1             1          10          20      0.761386        15.304539   \n",
       "2             2          10          30      1.080978        15.541059   \n",
       "3             3          10          40      1.097761        15.777582   \n",
       "4             4          10          50      1.152921        16.014105   \n",
       "..          ...         ...         ...           ...              ...   \n",
       "395         395         200         160      2.706232       160.012900   \n",
       "396         396         200         170      2.351660       160.249400   \n",
       "397         397         200         180      2.354682       160.485950   \n",
       "398         398         200         190      2.437658       160.722460   \n",
       "399         399         200         200      2.620018       160.958970   \n",
       "\n",
       "     latency_min  latency_max  throughput_min  throughput_max  \n",
       "0       0.263789     1.205774       15.068465       19.562485  \n",
       "1       0.263789     1.205774       15.068465       19.562485  \n",
       "2       0.263789     1.205774       15.068465       19.562485  \n",
       "3       0.263789     1.205774       15.068465       19.562485  \n",
       "4       0.263789     1.205774       15.068465       19.562485  \n",
       "..           ...          ...             ...             ...  \n",
       "395     1.238566     2.815543      156.465070      160.958970  \n",
       "396     1.238566     2.815543      156.465070      160.958970  \n",
       "397     1.238566     2.815543      156.465070      160.958970  \n",
       "398     1.238566     2.815543      156.465070      160.958970  \n",
       "399     1.238566     2.815543      156.465070      160.958970  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the processed data directly\n",
    "y = pd.read_csv('../../Data/Result/Related3/latency_throughput_pred_max_min_related3.csv')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "feeaff86",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "6c91ffb3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "latency_score = alpha * ((y['latency_max'] - y['latency_pred']) / (y['latency_max'] - y['latency_min']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "26a8b685",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "throughput_score = beta * ((y['throughput_pred'] - y['throughput_min']) / (y['throughput_max'] - y['throughput_min']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "8115a1e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1       , 0.09445335, 0.10789287, 0.15347862, 0.19499052,\n",
       "       0.24076077, 0.28411376, 0.34118122, 0.38532134, 0.43394256,\n",
       "       0.48595316, 0.53295217, 0.57992315, 0.62762707, 0.67538496,\n",
       "       0.72362396, 0.76801883, 0.81344691, 0.86210508, 0.91087455,\n",
       "       0.1       , 0.12332625, 0.14195362, 0.1713106 , 0.20067348,\n",
       "       0.244484  , 0.28768529, 0.3352688 , 0.3816156 , 0.42631516,\n",
       "       0.47876544, 0.52652549, 0.57426005, 0.62199595, 0.66973126,\n",
       "       0.71653811, 0.76201506, 0.81077908, 0.85935931, 0.90618526,\n",
       "       0.1       , 0.13520345, 0.1614806 , 0.19100421, 0.22055195,\n",
       "       0.2500996 , 0.29296387, 0.33530923, 0.38155822, 0.42631584,\n",
       "       0.47560008, 0.52335341, 0.57108477, 0.61881651, 0.66654709,\n",
       "       0.71199535, 0.76166244, 0.81031299, 0.85888054, 0.90390031,\n",
       "       0.1       , 0.13258009, 0.16702463, 0.20276701, 0.23860265,\n",
       "       0.27192007, 0.3012662 , 0.34022406, 0.38523867, 0.42832928,\n",
       "       0.47398037, 0.52151144, 0.56923402, 0.61695472, 0.66418062,\n",
       "       0.71098881, 0.75959579, 0.80692995, 0.85263196, 0.90189624,\n",
       "       0.1       , 0.13543697, 0.17245441, 0.21031775, 0.24674646,\n",
       "       0.28317265, 0.32065154, 0.35799246, 0.39785832, 0.43478543,\n",
       "       0.47952589, 0.52257298, 0.56918083, 0.61578868, 0.6648654 ,\n",
       "       0.71171054, 0.76010437, 0.80824456, 0.85769756, 0.90702983,\n",
       "       0.1       , 0.13879147, 0.17912008, 0.21853725, 0.25538923,\n",
       "       0.29193131, 0.32789655, 0.36876755, 0.41175252, 0.44688521,\n",
       "       0.48759776, 0.52630336, 0.56978639, 0.61578896, 0.6648622 ,\n",
       "       0.71108649, 0.75855939, 0.80913252, 0.8601987 , 0.90797118,\n",
       "       0.1       , 0.13844955, 0.17853062, 0.21876607, 0.25805898,\n",
       "       0.29652434, 0.33096109, 0.37119425, 0.41142944, 0.45472177,\n",
       "       0.49063259, 0.52843869, 0.5684229 , 0.6178158 , 0.66721208,\n",
       "       0.71871827, 0.76383757, 0.8112859 , 0.86203922, 0.90637109,\n",
       "       0.1       , 0.13784769, 0.17699965, 0.2161508 , 0.25467109,\n",
       "       0.29167347, 0.32741188, 0.36636697, 0.40900298, 0.44321319,\n",
       "       0.48943957, 0.52697496, 0.56853667, 0.615788  , 0.66946815,\n",
       "       0.71671003, 0.7623743 , 0.80739523, 0.86089791, 0.90789859,\n",
       "       0.1       , 0.1394813 , 0.17831183, 0.2167282 , 0.25507115,\n",
       "       0.2912347 , 0.32730661, 0.36184174, 0.40405605, 0.43920279,\n",
       "       0.47368505, 0.53317896, 0.57583123, 0.61772793, 0.66495728,\n",
       "       0.71507883, 0.75860266, 0.80728204, 0.85937058, 0.90656904,\n",
       "       0.1       , 0.14062199, 0.18018455, 0.219745  , 0.2588281 ,\n",
       "       0.29546996, 0.3313836 , 0.36341636, 0.40555828, 0.44142087,\n",
       "       0.47466054, 0.52105358, 0.57335645, 0.61852234, 0.66685376,\n",
       "       0.7245551 , 0.76604569, 0.81591507, 0.86605773, 0.91348849,\n",
       "       0.1       , 0.14304056, 0.18426051, 0.22548152, 0.26670048,\n",
       "       0.3071859 , 0.34645471, 0.37992172, 0.42317247, 0.46239761,\n",
       "       0.49812875, 0.54573775, 0.59068917, 0.63665282, 0.67329346,\n",
       "       0.73672611, 0.78890524, 0.805263  , 0.8598155 , 0.93729019,\n",
       "       0.1       , 0.14484231, 0.18755467, 0.23026906, 0.27298346,\n",
       "       0.31552835, 0.356857  , 0.39307261, 0.43806128, 0.48069175,\n",
       "       0.51902611, 0.56662365, 0.614344  , 0.61579105, 0.69472165,\n",
       "       0.75845312, 0.80563177, 0.85382724, 0.90256241, 0.94948711,\n",
       "       0.1       , 0.1450415 , 0.18545271, 0.22586597, 0.2662792 ,\n",
       "       0.3066925 , 0.3454253 , 0.37406812, 0.41891135, 0.46144639,\n",
       "       0.49580747, 0.54263764, 0.5898653 , 0.61578855, 0.67564457,\n",
       "       0.73028713, 0.77892571, 0.82324381, 0.87458016, 0.92741885,\n",
       "       0.1       , 0.14379891, 0.1835448 , 0.22329078, 0.26303374,\n",
       "       0.30275063, 0.34086697, 0.36856418, 0.41028987, 0.45152232,\n",
       "       0.48611825, 0.53218844, 0.58219427, 0.62460085, 0.66315895,\n",
       "       0.71300859, 0.7763908 , 0.81898296, 0.8689112 , 0.91840768,\n",
       "       0.09586602, 0.14736621, 0.18858688, 0.22980754, 0.27102919,\n",
       "       0.30788925, 0.34906706, 0.33157947, 0.41886064, 0.46272079,\n",
       "       0.51069368, 0.55798461, 0.61221602, 0.65600424, 0.68979369,\n",
       "       0.74444604, 0.79663277, 0.84321856, 0.89064428, 0.93071399,\n",
       "       0.08942131, 0.147368  , 0.19084983, 0.23711027, 0.28551539,\n",
       "       0.32320772, 0.35717273, 0.36420162, 0.42617755, 0.45999457,\n",
       "       0.48473402, 0.5629972 , 0.6162749 , 0.65852756, 0.69490498,\n",
       "       0.710528  , 0.8062062 , 0.85051212, 0.91078652, 0.95644354,\n",
       "       0.04893623, 0.14006366, 0.19126872, 0.24210263, 0.282502  ,\n",
       "       0.32573502, 0.3681076 , 0.35438559, 0.45315806, 0.4737077 ,\n",
       "       0.49906149, 0.5972666 , 0.64420373, 0.68673343, 0.71407486,\n",
       "       0.71052516, 0.8045256 , 0.85186734, 0.91486918, 0.96936286,\n",
       "       0.05501425, 0.1473661 , 0.19184549, 0.23177597, 0.27763477,\n",
       "       0.32432746, 0.37102617, 0.33158274, 0.42099432, 0.44850924,\n",
       "       0.48219383, 0.59412574, 0.64383998, 0.68631384, 0.71696196,\n",
       "       0.71509892, 0.78765402, 0.85576144, 0.91738132, 0.96946629,\n",
       "       0.06506161, 0.14254801, 0.19473421, 0.2393479 , 0.28648691,\n",
       "       0.33517608, 0.38386728, 0.34824992, 0.42863918, 0.43967002,\n",
       "       0.47368505, 0.59385173, 0.6551497 , 0.693926  , 0.72831366,\n",
       "       0.7174087 , 0.77669053, 0.85056164, 0.91553218, 0.95555102,\n",
       "       0.04616026, 0.13182339, 0.19474043, 0.24138361, 0.28803505,\n",
       "       0.33468443, 0.38172406, 0.36229417, 0.42584382, 0.44002881,\n",
       "       0.47368611, 0.59192809, 0.62679216, 0.66174716, 0.6897599 ,\n",
       "       0.71746085, 0.78730929, 0.83449187, 0.87659637, 0.9123987 ])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score = latency_score.values + throughput_score.values\n",
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "bbaaecfc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y['score'] = y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "424aebd6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y.to_csv('../../Data/Result/Related3/latency_throughput_score_alpha' + str(alpha) + '_beta' + str(beta) + '_related3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780a326",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5475f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22812b0d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "280.281px",
    "left": "1365.86px",
    "top": "537.052px",
    "width": "297.438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}